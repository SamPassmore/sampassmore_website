[
  {
    "objectID": "posts/2022-02-25-longitudinal-multilevel-bayes-4/index.html",
    "href": "posts/2022-02-25-longitudinal-multilevel-bayes-4/index.html",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 4",
    "section": "",
    "text": "This is a continuation with some side analyses of this tutorial illustrating how one can use the brms and rethinking R packages to perform a Bayesian analysis of longitudinal data using a multilevel/hierarchical/mixed-effects setup."
  },
  {
    "objectID": "posts/2022-02-25-longitudinal-multilevel-bayes-4/index.html#a-model-that-doesnt-work",
    "href": "posts/2022-02-25-longitudinal-multilevel-bayes-4/index.html#a-model-that-doesnt-work",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 4",
    "section": "A model that ‚Äúdoesn‚Äôt work‚Äù",
    "text": "A model that ‚Äúdoesn‚Äôt work‚Äù\nWhen we originally scribbled down models that we might use to fit our data, we came up with this model.\n\\[\n\\begin{align}\n\\textrm{Outcome} \\\\\nY_{i,t}  \\sim \\mathrm{Normal}\\left(\\mu_{i,t}, \\sigma\\right) \\\\\n\\\\\n\\textrm{Deterministic time-series trajectory} \\\\\n\\mu_{i,t} =  \\exp(\\alpha_{i}) \\log (t_{i}) -\\exp(\\beta_{i}) t_{i} \\\\\n\\\\\n\\textrm{Distribution for main parameters} \\\\\n\\alpha_{i}  \\sim \\mathrm{Normal}\\left(am_{i}, \\sigma_a \\right)  \\\\\n\\beta_{i}  \\sim \\mathrm{Normal}\\left(bm_{i}, \\sigma_b \\right) \\\\\n\\\\\n\\textrm{Deterministic models for main parameters} \\\\\nam_{i}   =  a_{0,i} + a_1 \\left(\\log (D_i) - \\log (D_m)\\right)  \\\\\nbm_{i}  =  b_{0,i} + b_1 \\left(\\log (D_i) - \\log (D_m)\\right) \\\\\n\\\\\n\\textrm{Distribution for (hyper)parameters} \\\\\na_{0,i}  \\sim \\mathrm{Normal}(2, 0.1) \\\\\na_{1}  \\sim \\mathrm{Normal}(0.5, 0.1) \\\\\nb_{0,i}  \\sim \\mathrm{Normal}(0, 0.1) \\\\\nb_{1}  \\sim \\mathrm{Normal}(-0.5, 0.1) \\\\\n\\sigma_a  \\sim \\mathrm{HalfCauchy}(0,1)  \\\\\n\\sigma_b  \\sim \\mathrm{HalfCauchy}(0,1)  \\\\\n\\sigma  \\sim \\mathrm{HalfCauchy}(0,1)  \n\\end{align}\n\\]\nThe model is similar to models 1-3, but with another distribution for parameters \\(\\alpha_i\\) and \\(\\beta_i\\). Fitting this model didn‚Äôt work, the fitting routine kept choking. We concluded that with this model we are overfitting. However, I am also not sure if there is something more fundamentally wrong in the way we wrote down this model. I‚Äôm not sure if a mix of having parameters defined by equations, then as distributions and equations again is a generally wrong way. This is currently beyond my Bayesian understanding. Feedback appreciated üòÑ.\nIt is straightforward to translate the model to rethinking or brms code, but since it didn‚Äôt fit well, and I‚Äôm not even sure if it‚Äôs a ‚Äúproper‚Äù model, there‚Äôs no point in showing the code. Implement it if you want, and let me know if you have some insights into what exactly might be wrong with the model."
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "",
    "text": "This is part two of a tutorial illustrating how to perform a Bayesian analysis of longitudinal data using a multilevel/hierarchical/mixed-effects setup. In this part, we‚Äôll fit the simulated data using the rethinking package.\nI assume you‚Äôve read part 1, otherwise this post won‚Äôt make much sense. You might even want to open that first part in a separate tab for quick comparison."
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#model-1",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#model-1",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "Model 1",
    "text": "Model 1\nThese lines of code specify the full set of equations for our model 1. Note how closely the R code resembles the mathematical notation. That close match between math and code is one of the nice features of rethinking/ulam. Also note the indexing of the parameters a0 and b0 by id, which indicates that each individual has their own values.\n\n#wide-prior, no-pooling model\n#separate intercept for each individual/id\n#2x(N+1)+1 parameters\nm1 &lt;- alist(\n  # distribution of outcome\n  outcome ~ dnorm(mu, sigma),\n\n  # main equation for time-series trajectory\n  mu &lt;- exp(alpha)*log(time) - exp(beta)*time,\n\n  #equations for alpha and beta\n  alpha &lt;-  a0[id] + a1*dose_adj,\n  beta &lt;-  b0[id] + b1*dose_adj,\n\n  #priors\n  a0[id] ~ dnorm(2,  10),\n  b0[id] ~ dnorm(0.5, 10),\n\n  a1 ~ dnorm(0.3, 1),\n  b1 ~ dnorm(-0.3, 1),\n  sigma ~ cauchy(0,1)\n)\n\nYou might have noticed that I chose some of the values in the priors to be different than the values we used to generate the simulated data. I don‚Äôt want to make things too easy for the fitting routine üòÅ. We want to have the fitting routine ‚Äúfind‚Äù the right answer (parameter estimates). Hopefully, even if we don‚Äôt start at the right values, we‚Äôll end up there."
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#model-2",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#model-2",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "Model 2",
    "text": "Model 2\nNow we‚Äôll set up model 2 exactly as for model 1 but with some of the priors changed as discussed previously. Specifically, the priors now force the individual-level parameters to be essentially all the same. Note that - as you will see below - this model is not a good model, and if one wanted to not allow the \\(a_0\\) and \\(b_0\\) parameters to have any individual level variation, one should just implement and run the model 2 alternative I describe below. We‚Äôll run this model anyway, to just illustration and to see what happens.\n\n#narrow-prior, full-pooling model\n#2x(N+2)+1 parameters\nm2 &lt;- alist(\n  outcome ~ dnorm(mu, sigma),\n  mu &lt;- exp(alpha)*log(time) - exp(beta)*time,\n  alpha &lt;-  a0[id] + a1*dose_adj,\n  beta &lt;-  b0[id] + b1*dose_adj,\n  a0[id] ~ dnorm(mu_a,  0.0001),\n  b0[id] ~ dnorm(mu_b, 0.0001),\n  mu_a ~ dnorm(2, 1),\n  mu_b ~ dnorm(0.5, 1),\n  a1 ~ dnorm(0.3, 1),\n  b1 ~ dnorm(-0.3, 1),\n  sigma ~ cauchy(0,1)\n)"
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#model-3",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#model-3",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "Model 3",
    "text": "Model 3\nThis is the same as model 1 but with different values for the priors. These priors are somewhat regularizing and more reasonable. As we‚Äôll see, the results are similar to those from model 1, but the model runs more efficiently and thus faster.\n\n#regularizing prior, partial-pooling model\nm3 &lt;- alist(\n  outcome ~ dnorm(mu, sigma),\n  mu &lt;- exp(alpha)*log(time) - exp(beta)*time,\n  alpha &lt;-  a0[id] + a1*dose_adj,\n  beta &lt;-  b0[id] + b1*dose_adj,\n  a0[id] ~ dnorm(2,  1),\n  b0[id] ~ dnorm(0.5, 1),\n  a1 ~ dnorm(0.3, 1),\n  b1 ~ dnorm(-0.3, 1),\n  sigma ~ cauchy(0,1)\n)"
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#model-4",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#model-4",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "Model 4",
    "text": "Model 4\nThis is our adaptive pooling model. For this model, we specify a few extra distributions.\n\n#adaptive priors, partial-pooling model\n#2x(N+2)+1 parameters\nm4 &lt;- alist(\n  outcome ~ dnorm(mu, sigma),\n  mu &lt;- exp(alpha)*log(time) - exp(beta)*time,\n  alpha &lt;-  a0[id] + a1*dose_adj,\n  beta &lt;-  b0[id] + b1*dose_adj,\n  a0[id] ~ dnorm(mu_a,  sigma_a),\n  b0[id] ~ dnorm(mu_b, sigma_b),\n  mu_a ~ dnorm(2, 1),\n  mu_b ~ dnorm(0.5, 1),\n  sigma_a ~ cauchy(0, 1),\n  sigma_b ~ cauchy(0, 1),\n  a1 ~ dnorm(0.3, 1),\n  b1 ~ dnorm(-0.3, 1),\n  sigma ~ cauchy(0, 1)\n)"
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#a-few-model-alternatives",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#a-few-model-alternatives",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "A few model alternatives",
    "text": "A few model alternatives\nThere are a few model alternatives I also want to consider. The first one is a version of model 2 that gets rid of individual-level parameters and instead has only population-level parameters. I discussed this model in part 1 of the tutorial and called it 2a there. Here is the model definition\n\nModel 2a\n\n#full-pooling model, population-level parameters only\n#2+2+1 parameters\nm2a &lt;- alist(\n  outcome ~ dnorm(mu, sigma),\n  mu &lt;- exp(alpha)*log(time) - exp(beta)*time,\n  alpha &lt;-  a0 + a1*dose_adj,\n  beta &lt;-  b0 + b1*dose_adj,\n  a0 ~ dnorm(2,  0.1),\n  b0 ~ dnorm(0.5, 0.1),\n  a1 ~ dnorm(0.3, 1),\n  b1 ~ dnorm(-0.3, 1),\n  sigma ~ cauchy(0,1)\n)\n\nNote that a0 and b0 are not indexed by id anymore and are now single numbers, instead of \\(N\\) values as before.\n\n\nModel 4a\nAnother model I want to look at is a variant of model 4. This is in fact the same model as model 4, but written in a different way. A potential problem with model 4 and similar models is that parameters inside parameters can lead to inefficient or unreliable numerical results when running your Monte Carlo routine (in our case, this is Stan-powered Hamilton Monte Carlo). It is possible to rewrite the model such that it is the same model, but it looks different in a way that makes the numerics often run better. It turns out for our example, model 4 above runs ok. But it‚Äôs a good idea to be aware of the fact that one can re-write models if needed, therefore I decided to include this model alternative here.\nThe above model 4 is called a centered model and the re-write for model 4a is called a non-centered model. The trick is to pull out the parameters from inside the distributions for \\(a_{0,i}\\) and \\(b_{0,i}\\). The non-centered model looks like this:\n\n#adaptive priors, partial-pooling model\n#non-centered\nm4a &lt;- alist(\n  outcome ~ dnorm(mu, sigma),\n  mu &lt;- exp(alpha)*log(time) - exp(beta)*time,\n  #rewritten to non-centered\n  alpha &lt;-  mu_a + az[id]*sigma_a + a1*dose_adj,\n  beta  &lt;-  mu_b + bz[id]*sigma_b + b1*dose_adj,\n  #rewritten to non-centered\n  az[id] ~ dnorm(0, 1),\n  bz[id] ~ dnorm(0, 1),\n  mu_a ~ dnorm(2, 1),\n  mu_b ~ dnorm(0.5, 1),\n  sigma_a ~ cauchy(0, 1),\n  sigma_b ~ cauchy(0, 1),\n  a1 ~ dnorm(0.3, 1),\n  b1 ~ dnorm(-0.3, 1),\n  sigma ~ cauchy(0, 1)\n\n  )\n\nAgain, this model is mathematically the same as the original model 4. If this is confusing and doesn‚Äôt make sense (it sure wouldn‚Äôt to me if I just saw that for the first time üòÅ), check the Statistical Rethinking book. (And no, I do not get a commission for continuing to point you to the book, and I wish there was a free online version (or a cheap paperback). But it is a great book and if you want to learn this kind of modeling for real, I think it‚Äôs worth the investment.)\n\n\nModel 5\nAnother model, which I‚Äôm calling model 5 here, is one that does not include the dose effect. That means, parameters \\(a_1\\) and \\(b_1\\) are gone. Otherwise I‚Äôm following the setup of model 1. The reason I‚Äôm doing this is because on initial fitting of the above models, I could not obtain estimates for the dose parameters I used for the simulation. I noticed strong correlations between posterior distributions of the model parameters. I suspected an issue with non-identifiable parameters (i.e, trying to estimate more parameters than the data supports). To figure out what was going on, I wanted to see how a model without the dose component would perform. It turned out that the main reason things didn‚Äôt look right was because I had a typo in the code that generated the data, so what I thought was the generating model actually wasn‚Äôt ü§¶. A helpful colleague and reader pointed this out. Once I fixed it, things made more sense. But I figured it‚Äôs instructive to keep this model anyway.\n\n#no dose effect\n#separate intercept for each individual/id\n#2xN+1 parameters\nm5 &lt;- alist(\n  # distribution of outcome\n  outcome ~ dnorm(mu, sigma),\n\n  # main equation for time-series trajectory\n  mu &lt;- exp(alpha)*log(time) - exp(beta)*time,\n\n  #equations for alpha and beta\n  alpha &lt;-  a0[id],\n  beta &lt;-  b0[id],\n\n  #priors\n  a0[id] ~ dnorm(2,  10),\n  b0[id] ~ dnorm(0.5, 10),\n\n  sigma ~ cauchy(0,1)\n)"
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#setting-starting-values",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#setting-starting-values",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "Setting starting values",
    "text": "Setting starting values\nAny fitting routine needs to start with some parameter values and then from there tries to improve. Stan uses a heuristic way of picking some starting values. Often that works, sometimes it fails initially but then the routine fixes itself, and sometimes it fails all the way. In either case, I find it a good idea to specify starting values, even if they are not strictly needed. And it‚Äôs good to know that this is possible and how to do it, just in case you need it at some point. Setting starting values gives you more control, and you also know exactly what should happen when you look at for instance the traceplots of the chains.\n\n## Setting starting values\n#starting values for model 1\nstartm1 = list(a0 = rep(2,Nind), b0 = rep(0.5,Nind), a1 = 0.3 , b1 = -0.3, sigma = 1)\n#starting values for model 2\nstartm2 = list(a0 = rep(2,Nind), b0 = rep(0.5,Nind), mu_a = 2, mu_b = 1, a1 = 0.3 , b1 = -0.3, sigma = 1)\n#starting values for model 3\nstartm3 = startm1\n#starting values for models 4 and 4a\nstartm4 = list(mu_a = 2, sigma_a = 1, mu_b = 1, sigma_b = 1, a1 = 0.3 , b1 = -0.3, sigma = 1)\nstartm4a = startm4\n#starting values for model 2a\nstartm2a = list(a0 = 2, b0 = 0.5, a1 = 0.3, b1 = -0.3, sigma = 1)\n#starting values for model 5\nstartm5 = list(a0 = rep(2,Nind), b0 = rep(0.5,Nind), sigma = 1)\n\n#put different starting values in list\n#need to be in same order as models below\nstartlist = list(startm1,startm2,startm3,startm4,startm2a,startm4,startm5)\n\nNote that we only specify values for the parameters that are directly estimated. Parameters that are built from other parameters (e.g.¬†\\(\\alpha\\) and \\(\\beta\\)) are computed and don‚Äôt need starting values.\nFor some more detailed discussion on starting values, see for instance this post by Solomon Kurz. He uses brms in his example, but the same idea applies with any package/fitting routine. He also explains that it is a good idea to set different starting values for each chain. I am not sure if/how this could be done with rethinking, it seems ulam does not support this? But it can be done for brms (and I‚Äôm doing it there)."
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#model-fitting",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#model-fitting",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "Model fitting",
    "text": "Model fitting\nNow that we specified all models, we can loop through all models and fit them. First, some setup before the actual fitting loop.\n\n#general settings for fitting\n#you might want to adjust based on your computer\nwarmup = 3000 \niter = 5000\nmax_td = 15 #tree depth\nadapt_delta = 0.999\nchains = 5\ncores  = chains\nseed = 4321\n# for quick testing, use the settings below\n# results won't make much sense, but can make sure the code runs\n#warmup = 600 #for testing\n#iter = warmup + floor(warmup/2)\n#max_td = 10 #tree depth\n#adapt_delta = 0.99\n\n\n#stick all models into a list\nmodellist = list(m1=m1,m2=m2,m3=m3,m4=m4,m2a=m2a,m4a=m4a,m5=m5)\n# set up a list in which we'll store our results\nfl = vector(mode = \"list\", length = length(modellist))\n\n\n#setting for parameter constraints\nconstraints = list(sigma=\"lower=0\",sigma_a=\"lower=0\",sigma_b=\"lower=0\")\n\nThe first code block defines various settings for the ulam function. Look at the help file for details. Then we place all models into a list, set up an empty list for our fit results, and specify the data needed for fitting. The final command enforces some constraints on parameters. For our model, we want Half-Cauchy distributions for all variance parameters to ensure they are positive. Above, I specified them as Cauchy. There is no direct Half-Cauchy implementation. The way one achieves one is to tell ulam/Stan that the values for those parameters need to be positive. That‚Äôs what the constraints line in the code below does.\nLooping over each model and fitting it. In addition to the actual fitting call to ulam, I‚Äôm also printing a few messages and storing the model name and the time it took to run. That‚Äôs useful for diagnostic. It‚Äôs generally a good idea to do short runs/chains until things work, then do a large run to get the actual result. Recording the running time helps decide how long a real run can be and how long it might take.\n\n# fitting models\n#loop over all models and fit them using ulam\nfor (n in 1:length(modellist))\n{\n\n  cat('************** \\n')\n  cat('starting model', names(modellist[n]), '\\n')\n\n  tstart=proc.time(); #capture current time\n\n  #run model fit\n  fit &lt;- ulam(flist = modellist[[n]],\n                          data = fitdat,\n                          start=startlist[[n]],\n                          constraints=constraints,\n                          log_lik=TRUE, cmdstan=TRUE,\n                          control=list(adapt_delta=adapt_delta,\n                                       max_treedepth = max_td),\n                          chains=chains, cores = cores,\n                          warmup = warmup, iter = iter,\n                          seed = seed\n  )# end ulam\n\n  # save fit object to list\n  fl[[n]]$fit &lt;- fit\n  \n  #capture time taken for fit\n  tdiff=proc.time()-tstart;\n  runtime_minutes=tdiff[[3]]/60;\n\n  cat('model fit took this many minutes:', runtime_minutes, '\\n')\n  cat('************** \\n')\n\n  #add some more things to the fit object\n  fl[[n]]$runtime = runtime_minutes\n  fl[[n]]$model = names(modellist)[n]\n\n} #end fitting of all models\n\n# saving the list of results so we can use them later\n# the file is too large for standard Git/GitHub\n# Git Large File Storage should be able to handle it\n# I'm using a simple hack so I don't have to set up Git LFS\n# I am saving these large file to a folder that is synced with Dropbox\n# adjust accordingly for your setup\n#filepath = fs::path(\"C:\",\"Data\",\"Dropbox\",\"datafiles\",\"longitudinalbayes\",\"ulamfits\", ext=\"Rds\")\nfilepath = fs::path(\"D:\",\"Dropbox\",\"datafiles\",\"longitudinalbayes\",\"ulamfits\", ext=\"Rds\")\nsaveRDS(fl,filepath)"
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#models-1-and-3",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#models-1-and-3",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "Models 1 and 3",
    "text": "Models 1 and 3\nNow I‚Äôll look at bit more carefully at the different models. We start by comparing fits for models 1 and 3. Those two are essentially the same model, with the only difference being wider priors for the individual-level parameters in model 1. It is worth mentioning that when running the fitting routine, model 1 takes much longer to fit than model 3. With the settings I used, runtimes were 214 versus 53 minutes. The wide priors made the fitting efficiency poor. But let‚Äôs see how it impacts the results.\nFirst, we explore priors and posteriors. They are easy to extract from the models using the extract.prior() and extract.samples() functions from rethinking.\n\n#get priors and posteriors for models 1 and 3\nm1prior &lt;- rethinking::extract.prior(fl[[1]]$fit, n = 1e4)\nm1post &lt;- rethinking::extract.samples(fl[[1]]$fit, n = 1e4)\n\nm3prior &lt;- rethinking::extract.prior(fl[[3]]$fit, n = 1e4)\nm3post &lt;- rethinking::extract.samples(fl[[3]]$fit, n = 1e4)\n\nNow we can plot the distributions. Note that for the individual-level parameters \\(a_0\\) and \\(b_0\\), the plots show the distribution across all individuals. The dashed lines show the priors, the solid the posteriors. Black is model 1, blue is model 3.\n\n#showing density plots for a0\nplot(density(m1prior$a0), xlim = c (-20,20), ylim = c(0,2), lty=2)\nlines(density(m1post$a0), lty=1)\nlines(density(m3prior$a0), col = \"blue\", lty=2)\nlines(density(m3post$a0), col = \"blue\", lty=1)\n\n\n\n\n\n\n\n#showing density plots for b0\nplot(density(m1prior$b0), xlim = c (-20,20), ylim = c(0,2), lty=2)\nlines(density(m1post$b0), lty=1)\nlines(density(m3prior$b0), col = \"blue\", lty=2)\nlines(density(m3post$b0), col = \"blue\", lty=1)\n\n\n\n\n\n\n\n#showing density plots for a1\nplot(density(m1prior$a1), xlim = c (-3,3), ylim = c(0,2), lty=2)\nlines(density(m1post$a1), lty=1)\nlines(density(m3prior$a1), col = \"blue\", lty=2)\nlines(density(m3post$a1), col = \"blue\", lty=1)\n\n\n\n\n\n\n\n#showing density plots for b1\nplot(density(m1prior$b1), xlim = c (-3,3), ylim = c(0,2), lty=2)\nlines(density(m1post$b1), lty=1)\nlines(density(m3prior$b1), col = \"blue\", lty=2)\nlines(density(m3post$b1), col = \"blue\", lty=1)\n\n\n\n\n\n\n\n\nWe set up the models to have wider \\(a_0\\) and \\(b_0\\) priors for model 1, and the same priors for the \\(a_1\\) and \\(b_1\\) parameters. The dashed lines in the figures show that. Looking at the posteriors, we find that changing the priors has an impact, especially for \\(a_1\\) and \\(b_1\\). Not only does model 3 lead to more peaked posteriors, they are also not centered at the same values, especially for \\(b_1\\). I don‚Äôt think that‚Äôs a good sign. We want the data to dominate the results, the priors should just be there to ensure the models explore the right parameter space efficiently and don‚Äôt do anything crazy. The fact that the same model, started with different priors, leads to different posterior distributions is in my opinion concerning. It could be that with more sampling, the posteriors might get closer. Or it might suggest that we are overfitting and have non-identifiability problems here.\nOne way to check that further is to look at potential correlations between parameter posterior distributions, e.g., using a pairs() plot as shown above. Here are such plots for the parameters associated with \\(\\alpha\\) for model 1. I only plot a few for each dose, otherwise the plots won‚Äôt be legible inside this html document. But you can try for yourself, if you make the plot large enough you can fit them all. You can also make plots for model 3 and for the \\(b\\) parameters, those look very similar.\n\n# all \"a\" parameters - too big to show\n#pairs(fl[[1]]$fit, pars = c(\"a0\",\"a1\"))\n# a few parameters for each dose\n#low dose\nrethinking::pairs(fl[[1]]$fit, pars = c(\"a0[1]\",\"a0[2]\",\"a0[3]\",\"a0[4]\",\"a1\"))\n\n\n\n\n\n\n\n#medium dose\nrethinking::pairs(fl[[1]]$fit, pars = c(\"a0[8]\",\"a0[9]\",\"a0[10]\",\"a0[11]\",\"a1\"))\n\n\n\n\n\n\n\n#high dose\nrethinking::pairs(fl[[1]]$fit, pars = c(\"a0[16]\",\"a0[17]\",\"a0[18]\",\"a0[19]\",\"a1\"))\n\n\n\n\n\n\n\n\nRecall that we set up the model such that dose is non-zero for low and high dose, while for the intermediate dose it drops out of the model. What seems to happen is that if the dose effect, i.e., \\(a_1\\), is present, there is a strong correlation among that parameter and the individual-level parameters for that dose. That part makes some sense to me. Both \\(a_{0,i}\\) or \\(a_1\\) can change \\(\\alpha\\) and thus the trajectory. If one is low, the other might be high, and the reverse, leading to similarly good fits.\nBecause every \\(a_{0,i}\\) is correlated with \\(a_1\\) in this way, this also leads to correlations among the \\(a_{0,i}\\) values. I am surprised that this is an essentially perfect correlation. Maybe, if I thought about it harder and/or did the math, it would be clear that it needs to be that way. But I haven‚Äôt yet, so for now I‚Äôm just taking it as given üòÅ. Overall, this is another sign of that we might be overfitting and have non-identifiability problems, i.e.¬†combinations for different values of \\(a_{0,i}\\) and \\(a_1\\) can lead to more or less the same results (everything I write here of course also holds for the \\(b_{0,i}\\) and \\(b_1\\) parameters).\nLet‚Äôs move on and now look at the posterior distributions in numerical form. For that, I use the precis function from rethinking. Instead of printing all the \\(N\\) different values of \\(a_{0,i}\\) and \\(b_{0,i}\\), I compute their means. If you want to see them all, change to depth=2 in the precis function.\n\n# Model 1\na0mean = mean(rethinking::precis(fl[[1]]$fit,depth=2,\"a0\")$mean)\nb0mean = mean(rethinking::precis(fl[[1]]$fit,depth=2,\"b0\")$mean)\nprint(rethinking::precis(fl[[1]]$fit,depth=1),digits = 2)\n\n       mean    sd  5.5% 94.5% n_eff Rhat4\na1     0.20 0.743 -1.00  1.38  1359     1\nb1    -0.19 0.744 -1.38  0.99  1120     1\nsigma  1.06 0.051  0.99  1.15 10765     1\n\nprint(c(a0mean,b0mean))\n\n[1] 2.963794 1.001530\n\n# Model 3\na0mean = mean(rethinking::precis(fl[[3]]$fit,depth=2,\"a0\")$mean)\nb0mean = mean(rethinking::precis(fl[[3]]$fit,depth=2,\"b0\")$mean)\nprint(rethinking::precis(fl[[3]]$fit,depth=1),digits = 2)\n\n        mean    sd   5.5% 94.5% n_eff Rhat4\na1     0.142 0.109 -0.033 0.316  3298     1\nb1    -0.081 0.107 -0.253 0.092  2648     1\nsigma  1.061 0.051  0.983 1.146 13106     1\n\nprint(c(a0mean,b0mean))\n\n[1] 2.9755846 0.9807953\n\n\nThe models seem to have converged ok, based on Rhat values of 1. Some parameters sampled better than others, as can be seen by the varying n_eff values. I used 5 chains of 2000 post-warmup samples for each chain, so the actual samples are 10000. If n_eff is lower than that, it means the sampling was not efficient, more means it worked very well (see e.g.¬†Statistical Rethinking why it‚Äôs possible to get more effective samples than actual samples.)\nWe find that estimates for \\(a_{0}\\), \\(b_0\\) and \\(\\sigma\\) are similar, \\(a_1\\) and \\(b_1\\) differ more.\nAgain, note that the only thing we changed between models 1 and 3 are to make the priors for the \\(a_{0,i}\\) and \\(b_{0,i}\\) parameters tighter. It didn‚Äôt seem to impact estimates for those parameters, but it did impact the estimates for the posterior distributions of parameters \\(a_1\\) and \\(b_1\\). The numbers are consistent with the posterior distribution figures above."
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#comparing-model-estimates-with-the-truth",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#comparing-model-estimates-with-the-truth",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "Comparing model estimates with the truth",
    "text": "Comparing model estimates with the truth\nWe know the ‚Äútruth‚Äù here, i.e., the actual values of the parameters which we used to created the simulated data. To generate the data, we used these parameter values: \\(\\sigma =\\) 1, \\(\\mu_a =\\) 3, \\(\\mu_b =\\) 1, \\(a_1 =\\) 0.1, \\(b_1 =\\) -0.1. We also said that our main scientific question is if there is a dose effect, i.e.¬†non-zero \\(a_1\\) and \\(b_1\\).\nThe models find estimates of \\(\\mu_a\\), \\(\\mu_b\\) and \\(\\sigma\\) that are close to what we used. The estimates for \\(a_1\\) and \\(b_1\\) are not that great. That‚Äôs especially true for model 1. With these models, we aren‚Äôt able to convincingly recover the parameters used to generate the data. I‚Äôm not sure if increasing the sampling (longer or more chains) would help. Both models, especially model 1, already took quite a while to run. Thus I‚Äôm not too keen to try it with even more samples. As we‚Äôll see below, alternative models do better."
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#models-2-and-2a",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#models-2-and-2a",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "Models 2 and 2a",
    "text": "Models 2 and 2a\nNext, let‚Äôs look at models 2 and 2a. The estimates should be similar since the two models are conceptually pretty much the same.\n\n# Compare models 2 and 2a\n# first we compute the mean across individuals for model 2\na0mean = mean(rethinking::precis(fl[[2]]$fit,depth=2,\"a0\")$mean)\nb0mean = mean(rethinking::precis(fl[[2]]$fit,depth=2,\"b0\")$mean)\n\n#rest of model 2\nprint(rethinking::precis(fl[[2]]$fit,depth=1),digits = 2)\n\n        mean     sd  5.5%  94.5% n_eff Rhat4\nmu_a   2.982 0.0221  2.95  3.020    34   1.1\nmu_b   0.991 0.0191  0.96  1.023    30   1.2\na1     0.096 0.0097  0.08  0.110    87   1.0\nb1    -0.097 0.0080 -0.11 -0.085   165   1.0\nsigma  6.825 0.3217  6.35  7.334    50   1.1\n\nprint(c(a0mean,b0mean))\n\n[1] 2.9823079 0.9908912\n\n#model 2a\nprint(rethinking::precis(fl[[5]]$fit,depth=1),digits = 2)\n\n        mean     sd  5.5%  94.5% n_eff Rhat4\na0     2.919 0.0237  2.88  2.955  5827     1\nb0     0.938 0.0208  0.90  0.970  5976     1\na1     0.108 0.0108  0.09  0.125  6119     1\nb1    -0.098 0.0096 -0.11 -0.083  6474     1\nsigma  7.007 0.3188  6.51  7.542  7481     1\n\n\nThe first thing to note is that model 2 performs awfully, with Rhat values &gt;1 and very low effective sample size n_eff. This indicates that this model doesn‚Äôt work well for the data. Whenever you see diagnostics like that, you should not take the estimated values seriously. However, let‚Äôs pretend for a moment that we can take them seriously. Here is what we find.\nFirst, both models produce similar estimates. Since model 2a is simpler and doesn‚Äôt have that strange feature of us enforcing a very tight distribution for the \\(a_{0,i}\\) and \\(b_{0,i}\\) parameters, it actually samples much better, see the higher n_eff numbers. It also runs much faster, 1.6 minutes compared to 29 minutes for model 2.\nBoth models do a very poor job estimating \\(\\sigma\\). That‚Äôs because we don‚Äôt allow the models to have the flexibility needed to fit the data, so it has to account for any variation between its estimated mean trajectory and the real data by making \\(\\sigma\\) large.\nSince the models are more constrained compared to models 1 and 3, they produce estimates for \\(a_1\\) and \\(b_1\\) that are tighter. However, these estimates are over-confident. Overall these models are underfitting and not reliable. We can for instance look at this using the compare function:\n\nrethinking::compare(fl[[1]]$fit,fl[[3]]$fit,fl[[2]]$fit,fl[[5]]$fit)\n\n                 WAIC       SE       dWAIC        dSE    pWAIC        weight\nfl[[3]]$fit  832.3041 23.37289   0.0000000         NA 43.47114  5.465565e-01\nfl[[1]]$fit  832.6776 23.35188   0.3735338  0.3659438 43.55465  4.534435e-01\nfl[[2]]$fit 1779.6905 45.62606 947.3863988 46.8519545 11.11914 1.035839e-206\nfl[[5]]$fit 1788.5509 45.32354 956.2468479 47.0719252 10.52777 1.233873e-208\n\n\nI‚Äôm not going to discuss things in detail (see Statistical Rethinking), but a lower WAIC means a model that fits best in the sense that it strikes a good balance between fitting the data while not overfitting. As you can see, models 1 and 3 perform very similarly and models 2 and 2a are much worse.\nThe larger WAIC indicates either strong overfitting or underfitting. In this case, it‚Äôs underfitting. The models are not flexible enough to capture the individual-level variation. You‚Äôll see that clearly in the plots shown further below. If we did indeed not want to account for individual-level variation, we should go with a model that simply doesn‚Äôt include it, i.e.¬†model 2a. The contrived model 2 with very narrow priors is just a bad model, and I‚Äôm really only exploring it here for demonstration purposes."
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#models-4-and-4a",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#models-4-and-4a",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "Models 4 and 4a",
    "text": "Models 4 and 4a\nNow we get to the models we really care about. When I set up the models, I suggested that model 4 was similar to models 1-3, but with priors adaptively chosen. That didn‚Äôt apply during data generation/simulation since in that step, we always need to manually choose values. But during the fitting/estimation, we should expect that model 4 chooses priors in a smart way, such that it is better than the models where we fixed the priors. Let‚Äôs see what model 4 produces. We also look at model 4a, which is exactly the same model, just rewritten to potentially make the numerical fitting routine more efficient.\nLet‚Äôs start with prior and posterior plots.\n\n#get priors and posteriors for models 4 and 4a\nm4prior &lt;- rethinking::extract.prior(fl[[4]]$fit, n = 1e4)\nm4post &lt;- rethinking::extract.samples(fl[[4]]$fit, n = 1e4)\n\nm4aprior &lt;- rethinking::extract.prior(fl[[6]]$fit, n = 1e4)\nm4apost &lt;- rethinking::extract.samples(fl[[6]]$fit, n = 1e4)\n\nAs before, the dashed lines show the priors, the solid the posteriors. Black is model 4, blue is model 4a.\n\n#showing density plots for a0\nplot(density(m4prior$mu_a), xlim = c (-10,10), ylim = c(0,2), lty=2)\nlines(density(m4post$mu_a), lty=1)\nlines(density(m4aprior$mu_a), col = \"blue\", lty=2)\nlines(density(m4apost$mu_a), col = \"blue\", lty=1)\n\n\n\n\n\n\n\n#showing density plots for b0\nplot(density(m4prior$mu_b), xlim = c (-10,10), ylim = c(0,2), lty=2)\nlines(density(m4post$mu_b), lty=1)\nlines(density(m4aprior$mu_b), col = \"blue\", lty=2)\nlines(density(m4apost$mu_b), col = \"blue\", lty=1)\n\n\n\n\n\n\n\n#showing density plots for a1\nplot(density(m4prior$a1), xlim = c (-3,3), ylim = c(0,2), lty=2)\nlines(density(m4post$a1), lty=1)\nlines(density(m4aprior$a1), col = \"blue\", lty=2)\nlines(density(m4apost$a1), col = \"blue\", lty=1)\n\n\n\n\n\n\n\n#showing density plots for b1\nplot(density(m4prior$b1), xlim = c (-3,3), ylim = c(0,2), lty=2)\nlines(density(m4post$b1), lty=1)\nlines(density(m4aprior$b1), col = \"blue\", lty=2)\nlines(density(m4apost$b1), col = \"blue\", lty=1)\n\n\n\n\n\n\n\n\nAs you can see, up to numerical sampling variability, the results for models 4 and 4a are pretty much the same. That should be expected, since they are the same model, just reformulated for potential efficiency. Also, the posterior distributions are much narrower than the priors. I think that‚Äôs a good sign as well, it indicates the data mostly informed the posterior distributions, the priors just helped to keep things efficient.\nWe can also explore pair plots again, showing them here for model 4.\n\n# a few parameters for each dose\n#low dose\nrethinking::pairs(fl[[4]]$fit, pars = c(\"a0[1]\",\"a0[2]\",\"a0[3]\",\"a0[4]\",\"a1\"))\n\n\n\n\n\n\n\n#medium dose\nrethinking::pairs(fl[[4]]$fit, pars = c(\"a0[8]\",\"a0[9]\",\"a0[10]\",\"a0[11]\",\"a1\"))\n\n\n\n\n\n\n\n#high dose\nrethinking::pairs(fl[[4]]$fit, pars = c(\"a0[16]\",\"a0[17]\",\"a0[18]\",\"a0[19]\",\"a1\"))\n\n\n\n\n\n\n\n# mean of a0 prior\nrethinking::pairs(fl[[4]]$fit, pars = c(\"mu_a\",\"mu_b\",\"a1\",\"b1\"))\n\n\n\n\n\n\n\n#saving one plot so I can use as featured image\npng(filename = \"featured.png\", width = 6, height = 6, units = \"in\", res = 300)\nrethinking::pairs(fl[[4]]$fit, pars = c(\"mu_a\",\"mu_b\",\"a1\",\"b1\"))\ndev.off()\n\npng \n  2 \n\n\nWe still see the same issue with correlations among the parameters for dose levels where \\(a_1\\) is acting, though the correlations are not as extreme. They are also minor between the overall estimates for the mean of the \\(a_0\\) and \\(b_0\\) parameters and \\(a_1\\) and \\(b_1\\). I interpret this to mean that the adaptive sampling helped somewhat with the identifiability and overfitting problem, though it seems to not fully resolve it. The fact that we gave each individual their own \\(a_{0,i}\\) and \\(b_{0,i}\\) values allows those parameters to still ‚Äúabsorb‚Äù some of the dose-dependent signal in \\(a_1\\) and \\(b_1\\).\nWe can also again look at the numerical outputs from the precis function.\n\n# model 4\nprint(rethinking::precis(fl[[4]]$fit,depth=1),digits = 2)\n\n          mean    sd   5.5%  94.5% n_eff Rhat4\nmu_a     2.987 0.020  2.956  3.018 16421     1\nmu_b     0.986 0.025  0.946  1.027 15631     1\nsigma_a  0.093 0.015  0.072  0.120 11785     1\nsigma_b  0.119 0.020  0.092  0.153 12447     1\na1       0.086 0.011  0.069  0.103  2057     1\nb1      -0.106 0.013 -0.126 -0.086  2380     1\nsigma    1.062 0.052  0.983  1.147 15009     1\n\n# model 4a\nprint(precis(fl[[6]]$fit,depth=1),digits = 2)\n\n          mean    sd   5.5%  94.5% n_eff Rhat4\nmu_a     2.987 0.020  2.956  3.018  3254     1\nmu_b     0.986 0.025  0.947  1.024  3029     1\nsigma_a  0.093 0.016  0.071  0.120  3859     1\nsigma_b  0.119 0.020  0.092  0.154  3542     1\na1       0.086 0.010  0.069  0.102  3771     1\nb1      -0.106 0.013 -0.127 -0.085  3466     1\nsigma    1.063 0.052  0.984  1.148  9032     1\n\n\nThe numerics confirm that the two models lead to essentially the same results. The values for n_eff differ between models, though neither model is consistently larger. This suggests that each model formulation had advantages in sampling for some of the parameters.\nIn terms of run times, there wasn‚Äôt much difference, with 6 minutes for model 4 versus 9 minutes for model 4a (much better than models 1-3).\nIf we compare the parameter estimates with the true values and those found for models 1 and 3 above, we find that again the true \\(\\mu_a\\), \\(\\mu_b\\) and \\(\\sigma\\) are estimated fairly well. Estimates for \\(a_1\\) and \\(b_1\\) are now also pretty good, and the credible intervals are less wide.\nNow let‚Äôs briefly run the compare function too and include model 3 as well. In addition to using WAIC for comparison, I‚Äôm also including PSIS. Read about it in the Statistical Rethinking book. One advantage of PSIS is that it gives warnings if the estimates might not be reliable. You see that this happens here.\n\nrethinking::compare(fl[[3]]$fit,fl[[4]]$fit,fl[[6]]$fit, func = WAIC)\n\n                WAIC       SE     dWAIC       dSE    pWAIC    weight\nfl[[4]]$fit 831.3515 23.37623 0.0000000        NA 42.67688 0.4263132\nfl[[6]]$fit 831.9958 23.38283 0.6442751 0.2846596 42.89029 0.3089059\nfl[[3]]$fit 832.3041 23.37289 0.9525426 2.6103005 43.47114 0.2647810\n\n#rethinking::compare(fl[[3]]$fit,fl[[4]]$fit,fl[[6]]$fit, func = PSIS)\n\nWe do find that model 4/4a performs a bit better, but not by much. Note that model 4/4a has more actual parameters, but the effective parameters (which is described by pWAIC) is a bit smaller.\nOverall, this suggests that the adaptive pooling approach helped to estimate results more precisely and efficiently and is the best of the models."
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#model-5-1",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#model-5-1",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "Model 5",
    "text": "Model 5\nAs stated above, due to a typo in my code, the above models, including 4/4a, initially produced estimates for \\(a_1\\) and \\(b_1\\) that were not close to those (I thought I) used to generate the data. Based on the pair plots, I suspected non-identifiability issues and wanted to explore what would happen if I removed the dose.\nIf we now look at the pairs plots, maybe not surprisingly, the correlations between individual \\(a_0\\) parameters are gone.\n\n# a few parameters for each dose\n#low dose\nrethinking::pairs(fl[[7]]$fit, pars = c(\"a0[1]\",\"a0[2]\",\"a0[3]\",\"a0[4]\",\"a0[5]\"))\n\n\n\n\n\n\n\n#medium dose\nrethinking::pairs(fl[[7]]$fit, pars = c(\"a0[8]\",\"a0[9]\",\"a0[10]\",\"a0[11]\",\"a0[12]\"))\n\n\n\n\n\n\n\n#high dose\nrethinking::pairs(fl[[7]]$fit, pars = c(\"a0[16]\",\"a0[17]\",\"a0[18]\",\"a0[19]\",\"a0[20]\"))\n\n\n\n\n\n\n\n\nThe model estimates the parameters reasonably well\n\na0mean = mean(rethinking::precis(fl[[7]]$fit,depth=2,\"a0\")$mean)\nb0mean = mean(rethinking::precis(fl[[7]]$fit,depth=2,\"b0\")$mean)\nprint(rethinking::precis(fl[[7]]$fit,depth=1),digits = 2)\n\n      mean    sd 5.5% 94.5% n_eff Rhat4\nsigma  1.1 0.051 0.98   1.1 14886     1\n\nprint(c(a0mean,b0mean))\n\n[1] 3.0031082 0.9655898\n\n\nIt doesn‚Äôt seem quite as good as the previous models.\n\nrethinking::compare(fl[[3]]$fit,fl[[4]]$fit,fl[[7]]$fit)\n\n                WAIC       SE     dWAIC      dSE    pWAIC    weight\nfl[[4]]$fit 831.3515 23.37623 0.0000000       NA 42.67688 0.4462839\nfl[[3]]$fit 832.3041 23.37289 0.9525426 2.610301 43.47114 0.2771847\nfl[[7]]$fit 832.3088 23.33727 0.9572625 2.512509 43.43226 0.2765313\n\n\nAnd of course the main problem with this model: It can‚Äôt answer any question about the role of dose, since we removed that component from the model! So while ok to explore, scientifically not useful since it can‚Äôt help us address the question we want to answer regarding the potential impact of dose."
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#models-1-and-3-1",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#models-1-and-3-1",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "Models 1 and 3",
    "text": "Models 1 and 3\n\nplot(plotlist[[1]])\n\n\n\n\n\n\n\nplot(plotlist[[3]])\n\n\n\n\n\n\n\n\nDespite differences in estimates for the dose related parameters, the predicted outcomes of the models are very similar. In fact, it‚Äôs hard to tell any difference by just looking at the plots (but they are slightly different, I checked). Thus, despite the inability of these models to provide precises estimates of all the parameter values, the predictions/outcomes are fine, they fit the data well."
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#models-2-and-2a-1",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#models-2-and-2a-1",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "Models 2 and 2a",
    "text": "Models 2 and 2a\nFor models 2 and 2a, recall that the only variation is for dose, we didn‚Äôt allow variation among individuals. That‚Äôs reflected in the plots. The credible intervals based on parameters are tight, but because the variability, \\(\\sigma\\), had to account for all the differences, the prediction intervals are very wide.\n\nplot(plotlist[[2]])\n\n\n\n\n\n\n\nplot(plotlist[[5]])"
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#models-4-and-4a-1",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#models-4-and-4a-1",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "Models 4 and 4a",
    "text": "Models 4 and 4a\nThese models look good again, and very similar to models 1 and 3.\n\nplot(plotlist[[4]])\n\n\n\n\n\n\n\nplot(plotlist[[6]])"
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#model-5-2",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#model-5-2",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "Model 5",
    "text": "Model 5\nThe model fits look fine, suggesting that one parameter for each individual is enough to capture the data. That‚Äôs not surprising. However, this of course does not allow us to ask and answer any scientific questions about the role of dose.\n\nplot(plotlist[[7]])\n\n\n\n\n\n\n\n\nSo overall, the figures make sense. It seems that if we want to do prediction, all models that include individual variability are fine, models 2/2a are not great. If we wanted to estimate the model parameters, specifically \\(a_1\\) and \\(b_1\\), models 1 and 3 and of course 5 don‚Äôt work. In that case, model 2/2a works ok. I consider model 4/4a the best one overall.\nFor another example and more discussion of estimation versus prediction, see e.g.¬†Section 6.1. in Statistical Rethinking, as well as 9.5.4 (all referring to the 2nd edition of the book)."
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#footnotes",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#footnotes",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that I made the unfortunate choice of assigning the previously generated data to fitdat, which is not the same as the fitdat object I created in the Data Loading section above. I just noticed that a good while after I had written it all. So I won‚Äôt change it, but it would have been better if I had called it something different.‚Ü©Ô∏é"
  },
  {
    "objectID": "projects/quizgrader/index.html",
    "href": "projects/quizgrader/index.html",
    "title": "Quizgrader",
    "section": "",
    "text": "quizgrader is an R package allows teachers to administer and auto-grade quizzes that students submit online. It replaces the functionality often found in learning management systems (LMS). Student submissions can be fully analyzed to gain insights into problem areas. It is functional, but not yet fully tested. Learn more about it on the package website."
  },
  {
    "objectID": "projects/iblir/index.html",
    "href": "projects/iblir/index.html",
    "title": "Introduction to Biostatistics Labs in R (iblir)",
    "section": "",
    "text": "iblir is an R package that contains several Shiny/learnr based tutorials that teach introductory aspects of biostatistics in an interactive way. Learn more about it on the package website."
  },
  {
    "objectID": "projects/dsairm/index.html",
    "href": "projects/dsairm/index.html",
    "title": "DSAIRM - Dynamical Systems Approach to Immune Response Modeling",
    "section": "",
    "text": "We developed an R package that provides immunologists and other bench scientists a user-friendly introduction to simulation modeling of within-host infection, without the need to write computer code. Learn more about it on the package website."
  },
  {
    "objectID": "projects/SMIcourse/index.html",
    "href": "projects/SMIcourse/index.html",
    "title": "Simulation Modeling in Immunology",
    "section": "",
    "text": "Simulation Modeling in Immunology (SMI) is an annual workshop that I co-teach with my colleague Paul Thomas. It usually occurs each year in July in Seattle. In 2020 we taught it online. All course materials are available in the form of a simple GitHub website and can be used by anyone for self-learning. You can find the course on this site.\nThe course makes heavy use of my DSAIRM R package."
  },
  {
    "objectID": "projects/IDEMAcourse/index.html",
    "href": "projects/IDEMAcourse/index.html",
    "title": "Infectious Disease Epidemiology - a Model-based Approach",
    "section": "",
    "text": "Infectious Disease Epidemiology - A model-based approach (IDEMA) is an online course I teach at the University of Georgia. All course materials (or links to the materials) are available in the form of a simple GitHub website and can be used by anyone. You can find the course on this site.\nThe course also makes heavy use of my IDEMA online book and the DSAIDE R package."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nDiversity of kinship terminology and Kinbank\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDocumenting Three Pacific Creoles\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinguisitic Diversity in New Guinea\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMusical Diversity and Cultural History\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "presentations/2025-02-UGA-fluday/index.html",
    "href": "presentations/2025-02-UGA-fluday/index.html",
    "title": "Modeling to support influenza vaccine development",
    "section": "",
    "text": "These are the slides for this presentation."
  },
  {
    "objectID": "presentations/2024-11-Drake-FYOS/index.html",
    "href": "presentations/2024-11-Drake-FYOS/index.html",
    "title": "Vaccines to combat infectious diseases",
    "section": "",
    "text": "These are the slides for this presentation."
  },
  {
    "objectID": "presentations/2024-07-sismid-brand-workshop/index.html",
    "href": "presentations/2024-07-sismid-brand-workshop/index.html",
    "title": "Building and curating your brand (online presence)",
    "section": "",
    "text": "These are slides for a discussion I led on online presence/brand building as part of a career development workshop at SISMID 2024.\nThe presentation slides are here."
  },
  {
    "objectID": "presentations/2024-04-CIVR-HRP/index.html",
    "href": "presentations/2024-04-CIVR-HRP/index.html",
    "title": "Antibody ceiling effects following influenza vaccination",
    "section": "",
    "text": "These are the slides for this presentation."
  },
  {
    "objectID": "presentations/2023-10-Vaccine-Assessment/index.html",
    "href": "presentations/2023-10-Vaccine-Assessment/index.html",
    "title": "Assessment of Vaccines",
    "section": "",
    "text": "These are the slides for this presentation."
  },
  {
    "objectID": "presentations/2023-10-Drug-Resistance-Lecture/index.html",
    "href": "presentations/2023-10-Drug-Resistance-Lecture/index.html",
    "title": "Evolution of Drug resistance",
    "section": "",
    "text": "These are the slides for this presentation."
  },
  {
    "objectID": "presentations/2023-09-Emory/index.html",
    "href": "presentations/2023-09-Emory/index.html",
    "title": "Analysis and Modeling to evaluate influenza Vaccines responses",
    "section": "",
    "text": "These are the slides for this presentation."
  },
  {
    "objectID": "presentations/2023-06-CAIMS/index.html",
    "href": "presentations/2023-06-CAIMS/index.html",
    "title": "Analysis and Modeling to evaluate influenza Vaccines responses",
    "section": "",
    "text": "These are the slides for this presentation."
  },
  {
    "objectID": "presentations/2023-05-CEIRR/index.html",
    "href": "presentations/2023-05-CEIRR/index.html",
    "title": "Tools and resources for mechanistic within-host modeling",
    "section": "",
    "text": "These are the slides for this presentation."
  },
  {
    "objectID": "presentations/2023-01-academia-vs-industry/index.html",
    "href": "presentations/2023-01-academia-vs-industry/index.html",
    "title": "Some thoughts on academic versus industry positions",
    "section": "",
    "text": "These are slides for a discussion I led with our graduate students covering differences in academic versus industry jobs. The presentation slides are here."
  },
  {
    "objectID": "presentations/2022-07-sismid/index.html",
    "href": "presentations/2022-07-sismid/index.html",
    "title": "Infectious Diseases, Immunology, and Within-Host Models",
    "section": "",
    "text": "The workshop website, is here. This is mainly for archival reasons. All the useful material is on a dedicated, stand-alone website called Simulation Modeling in Immunology."
  },
  {
    "objectID": "presentations/2022-01-tips-for-publishing-papers/index.html",
    "href": "presentations/2022-01-tips-for-publishing-papers/index.html",
    "title": "Tips for publishing academic papers",
    "section": "",
    "text": "These are slides for a talk/discussion I led with our graduate students on how to succeed at publishing (academic) papers. The presentation slides are here."
  },
  {
    "objectID": "presentations/2021-10-fyos/index.html",
    "href": "presentations/2021-10-fyos/index.html",
    "title": "Introduction to infectious disease modeling",
    "section": "",
    "text": "These are slides for a talk I gave to a group of undergraduate students as part of a First Year Odyssey Seminar class. I provided a very brief introduction to infectious disease modeling, then we explored the DSAIDE software. The presentation slides are here."
  },
  {
    "objectID": "presentations/2021-09-DSATL/index.html",
    "href": "presentations/2021-09-DSATL/index.html",
    "title": "Adventures in Public Health Data Analytics - COVID-19 and beyond",
    "section": "",
    "text": "These are the slides for a presentation I gave at a (virtual) meetup event of the Data Science & Business Intelligence Society of Atlanta."
  },
  {
    "objectID": "presentations/2021-07-sismid/index.html",
    "href": "presentations/2021-07-sismid/index.html",
    "title": "Infectious Diseases, Immunology, and Within-Host Models",
    "section": "",
    "text": "The workshop website, is here. This is mainly for archival reasons. All the useful material is on a dedicated, stand-alone website called Simulation Modeling in Immunology."
  },
  {
    "objectID": "presentations/2021-04-good-projects/index.html",
    "href": "presentations/2021-04-good-projects/index.html",
    "title": "Coming up with good (research) projects",
    "section": "",
    "text": "These are slides for a talk/discussion I led with our graduate students discussing how to come up and organize projects in a good way. The talk covered both projects in general and academic projects in particular. The presentation slides are here."
  },
  {
    "objectID": "presentations/2021-02-cv-resume/index.html",
    "href": "presentations/2021-02-cv-resume/index.html",
    "title": "Tips for writing a (hopefully good) CV or resume",
    "section": "",
    "text": "These are slides for a talk/discussion I led with our graduate students discussing CVs and resumes and how to create good ones. The presentation slides are here."
  },
  {
    "objectID": "presentations/2021-01-idp/index.html",
    "href": "presentations/2021-01-idp/index.html",
    "title": "Introduction to Individual Development Plans and the AAAS myIDP",
    "section": "",
    "text": "These are slides for a talk/discussion I led with our graduate students discussing Individual Development Plans, and specifically the AAAS myIDP plan. This is the kick-off presentation/discussion for a semester-long IDP development project done as part of the seminar class. The presentation slides are here."
  },
  {
    "objectID": "presentations/2020-12-isv/index.html",
    "href": "presentations/2020-12-isv/index.html",
    "title": "The Role of Influenza Vaccines Dose Towards Homologous and Heterologous Protection",
    "section": "",
    "text": "These are slides for a talk discussing a recent project that investigated the impact of standard-dose versus high-dose Influenza Vaccines on immune responses. I gave the talk during the ISV Virtual Congress North America Influenza Vaccines Symposium. The presentation slides are here.\nThe organizers also recorded the event, the video recording can be found here."
  },
  {
    "objectID": "presentations/2020-11-fyos/index.html",
    "href": "presentations/2020-11-fyos/index.html",
    "title": "User-friendly software for simulation modeling of infectious diseases",
    "section": "",
    "text": "These are slides for a talk I gave to a group of undergraduate students as part of a First Year Odyssey Seminar class. I provide a very brief introduction to infectious disease modeling and show the DSAIDE software and a worked example. The presentation slides are here."
  },
  {
    "objectID": "presentations/2020-10-uga-pha/index.html",
    "href": "presentations/2020-10-uga-pha/index.html",
    "title": "An overview of COVID-19 Vaccines and modeling",
    "section": "",
    "text": "These are slides for a brief presentation on COVID-19 Vaccines and how to model their deployment. The presentation slides are here."
  },
  {
    "objectID": "presentations/2020-09-mjh/index.html",
    "href": "presentations/2020-09-mjh/index.html",
    "title": "Population-level patterns of COVID-19 and Flu: What should we expect?",
    "section": "",
    "text": "Short talk for an online webinar hosted by MJH life sciences on the topic Battling Dual Threats: Flu and COVID-19 Converge. My talk was titled Population-level patterns of COVID-19 and Flu: What should we expect? The presentation slides are here."
  },
  {
    "objectID": "presentations/2020-07-sismid/index.html",
    "href": "presentations/2020-07-sismid/index.html",
    "title": "Infectious Diseases, Immunology, and Within-Host Models",
    "section": "",
    "text": "The workshop website, is here. This is mainly for archival reasons. All the useful material is on a dedicated, stand-alone website called Simulation Modeling in Immunology."
  },
  {
    "objectID": "presentations/2020-03-tb-superspreaders/index.html",
    "href": "presentations/2020-03-tb-superspreaders/index.html",
    "title": "Tuberculosis Superspreading",
    "section": "",
    "text": "These are slides for a talk discussing a recent TB superspreader project I did, as well as some general background on the topic. I gave the talk to the Epidemiology in Action group (Dr.¬†Whalen‚Äôs research group) at UGA. The presentation slides are here."
  },
  {
    "objectID": "presentations/2020-01-your-brand/index.html",
    "href": "presentations/2020-01-your-brand/index.html",
    "title": "Building and curating your brand",
    "section": "",
    "text": "These are slides for a talk/discussion I led with our graduate students on ways to build and curate an online presence and overall personal brand. The presentation slides are here."
  },
  {
    "objectID": "presentations/2019-07-sismid/index.html",
    "href": "presentations/2019-07-sismid/index.html",
    "title": "Introduction to within-host modeling",
    "section": "",
    "text": "Preparation\n\nInstall R and (optional) Rstudio.\nInstall the DSAIRM R package and take a brief look. This website provides instructions.\n\n\n\nOutline\nI cover the following topics in this workshop (my co-teacher Paul Thomas covers the Immunology part):\n\nIntroduction to modeling\nSome example models\nHow to use simulation models\nSources of uncertainty\nTypes of models\nHow to build (good) models\nHow to assess modeling studies\n\n\n\nPresentation Slides\nAll pdf slides as zip file"
  },
  {
    "objectID": "presentations.html",
    "href": "presentations.html",
    "title": "Presentations",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nIntroduction to Infectious Disease Modeling\n\n\n\n\n\n\nWorkshop\n\n\nInfectious Disease\n\n\nResearch\n\n\n\n\n\n\nJul 10, 2019\n\n\nHangzhou, China\n\n\nZhejiang CDC training course\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2025-02-25-exercises-in-quarto/index.html",
    "href": "posts/2025-02-25-exercises-in-quarto/index.html",
    "title": "Quizzes and exercises for Quarto websites",
    "section": "",
    "text": "I am currently using Quarto for my course websites. Sometimes, it‚Äôs useful to include interactive exercises or quizzes. I was looking around to find something that fits my needs. Ideally, what I want is:\n\nEasy to set up and implement/use\nNo need to run server-side software\nWell maintained\nProgramming language idependent\nDefinitely support multiple choice questions, if also supports coding exercises/questions, even better.\n\nI figured I write down what I explored. Mostly for my future self who won‚Äôt be able to remember any of that in a few weeks 1. And maybe others find it useful too."
  },
  {
    "objectID": "posts/2025-02-25-exercises-in-quarto/index.html#shinylearnr",
    "href": "posts/2025-02-25-exercises-in-quarto/index.html#shinylearnr",
    "title": "Quizzes and exercises for Quarto websites",
    "section": "Shiny/learnr",
    "text": "Shiny/learnr\nIn the old days, there was nothing much for interactive web-based quizzes/exercises (in the R language space). Then came Shiny, which allowed interactive websites. Building on top of that, along came the learnr R package which allows nice implementation of exercises and quizzes on websites.\nA colleague and I developed a set of learnr/Shiny tutorials for our interactive Introductory Biostatistics Labs in R. We currently still use these labs as part of a course we regulary teach. (You can see the labs in action here.)\nThis setup works overall well. The main drawback is that one needs to deploy the content to a Shiny server. That can be costly and/or somewhat tricky to administer. Another current disadvantage with learnr is that development has slowed down (ceased?). The GitHub page shows open pull requests from 5 years ago, over 100 open issues, and only 3 commits in 2024. Posit (formerly RStudio) folks, who were the main developers of learnr, seem to have moved on to newer things (see below.)"
  },
  {
    "objectID": "posts/2025-02-25-exercises-in-quarto/index.html#other-options",
    "href": "posts/2025-02-25-exercises-in-quarto/index.html#other-options",
    "title": "Quizzes and exercises for Quarto websites",
    "section": "Other options",
    "text": "Other options\nThe R package webexercises provides some simple functionality to embed quizzes in websites. It can‚Äôt do coding exercises, but is nice for simple multi-choice/true-false and similar questions. The main potential drawbacks are that questions need to be written as R code, and that package development and maitenance doesn‚Äôt seem very active. The last commit was about 2 years ago, and there are several open GitHub PRs and Issues. If something were to break/not work (I‚Äôve noticed occasional iffy behaviour in my explorations), it‚Äôs unclear if things would be addressed by the maintainer2.\nSomething similar to webexercises is the checkdown R package. It has very similar functionality, and I have exactly the same robustness/maintenance concerns for this package that I have for webexercises. The prolific (and way more technically savy than me) Andrew Heiss uses checkdown for his R Studio Primer port. That makes me a bit confident that if I were to use this package and something went wrong, I could ask him for a hack/fix üòÅ.\nAnother exercise/quiz/exam focused package that has been around for a while is exams. They have a new package called exams2forms which allows integration of questions into websites. The whole package seems fairly full-featured and initially a bit intimidating, but after reading the tutorials, I realized it‚Äôs not that difficult. I experimented with it, and it seems to work well. The developers/maintainers are highly active, so my concerns regarding future support are minimal. I‚Äôm giving a quick worked example using exams below."
  },
  {
    "objectID": "posts/2025-02-25-exercises-in-quarto/index.html#quarto-based",
    "href": "posts/2025-02-25-exercises-in-quarto/index.html#quarto-based",
    "title": "Quizzes and exercises for Quarto websites",
    "section": "Quarto based",
    "text": "Quarto based\nQuarto allows individuals to write extensions that supply additional functionality. I‚Äôm aware of two that implement some form of quiz/exercise functionality. The first, quarto-quiz only works for the revealJS format, so is not suitable for html-based websites.\nThe second, naquiz can be included within html websites. Possible advantages of this package is that the quizzes are programming-language agnostic, and implementing simple questions is rather easy. The main hesitation I have is again robustness/maintenance. The package has a single developer/maintainer, the extension hasn‚Äôt been updated in over 2 years. There‚Äôs an open GitHub issue filed in Sept 2024 with what seems like a somewhat serious and simple to fix bug that has not been addressed. This makes me wonder as a user how likely any issues I might run into would be addressed."
  },
  {
    "objectID": "posts/2025-02-25-exercises-in-quarto/index.html#webr-based",
    "href": "posts/2025-02-25-exercises-in-quarto/index.html#webr-based",
    "title": "Quizzes and exercises for Quarto websites",
    "section": "webR based",
    "text": "webR based\nThere are several new options that make use of the new webR framework to allow one to run interactive coding exercises inside Quarto documents/websites, without needing a Shiny server on the backend.\nquarto-webr is one such option, quarto-live is an alternative, very similar option. As far as I can tell these two options provide roughly similar functionality3. They are meant to allow execution of code - and exercises based on code - being run in the browser without the need for an R/Shiny server. A related package, shinylive, provides somwhat similar functionality (R/Shiny without a server) but is not as Quarto focused.\nThe main drawback (for me) is that these packages do not - as far as I can tell - provide any user-friendly and simple options to implement non-coding exercises, such as simple multiple choice or true-false questions. Another, minor inconvenience is that they are R code based, but as long as the code can be hidden from the user, I‚Äôd be ok with that.\nIf someone were to implement an easy way to write non-coding quiz questions using either of those approaches, it would likely be ideal for me since then I could use the same approach for classes where I want to show/do coding exercises and for those where I only want to have interactive quiz functionality. I guess I basically want a re-implementation of learnr using the webR framework to allow deployment on static websites. Alas, that does not currently seem to exist4."
  },
  {
    "objectID": "posts/2025-02-25-exercises-in-quarto/index.html#footnotes",
    "href": "posts/2025-02-25-exercises-in-quarto/index.html#footnotes",
    "title": "Quizzes and exercises for Quarto websites",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nJust the other day I wanted to know how to customize Quarto callouts and a quick Google search turned up a helpful blogpost that seemed vaguely familiar üòÇ. Turns out I had the same question and figured it out about 2 years ago - I‚Äôm getting old üôÑ.‚Ü©Ô∏é\nI‚Äôm certainly not here to criticize any volunteer software development/maintenance work! I know from my own R package development that things like software maintenance are rarely rewarded bye one‚Äôs main employer and therefore often need to be low priority. I‚Äôm just making a comment as a user who is thinkinng about robustness/sustainability for my courses.‚Ü©Ô∏é\nWithout having actually tried each, that‚Äôs just my impression after reading through some of the documentation.‚Ü©Ô∏é\nIt seems to be possible to do it, but that approach looks too involved and complex to me.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2024-02-15-longitudinal-multilevel-bayes-5/index.html",
    "href": "posts/2024-02-15-longitudinal-multilevel-bayes-5/index.html",
    "title": "Bayesian analysis of longitudinal multilevel data - part 5",
    "section": "",
    "text": "This is a re-implementation of a prior model using cmdstanr and Stan code. It is a continuation of a prior series of posts. You should start at the beginning.\nHere is the Stan code for this example and this is the R script that runs everything."
  },
  {
    "objectID": "posts/2024-02-15-longitudinal-multilevel-bayes-5/index.html#footnotes",
    "href": "posts/2024-02-15-longitudinal-multilevel-bayes-5/index.html#footnotes",
    "title": "Bayesian analysis of longitudinal multilevel data - part 5",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe‚Äôll, it‚Äôs almost the same model, as I mentioned above some of the prior distributions got changed, but that did fortunately not impact our posterior estimates by much.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2023-06-13-custom-callouts-quarto/index.html",
    "href": "posts/2023-06-13-custom-callouts-quarto/index.html",
    "title": "How to make custom callouts for Quarto",
    "section": "",
    "text": "I wrote this post about 2 years ago. I since had forgotton about it. I recently needed to revisit the topic, re-discovered my own post, and realized that what I was writing here, namely that I was making custom callouts for Quarto, is not quite correct. I‚Äôm just defining styling for new divs that look somewhat like Quarto callouts, but I‚Äôm not using the ‚Äúcallout machinery‚Äù. For my updated thoughts and notes, see this new blog post."
  },
  {
    "objectID": "posts/2023-06-13-custom-callouts-quarto/index.html#the-setup",
    "href": "posts/2023-06-13-custom-callouts-quarto/index.html#the-setup",
    "title": "How to make custom callouts for Quarto",
    "section": "The setup",
    "text": "The setup\nQuarto uses either CSS or SCSS files for styling. CSS files do the heavy lifting for styling of websites. SCSS files are basically extensions that allow for additional features. This presentation by Sam Csik gives a nice non-technical introduction for CSS/SCSS styling with Quarto. The details aren‚Äôt too important and I certainly don‚Äôt fully understand CSS/SCSS. For our purpose, we could use CSS but since SCSS seems to be more flexible, and we can stick all CSS into that file, we might as well use that format. Start by creating an empty SCSS file (or find some template online). Let‚Äôs assume it‚Äôs called customstyle.scss and you placed it in the same folder as your _quarto.yml file. Then add this SCSS to _quarto.yml, e.g.¬†to look like this:\nformat:\n  html:\n    theme: \n      light: [matria,customstyle.scss]\n      dark: [slate,customstyle.scss]\nSee here for some more information."
  },
  {
    "objectID": "posts/2023-06-13-custom-callouts-quarto/index.html#the-styling",
    "href": "posts/2023-06-13-custom-callouts-quarto/index.html#the-styling",
    "title": "How to make custom callouts for Quarto",
    "section": "The styling",
    "text": "The styling\nNow comes the main part, adding instructions to the SCSS file to get the look we want. I‚Äôm basically combining bits and pieces from the sources I linked above. All the code below goes into the /*-- scss:rules --*/ section of your SCSS file. I‚Äôm naming my callout goals but you can use any name you want.\nFirst we specify the overall box with a rounded border.\n.goals {\n  margin: 1em 0;\n  border: 2px solid gray;\n  border-left: 10px solid teal; \n  border-radius: 10px;\n}\nThe next code block sets up the header/title section.\nNote that I needed to round the background box in the top right corner so it has the same shape as the overall box/border defined above.\nI‚Äôm also including a small icon here, following the blog post instructions by Desir√©e mentioned above. I got the icon from here. Depending on where you place the icon, you will have to adjust the file path inside the url() command.\n.goals-header {\n  margin-left: 0em;\n  padding-left:  60px;\n  padding-top:  5px;\n  padding-bottom: 0.1em;\n  color: black;\n  background-color: rgb(123, 173, 173);\n  border-top-right-radius: 10px;\n  font-size: 1em;\n  font-weight: bold;\n  background-size: 40px;\n  background-repeat: no-repeat;\n  background-position: 15px center;\n  background-image: url(\"goal.png\");\n}\nAnd the last bit of code styles the main text portion of the callout.\n.goals-container {\n  padding-top: 5px;\n  padding-left: 10px;\n  color: black;\n  background-color: white;\n  border-bottom-right-radius: 10px;\n}"
  },
  {
    "objectID": "posts/2023-06-13-custom-callouts-quarto/index.html#using-your-new-callout",
    "href": "posts/2023-06-13-custom-callouts-quarto/index.html#using-your-new-callout",
    "title": "How to make custom callouts for Quarto",
    "section": "Using your new callout",
    "text": "Using your new callout\nThis is fairly easy, you are using the same fenced div notation as for the built-in Quarto callouts shown above. The only additional part is that you need to nest them like so:\n:::{.goals}\n::::{.goals-header}\nLearning Goals\n::::\n::::{.goals-container}\n* Know how to make your own callouts.\n* Be able to mess with some SCSS/CSS styling.\n::::\n:::\nThe nesting is accomplished by giving each level up/down one more/fewer :.\nThis will produce the following.\n\n\nLearning Goals\n\n\n\nKnow how to make your own callouts.\nBe able to mess with some SCSS/CSS styling.\n\n\n\nI‚Äôm quite happy with this. It looks fairly similar to the Quarto built-in examples. It could use a bit more refinement and adjustment, but I think the overall setup is clear enough that you can take it from here üòÅ."
  },
  {
    "objectID": "posts/2023-06-13-custom-callouts-quarto/index.html#summary",
    "href": "posts/2023-06-13-custom-callouts-quarto/index.html#summary",
    "title": "How to make custom callouts for Quarto",
    "section": "Summary",
    "text": "Summary\nTo sum it all up, here is what you need to do:\n\nIf needed, create a new SCSS (or CSS) file and add the SCSS/CSS code chunks above to it.\nEdit your _quarto.yml file to include your new SCSS file.\nWrite .qmd documents using the fenced div (:::{}) approach shown above.\nKeep adding as many new styles as you want, and adjust them to however style you like."
  },
  {
    "objectID": "posts/2022-06-11-flowdiagramr-exploration/index.html",
    "href": "posts/2022-06-11-flowdiagramr-exploration/index.html",
    "title": "Exploring the flowdiagramr R package",
    "section": "",
    "text": "Motivation\nFor a while now, we‚Äôve been working on an R package called flowdiagramr which lets you create flow diagrams in R. The main reason we wrote this package is because while there are other R packages that can generate diagrams, I couldn‚Äôt find one that allowed me to create flow diagrams for the kind of compartmental simulation models that I often use in research and teaching. For instance right now, all model diagrams in my population-level and within-host modeling packages DSAIDE and DSAIRM are drawn by hand.\nWhile the main focus of flowdiagramr is to produce diagrams for those kind of dynamical process models with feedback loops, it can also be used to create other flow diagrams.\nAs of this writing, we are preparing for an initial CRAN submission. I‚Äôve been updating the package vignettes with hopefully useful examples. By chance, I recently came across a tweet by Nicola Rennie pointing to this blog post she wrote. The blog post shows how to make a nice flow diagram using ggplot2().\nI figured it would be fun to try and redo this using flowdiagramr. The following post shows that attempt.\n\n\nWho this is (not) for\nIf you are interested in making diagrams using R and are looking for an option that might do things that current packages such as diagrammR or others can‚Äôt do, you might want to follow along.\nThe flowdiagramr package has (in my biased opinion) fairly decent documentation and vignettes. I‚Äôm not going to repeat all that here. I basically assume that if you want to try this out, you go to the package website and skim through the first few vignettes to get an idea of how things work. Then come back here. Or first skim through this post, then if you want, go and explore the package vignettes more.\n\n\nGeneral setup\nThe following shows code and my comments mixed together. It‚Äôs often useful to go slow and type your own code, or copy and paste the code chunks below and execute one at a time. However, if you are in a hurry, you can find all the code shown below in this file.\nFirst we load all needed packages.\n\nlibrary('ggplot2')\nlibrary('flowdiagramr')\nlibrary('sysfonts') #for extra fonts\nlibrary('showtext') #for extra fonts\n\n\n\nDiagram setup\nNow we specify the model. Generally, a model consists of variables/compartments/boxes and flows/arrows connecting the boxes. This code chunk defines the variables/boxes, gives them names (used later), and provides the box grid layout as a matrix.\n\n# the unique internal labels, just abbreviations of the full names\nvariables = c(\"GL\",\"P\",\"PTH\",\"PTC\",\"PJR\",\n              \"C\",\"CTB\",\"CTS\",\"CJR\",\n              \"B\",\"BTH\",\"BTS\",\"BJR\",\n              \"BE\")\n# the eventual names for all boxes\n# we'll use that later to label the boxes\nvarnames = c(\"Goldilocks\",\n             \"Porridge\",\n             \"Too hot\",\"Too cold\",\"Just right\",\n             \"Chairs\",\n             \"Too big\", \"Too small\", \"Just right\",\n             \"Beds\",\n             \"Too hard\",\"Too soft\",\"Just right\",\n             \"Bears!\")\n# assigning the varnames the variable labels\n# needed later\nnames(varnames) &lt;- variables\n# a matrix specifying the locations on a matrix grid layout\n# this mimics the look of the original blog post\nvarlocations = matrix(data = c(\"\",\"GL\", \"\", \"\", \"\",\n                               \"\",\"P\", \"\", \"\", \"\",\n                               \"PTH\",\"PTC\", \"PJR\", \"\", \"\",\n                               \"\",   \"\",    \"C\", \"\", \"\",\n                               \"\",\"CTB\",  \"CTS\", \"CJR\", \"\",\n                               \"\",   \"\",   \"\",   \"B\", \"\",\n                               \"\",   \"\", \"BTH\", \"BTS\", \"BJR\",\n                               \"\",   \"\", \"\",     \"\", \"BE\"\n                      ), ncol = 5, byrow = TRUE)\n\nThe second component of every model are the flows between compartments/variables/boxes. Since flowdiagramr has as underlying logic the idea that flows occur between compartments, one needs to set up things as processes in such a way.\n\n# setting up the inflows and outflows (arrows) for each box\nflows = list( GL_flows = c(\"-k1*GL\"),\n              P_flows = c(\"k1*GL\",\"-k2*P\",\"-k3*P\",\"-k4*P\"),\n              PTH_flows = c(\"k2*P\"),\n              PTC_flows = c(\"k3*P\"),\n              PJR_flows = c(\"k4*P\",\"-k5*PJR\"),\n              C_flows = c(\"k5*PJR\",\"-k6*C\",\"-k7*C\",\"-k8*C\"),\n              CTB_flows = c(\"k6*C\"),\n              CTS_flows = c(\"k7*C\"),\n              CJR_flows = c(\"k8*C\",\"-k9*CJR\"),\n              B_flows = c(\"k9*CJR\",\"-k10*B\",\"-k11*B\",\"-k12*B\"),\n              BTH_flows = c(\"k10*B\"),\n              BTS_flows = c(\"k11*B\"),\n              BJR_flows = c(\"k12*B\",\"-k13*BJR\"),\n              BE_flows = c(\"k13*BJR\")\n)\n\n\n\nDiagram preparation\nThe first step for each flowdiagramr diagram is the preparation stage using the prepare_diagram() function. For that, one needs to supply the model as a list of variables and flows (boxes and arrows) and optional layout specifications.\n\n# model object\ngl_model = list(variables = variables, flows = flows)\n# model layout\nmodel_settings = list(varlocations=varlocations,\n                      varbox_x_size = 3)\n# prepare model\ngl_list = flowdiagramr::prepare_diagram(gl_model, model_settings)\n\n\n\nDiagram styling\nThe return from prepare_diagram is a list of data frames containing information about the variables and flows needed for plotting. One could go straight to making the diagram with make_diagram. But we already know the default doesn‚Äôt look like the blog post, therefore we apply some styling, which is done with the update_diagram function.\n\n#set colors that are similar to original blog post\nvarcolors = c(\"#b59dac\", rep(c(\"#D9AF6B\", \"#ACACAC\",\"#ACACAC\",\"#ACACAC\"),3), \"#b59dac\")\n# make them a named vector since that's required by update_diagram\nnames(varcolors) = variables\n# list of all style updates we want\ndiagram_settings = list(var_fill_color = varcolors,\n                        var_label_text = varnames,\n                        var_label_color = c(all =  \"#585c45\"),\n                        flow_show_label = c(all = FALSE),\n                        var_label_size = c(all = 4))\n\n# update the look\ngl_list2 &lt;- flowdiagramr::update_diagram(gl_list,diagram_settings)\n\n\n\nDiagram generation\nNow we can make and plot the diagram\n\n# create and plot diagram\ngl_diag &lt;- flowdiagramr::make_diagram(gl_list2)\nplot(gl_diag)\n\n\n\n\n\n\n\n\nThe result looks somewhat similar to the original, but not quite yet.\n\n\nMore diagram styling\nThe above is as far as we can get with flowdiagramr. The good news is that the created object is a a regular ggplot object and thus we can modify it further using ggplot2 code. A lot of this follows the original blog post, see there for details.\n\n# get different fonts\nsysfonts::font_add_google(name = \"Henny Penny\", family = \"henny\")\nshowtext::showtext_auto()\n\n# update the plot by adding ggplot2 commands\ngl_diag2 &lt;- gl_diag  +\n       labs(title = \"The Goldilocks Decision Tree\",\n            caption = \"Made with flowdiagramr:\\n https://andreashandel.github.io/flowdiagramr/\") +\n       theme_void() +\n        theme(plot.margin = unit(c(1, 1, 0.5, 1), \"cm\"),\n        legend.position = \"none\",\n        plot.background = element_rect(colour = \"#f2e4c1\", fill = \"#f2e4c1\"),\n        panel.background = element_rect(colour = \"#f2e4c1\", fill = \"#f2e4c1\"),\n        plot.title = element_text(family = \"henny\", hjust = 0, face = \"bold\",\n                                  size = 45, color = \"#585c45\",\n                                  margin = margin(t = 10, r = 0, b = 10, l = 0)),\n        plot.caption = element_text(family = \"henny\", hjust = 0,\n                                    size = 16, color = \"#585c45\",\n                                    margin = margin(t = 10)),\n       text = element_text(family = \"henny\")\n       )\n\nNow we can plot it again\n\nplot(gl_diag2)\n\n\n\n\n\n\n\n\nThis plot is fairly close. I‚Äôm skipping the addition of the image since that was done manually.\nOne aspect that isn‚Äôt working is having the font in the boxes be the the henry style. I did try to supply it by setting text = element_text(family = \"henny\"), but it seems that doesn‚Äôt work after one has already written the text. I‚Äôm not aware of a way to update text in an ggplot once it‚Äôs already placed (I wouldn‚Äôt be surprised if that‚Äôs possible, I just don‚Äôt know how.) Fortunately, we can fix that in a different way.\n\n\nEven more diagram styling\nLet‚Äôs see if we can fix the font issue. While setting the font through update_diagram is on the to-do list, as of this writing, this feature does not yet exist (contributions welcome üòÅ).\nWe can however ask flowdiagramr to write all the ggplot2 code that produces the diagram into a file, then modify ourselves.\nThis writes the full ggplot code to an R script file:\n\nwrite_diagram(gl_list2,filename = \"gl_diag.R\", always_overwrite = TRUE)\n\nNow we can open that file, find the part that creates the text for the boxes (that part starts with ‚Äúadd label text‚Äù) and simply add this part family = \"henny\" into the geom_text() statement. We‚Äôll also copy the commands from above to the end of the script to update the diagram_plot object. Then we‚Äôll save the updated file, source it, and thus have an updated diagram. Here is the updated script if it‚Äôs not clear what I‚Äôm doing.\n\nsource(\"gl_diag_mod.R\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n\n# plot new diagram\nplot(diagram_plot)\n\n\n\n\n\n\n\n\nSo that worked, very close to the original!\n\n\nA bit more exploration\nFor this specific diagram, using flowdiagramr is maybe not that much better than the original version. However, once one wants to include further features, including loops, flowdiagramr shows its strength. To illustrate this, let‚Äôs add a flows that shows that after Goldilock sits in the right chair, it induces hunger and she eats more porridge. (Yes, it‚Äôs silly, but I want to show how that can easily be implemented with flowdiagramr).\nTo show that, we update the flows as follows\n\n# need to define again since the above file overwrote it\nvariables = c(\"GL\",\"P\",\"PTH\",\"PTC\",\"PJR\",\n              \"C\",\"CTB\",\"CTS\",\"CJR\",\n              \"B\",\"BTH\",\"BTS\",\"BJR\",\n              \"BE\")\n\n# more complex flows\nflowsnew = list( GL_flows = c(\"-k1*GL\"),\n              P_flows = c(\"k1*GL\",\"-k2*P\",\"-k3*P\",\"-k4*P\",\"-kk1*P*CJR\"),\n              PTH_flows = c(\"k2*P\",\"k3a*PTC\"),\n              PTC_flows = c(\"k3*P\",\"-k3a*PTC\"),\n              PJR_flows = c(\"k4*P\",\"-k5*PJR\",\"kk1*P*CJR\"),\n              C_flows = c(\"k5*PJR\",\"-k6*C\",\"-k7*C\",\"-k8*C\"),\n              CTB_flows = c(\"k6*C\"),\n              CTS_flows = c(\"k7*C\"),\n              CJR_flows = c(\"k8*C\",\"-k9*CJR\"),\n              B_flows = c(\"k9*CJR\",\"-k10*B\",\"-k11*B\",\"-k12*B\"),\n              BTH_flows = c(\"k10*B\"),\n              BTS_flows = c(\"k11*B\"),\n              BJR_flows = c(\"k12*B\",\"-k13*BJR\"),\n              BE_flows = c(\"k13*BJR\")\n)\n\nThen we do the above steps to create the diagram:\n\n# model object\ngl_model_new = list(variables = variables, flows = flowsnew)\n# model layout\nmodel_settings = list(varlocations=varlocations,\n                      varbox_x_size = 3)\n# prepare model\ngl_list_new = flowdiagramr::prepare_diagram(gl_model_new, model_settings)\n# update the look\ngl_list_new2 &lt;- flowdiagramr::update_diagram(gl_list_new,diagram_settings)\n# create and plot diagram\ngl_diag_new &lt;- flowdiagramr::make_diagram(gl_list_new2)\nplot(gl_diag_new)\n\n\n\n\n\n\n\nggsave('featured.png',diagram_plot)\n\nSaving 7 x 5 in image\n\n\nI‚Äôm skipping the font adjustment and other parts, but of course you can apply that again. This ability to create feedback loops - which are very common in scientific process/simulation/mechanistic models - is the reason we built flowdiagramr. But as you saw, it can be used for all kinds of diagrams.\n\n\nFurther resources\nThe flowdiagramr website has a bunch of vignettes/tutorials with lots of examples and further use cases. If you are intrigued by this post, go check it out üòÑ.\n\n\nAcknowledgments\nDevelopment of flowdiagramr is a joint effort between myself and Andrew Tredennick, who did the majority of the actual coding work."
  },
  {
    "objectID": "posts/2021-03-21-simple-distill-website/index.html",
    "href": "posts/2021-03-21-simple-distill-website/index.html",
    "title": "Create a GitHub website with distill in less than 30 minutes",
    "section": "",
    "text": "Update 2022-09-25: Distill is in my opinion at this point outdated and replaced by Quarto. Going forward, I recommend using the Quarto framework. Here is a great starter tutorial. Most people have switched, so links on this page are likely outdated.\nThe following blog post provides step-by-step instructions for creating a website using R Markdown, the distill R package and GitHub."
  },
  {
    "objectID": "posts/2021-03-21-simple-distill-website/index.html#install-r-and-rstudio",
    "href": "posts/2021-03-21-simple-distill-website/index.html#install-r-and-rstudio",
    "title": "Create a GitHub website with distill in less than 30 minutes",
    "section": "Install R and RStudio",
    "text": "Install R and RStudio\nIf you don‚Äôt already have it on your computer, install R first. You can pick any mirror you like. If you already have R installed, make sure it is a fairly recent version. If yours is old, I suggest you install a new R version.\nOnce you have R installed, install the free version of RStudio Desktop. Again, make sure it‚Äôs a recent version. If you have an older version, you should update.\nInstalling R and RStudio should be fairly straightforward. If you want some more details or need instructions, see for instance content on this site (an online course I teach)."
  },
  {
    "objectID": "posts/2021-03-21-simple-distill-website/index.html#get-github-up-and-running",
    "href": "posts/2021-03-21-simple-distill-website/index.html#get-github-up-and-running",
    "title": "Create a GitHub website with distill in less than 30 minutes",
    "section": "Get GitHub up and running",
    "text": "Get GitHub up and running\nIf you are new to GitHub, you need to create an account. At some point, it would also be useful to learn more about what Git/GitHub is and how to use it, but for this purpose you actually don‚Äôt need to know much. If you want to read a bit about Git/GitHub, there‚Äôs again material on one of my course pages.."
  },
  {
    "objectID": "posts/2021-03-21-simple-distill-website/index.html#install-gitkraken-optional-but-assumed",
    "href": "posts/2021-03-21-simple-distill-website/index.html#install-gitkraken-optional-but-assumed",
    "title": "Create a GitHub website with distill in less than 30 minutes",
    "section": "Install Gitkraken (optional but assumed)",
    "text": "Install Gitkraken (optional but assumed)\nThere are many ways you can interact with Git/GitHub. I mostly use the fairly user-friendly and full-featured Gitkraken. You can get a basic version for free. If you are a student, you can get the Pro version through the Github developer pack, teachers can get it through the Github teacher toolbox. If you qualify for either, I highly recommend signing up. But you don‚Äôt need it for our purpose.\nOnce you have your GitHub account set up and Gitkraken installed, make sure you connect Gitkraken to your Github account.\nI assume for the rest of the post that you are using Gitkraken. If you have your own preferred Git/GitHub client (e.g.¬†the one that comes with RStudio), you can of course use that one too."
  },
  {
    "objectID": "posts/2021-03-21-simple-distill-website/index.html#gitkraken",
    "href": "posts/2021-03-21-simple-distill-website/index.html#gitkraken",
    "title": "Create a GitHub website with distill in less than 30 minutes",
    "section": "Gitkraken",
    "text": "Gitkraken\n\nOpen GitKraken, go to File -&gt; Init Repo -&gt; Local Only. Give it the name of your main website directory, e.g.¬†mywebsite. The Initialize In folder should be the folder above where you created the website, such that the Full path entry is the actual location of your website on your computer. For .gitignore Template you can choose R. The rest you can leave as is.\n\n\n\n\n\n\n\n\n\n\nOnce done, click Create repository. You should see a bunch of files ready for staging on the left. Click Stage all changes enter a commit message, commit. Then Click the Push button.\nAt this point, if you didn‚Äôt properly connect GitKraken and GitHub previously, you‚Äôll likely get an error message. Follow the error message and the connect Gitkraken to your Github account information to get it to work.\nYou‚Äôll see a message about no remote existing and if you want to add one. Say yes. A menu on the left should show up. Make sure the repository name is the same as your website folder name. Then click the green button. If things worked, your local website folder has been sent to GitHub and is ready to be turned into a website."
  },
  {
    "objectID": "posts/2021-03-21-simple-distill-website/index.html#github-website",
    "href": "posts/2021-03-21-simple-distill-website/index.html#github-website",
    "title": "Create a GitHub website with distill in less than 30 minutes",
    "section": "Github website",
    "text": "Github website\nFor the last step, go to your account on Github.com and find the repository for the website you just created. On the bar at the top, in the right corner there should be the Settings button. Click on it. Scroll down until you find the GitHub Pages section. Under Source, select Main and then choose /docs as the folder. Don‚Äôt choose a theme since we are using our own. Save those changes. If everything works (it could take a minute or so), your website is now live and public! Look right underneath the GitHub Pages section, there should be something like Your site is ready to be published at https://andreashandel.github.io/mywebsite/. Click on the link and your new site should show up.\nThat‚Äôs it. Now the hard part starts, creating good content üòÄ."
  },
  {
    "objectID": "posts/2020-10-15-customized-documents-with-Rmarkdown/index.html",
    "href": "posts/2020-10-15-customized-documents-with-Rmarkdown/index.html",
    "title": "Automatically generate personalized certificates and other documents with R",
    "section": "",
    "text": "Generating personalized documents is often useful. Since this is a very common task, programs like Word or similar software can do this. But I like to use R if I can. And the whole R Markdown system is perfectly suited for repeat generation of customized documents.\nI‚Äôm certainly not the first one to have the idea of using R, and in fact my initial approach is based on this prior post. Here, I describe a few ways of using R and R Markdown to auto-generate custom documents, and provide example code and explanations for anyone who might want to use this (including my future self)."
  },
  {
    "objectID": "posts/2020-10-15-customized-documents-with-Rmarkdown/index.html#latexpdf",
    "href": "posts/2020-10-15-customized-documents-with-Rmarkdown/index.html#latexpdf",
    "title": "Automatically generate personalized certificates and other documents with R",
    "section": "LaTeX/PDF",
    "text": "LaTeX/PDF\nFor this approach, you start with a template file that contains some LaTeX commands, including the placeholders that will get personalized. Here is code for an example file, you can get the file here. We‚Äôll look at the resulting output below.\n\n\n---\ntitle: \"\"\noutput: pdf_document\nclassoption: landscape\n---\n\n\\begin{center}\n\\includegraphics[height=4cm]{fig1.png}\n\\hfill\n\\includegraphics[height=4cm]{fig2.jpg} \\\\\n\\bigskip\n{\\Huge\\bf Certificate of Accomplishment } \\\\\n\\bigskip\n{\\Huge &lt;&lt;FIRSTNAME&gt;&gt; &lt;&lt;LASTNAME&gt;&gt; } \\\\\n\\bigskip\n{\\Large has successfully completed the course {\\it Generating Certificates with R} with a score of &lt;&lt;SCORE&gt;&gt; } \\\\\n\\bigskip\n{\\Huge Congratulations!}\n\\end{center}\n\n\nThis template places 2 images at the top, writes some text, and most importantly, adds some placeholder text that will be customized for each student with the script shown below. It doesn‚Äôt matter what placeholder text you write, as long as it‚Äôs unique such that when you do the replacement, only the instance you want replaced is actually changed. Enclosing with special characters such as &lt;&lt; &gt;&gt; is a good option for this, but it‚Äôs not required.\nThe advantages of the LaTeX/PDF approach are that 1) LaTeX allows you to do a lot of formatting and customization of the template so it looks exactly the way you want it, 2) the end product is a PDF file, which is easy to print or share with those for whom they are meant. The disadvantage is that you need to know a bit of LaTeX to set up your template, or at least be willing to spend some time with Google until you found all the snippets of commands you need for the layout you want to have."
  },
  {
    "objectID": "posts/2020-10-15-customized-documents-with-Rmarkdown/index.html#word",
    "href": "posts/2020-10-15-customized-documents-with-Rmarkdown/index.html#word",
    "title": "Automatically generate personalized certificates and other documents with R",
    "section": "Word",
    "text": "Word\nFor this approach, you start with a template file that contains commands that lead to a decent looking Word document. Again, it needs to include the placeholders that will get personalized. Here is code for an example file, you can get the file here. We‚Äôll look at the end result below.\n\n\n---\ntitle: \"\"\noutput: \n  word_document:\n    reference_docx: wordstyletemplate.docx\n---\n\n![Image](fig1.png)\n\n# Certificate of Accomplishment\n\n:::{custom-style=\"mystyle1\"}\n&lt;&lt;FIRSTNAME&gt;&gt;  &lt;&lt;LASTNAME&gt;&gt;\n:::\n\nhas successfully completed the course \"Generating Certificates with R\" with a score of &lt;&lt;SCORE&gt;&gt;\n\n:::{custom-style=\"mystyle2\"}\nCongratulations!\n:::\n\n```{r fig2, echo=FALSE, out.width=\"50%\"}\nknitr::include_graphics(\"fig2.jpg\")\n```\n\n\nNote that by default, going from R Markdown to Word doesn‚Äôt give you much ability to apply formatting. However, it is possible to do a decent amount of formatting using a word style template. I have another blog post which describes this approach, and I‚Äôm using it here.\nEven with the word style formatting, some things can‚Äôt be controlled well. Placement and sizing of figures is the main problem, no matter if you include the figures with basic Markdown commands or use the include_graphics() function. You‚Äôll see the problem if you try to run this example (code below). As such, for something that includes figures, using the LaTeX/PDF workflow seems almost always better. A scenario where the Word setup might be useful is if you want to produce customized letters. The one main advantage of a Word output (in addition to not having to figuring out LaTeX commands) is that the output can be further edited if needed."
  },
  {
    "objectID": "posts/2020-10-15-customized-documents-with-Rmarkdown/index.html#other-options",
    "href": "posts/2020-10-15-customized-documents-with-Rmarkdown/index.html#other-options",
    "title": "Automatically generate personalized certificates and other documents with R",
    "section": "Other options",
    "text": "Other options\nI believe the PDF or Word outputs are best for most instances, but occasionally another format might be needed. You can use this overall approach to generate other outputs, for instance the standard R Markdown html output, or different versions of presentation slides (e.g.¬†ioslides, Beamer, Powerpoint), etc. In principle, any R Markdown output format should work. You just need to alter your template file accordingly."
  },
  {
    "objectID": "posts/2020-05-29-automate-conflict-of-interest-form/index.html",
    "href": "posts/2020-05-29-automate-conflict-of-interest-form/index.html",
    "title": "Generating a conflict of interest form automatically",
    "section": "",
    "text": "Overview\nWhile most of my grant proposals go to the NIH, which thankfully does not require an arcane conflict of interest (COI) document, I am sometimes part of a grant proposal that goes to the NSF or another agency that require a COI. Instead of just asking for any actual conflicts of interest, these documents ask one to list every co-author in the last N years, which is fairly stupid these days when most papers in the biomedical sciences have many co-authors. I hope the agencies get rid of this in my opinion pointless document soon. Until then, I have to do it.\nI don‚Äôt want to retrieve all my co-authors and fill the form by hand. In a previous post, I showed how one can use the bibliometrix R package to do an analysis of a set of publications. Among other things, this approach returns all co-authors, which I will use here to make the COI table almost completely automated.\nThe RMarkdown/Quarto file to run this analysis is here.\n\n\nRequired packages\n\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(flextable)\n#remotes::install_github('massimoaria/bibliometrix')\nlibrary(bibliometrix)\n\n\n\nLoading data\nAs explained in a previous post, the currently best way to get all my papers is to download them from NIH‚Äôs ‚ÄúMy Bibliography‚Äù and export it in MEDLINE format. Then read in the file with the code below.\n\n#read bib file, turn file of references into data frame\npubs &lt;- bibliometrix::convert2df(\"medline.txt\", dbsource=\"pubmed\",format=\"pubmed\") \n\n\nConverting your pubmed collection into a bibliographic dataframe\n\nDone!\n\n\nGenerating affiliation field tag AU_UN from C1:  Done!\n\n\nEach row of the data frame created by the convert2df function is a publication, the columns contain information for each publication. For a list of what each column variable codes for, see the bibliometrix website.\n\n\nGetting the right time period\nThis specific funding agency I‚Äôm currently writing a COI for (NIFA) requires co-authors of the last 3 years, so let‚Äôs get them. I don‚Äôt know if they mean 3 full years. I‚Äôm doing this mid-2020, so to be on safe side, I go back to 2017.\n\nperiod_start = 2017\npubs_new = pubs[pubs$PY&gt;=period_start,]\n\nI need the full names of the authors. They are stored for each publication in the AF field. This is the only information I need for the COI form. I pull it out, then do a bit of processing to get it in the right shape, then remove duplicates and sort.\n\nallauthors = paste0(pubs_new$AF,collapse = \";\") #merge all authors into one vector\nallauthors2 = unlist(strsplit(allauthors, split =\";\"))\nauthors = sort(unique(allauthors2)) #split vector of authors, get unique authors\n\nNote that I originally did the above steps using biblioAnalysis(pubs_new). However, this function/approach broke in a recent version of the package, and I realized that I can just use a few base R commands to get what I need, which is the approach shown above. If you use the biblioAnalysis() function, the Authors are in the Authors field of the returned object.\n\n\nGetting a table of co-authors\nHere is the full table of my co-authors in the specified time period. I made a tibble that looks similar to what the COI document requires.\n\n#removing the 1st one since that's me\nauthortable = dplyr::tibble(Name = authors, \n                            \"Co-Author\" = 'x', \n                            Collaborator = '', \n                            'Advisees/Advisors' = '', \n                            'Other ‚Äì Specify Nature' = '')\n\nFinally, I‚Äôm using the flextable package to make a decent looking table and save it to a word document.\n\nft &lt;- flextable::flextable(authortable)\nflextable::autofit(ft)\n\nNameCo-AuthorCollaboratorAdvisees/AdvisorsOther ‚Äì Specify NatureAHMED, HASANxALIKHAN, MALIHA AxAMANNA, IAN JxANTIA, ALICExANTIA, RUSTOMxBOOM, W HENRYxBULUSHEVA, IRINAxCARLSON, NICHOLE ExCASTELLANOS, M ExCASTELLANOS, MARIAxCHAKRABURTY, SRIJITAxCHEN, ENFUxCHENG, WEIxCOATES, P TOBYxCROFT, NATHAN PxDALE, ARIELLA PERRYxDENHOLM, J TxDOBBIN, KEVINxDUDEK, NADINE LxEBELL, MARKxEBELL, MARK HxEGGENHUIZEN, PETER JxFOREHAND, RONALDxFUGGER, LARSxGAN, POH YxGARCIA-SASTRE, ADOLFOxGREGERSEN, JON WxGUAN, JINGxHALLORAN, M ELIZABETHxHANDEL, AxHANDEL, ANDREASxHECKMAN, TIMOTHY GxHOLDSWORTH, STEPHEN RxHOLT, STEPHEN GxHOUBEN, R M G JxHUANG, HAODIxHUDSON, BILLY GxHUO, XIANGxHUYNH, MEGANxJOLOBA, MOSES LxKAKAIRE, RxKIRIMUNDA, SxKITCHING, A RICHARDxKIWANUKA, NxLA GRUTA, NICOLE LxLI, CHANGWEIxLI, CHAOxLI, YANxLING, FENGxLOH, KHAI LxLONGINI, IRA MxMALONE, LASHAUNDA LxMANICASSAMY, BALAJIxMARTINEZ, LxMARTINEZ, LEONARDOxMCBRYDE, E SxMCKAY, BRIANxMOORE, JAMES RxMU, LANxOOI, JOSHUA DxPAWELEK, KASIA AxPETERSEN, JANxPOWER, DAVID AxPURCELL, ANTHONY WxQUACH, TxQUINN, FREDERICK DxRAGONNET, RxRAMARATHINAM, SRI HxREID, HUGH HxROSSJOHN, JAMIExSETTE, ALESSANDROxSHEN, YExSIDNEY, JOHNxSLIFKA, MARKxSNG, XAVIER Y XxSTEIN, CATHERINE MxSUMNER, TxTAN, YU HxTHOMAS, PAUL GxTRAUER, J MxTSCHARKE, DAVID CxWAKIM, LINDA MxWANG, XIAOXIAOxWATSON, KATHERINE AxWHALEN, C CxWHALEN, CHRISTOPHER CxWILLETT, ZOE JxWOLDU, HxWOLDU, HENOKxWU, TINGxZALWANGO, SxZALWANGO, SARAHxZARNITSYNA, VERONIKAxZARNITSYNA, VERONIKA IxZHU, LIMEIx\n\nflextable::save_as_docx(\"my table\" = ft, path = \"COItable.docx\")\n\nI notice a few duplicates in the table that need to be removed. Of course I also need to remove myself. And for some, the full name doesn‚Äôt show. I need to fill in a few of the other columns and potentially add a few individuals who were not captured. So it‚Äôs not fully automated, but I can copy this table into the COI statement and the remaining edits are still annoying but not that terrible.\n\n\nDiscussion\nThese kinds of COI documents that ask for all co-authors are in my opinion antiquated and should go away. In the meantime using a somewhat automated approach makes the problem not too bad. I will have to make a few manual adjustments to the table, but overall it‚Äôs not too bad. I‚Äôm still glad that NIH does not require this."
  },
  {
    "objectID": "posts/2020-02-02-publications-analysis-2/index.html",
    "href": "posts/2020-02-02-publications-analysis-2/index.html",
    "title": "Using R to analyze publications - part 2",
    "section": "",
    "text": "Overview\nI needed some information on all my publications for ‚Äúbean counting‚Äù purposes related to preparing my promotion materials. In the past, I also needed similar information for NSF grant applications.\nInstead of doing things by hand, there are nicer/faster ways using R. in part 1, I did a few things using the scholar package. While some parts worked nicely, I encountered 2 problems. First, since my Google Scholar record lists items other than peer-reviewed journal articles, they show up in the analysis and need to be cleaned out. Second, Google Scholar doesn‚Äôt like automated queries through the API and is quick to block, at which point things don‚Äôt work anymore.\nTo get around these issues, I decided to give a different R package a try, namely bibliometrix. The workflow is somewhat different.\n\n\nRequired packages\n\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(bibliometrix)\n\n\n\nLoading data\nOld: I keep all references to my published papers in a BibTeX file, managed through Zotero/Jabref. I know this file is clean and correct. I‚Äôm loading it here for processing. If you don‚Äôt have such a file, make one using your favorite reference manager. Or create it through a saved search on a bibliographic database, as explained on the bibliometrix website.\nNew: In the current version of bibliometrix, reading in my bibtex file failed. A fairly good alternative is to go to your NIH ‚ÄúMy Bibliography‚Äù (which anyone with NIH funding needs to have anyway) and export it in MEDLINE format. Then read in the file with the code below. As of the time of writing this, it requires the Github version of bibliometrix.\n\n#read bib file, turn file of references into data frame\npubs &lt;- bibliometrix::convert2df(\"medline.txt\", dbsource=\"pubmed\",format=\"pubmed\") \n\n\nConverting your pubmed collection into a bibliographic dataframe\n\nDone!\n\n\nGenerating affiliation field tag AU_UN from C1:  Done!\n\n\nEach row of the data frame created by the convert2df function is a publication, the columns contain information for each publication. For a list of what each column variable codes for, see the bibliometrix documentation.\n\n\nAnalyzing 2 time periods\nFor my purpose, I want to analyze 2 different time periods and compare them. Therefore, I split the data frame containing publications, then run the analysis on each.\n\n#get all pubs for an author (or multiple)\nperiod_1_start = 2009\nperiod_2_start = 2015\n#here I want to separately look at publications in the 2 time periods I defined above\npubs_old &lt;- data.frame(pubs) %&gt;% dplyr::filter((PY&gt;=period_1_start & PY&lt;period_2_start ))\npubs_new &lt;- data.frame(pubs) %&gt;% dplyr::filter(PY&gt;=period_2_start)\nres_old &lt;- bibliometrix::biblioAnalysis(pubs_old, sep = \";\") #perform analysis\nres_new &lt;- bibliometrix::biblioAnalysis(pubs_new, sep = \";\") #perform analysis\n\n\n\nGeneral information\nThe summary functions provide a lot of information in a fairly readable format. I apply them here to both time periods so I can compare.\nTime period 1\n\nsummary(res_old, k = 10)\n\n\n\nMAIN INFORMATION ABOUT DATA\n\n Timespan                              2009 : 2014 \n Sources (Journals, Books, etc)        12 \n Documents                             19 \n Annual Growth Rate %                  3.71 \n Document Average Age                  12.3 \n Average citations per doc             0 \n Average citations per year per doc    0 \n References                            1 \n \nDOCUMENT TYPES                     \n clinical trial;journal article;research support, non-u.s. gov't                                               1 \n comparative study;journal article;research support, n.i.h., extramural;research support, non-u.s. gov't                                               1 \n journal article                                               2 \n journal article;research support, n.i.h., extramural                                               5 \n journal article;research support, n.i.h., extramural;research support, non-u.s. gov't                                               3 \n journal article;research support, n.i.h., extramural;research support, non-u.s. gov't;research support, u.s. gov't, non-p.h.s.                               1 \n journal article;research support, n.i.h., extramural;research support, non-u.s. gov't;research support, u.s. gov't, non-p.h.s.;review;systematic review      1 \n journal article;research support, non-u.s. gov't                                               3 \n journal article;research support, non-u.s. gov't;research support, u.s. gov't, non-p.h.s.;research support, u.s. gov't, p.h.s.                               1 \n journal article;review                                               1 \n \nDOCUMENT CONTENTS\n Keywords Plus (ID)                    148 \n Author's Keywords (DE)                148 \n \nAUTHORS\n Authors                               45 \n Author Appearances                    80 \n Authors of single-authored docs       0 \n \nAUTHORS COLLABORATION\n Single-authored docs                  0 \n Documents per Author                  0.422 \n Co-Authors per Doc                    4.21 \n International co-authorships %        0 \n \n\nAnnual Scientific Production\n\n Year    Articles\n    2009        5\n    2010        2\n    2011        1\n    2012        3\n    2013        2\n    2014        6\n\nAnnual Percentage Growth Rate 3.71 \n\n\nMost Productive Authors\n\n   Authors        Articles Authors        Articles Fractionalized\n1   HANDEL A            19  HANDEL A                         5.55\n2   ANTIA R              6  ANTIA R                          1.78\n3   DOHERTY PC           3  LONGINI IM JR                    1.00\n4   LA GRUTA NL          3  DOHERTY PC                       0.56\n5   LONGINI IM JR        3  LA GRUTA NL                      0.56\n6   THOMAS PG            3  THOMAS PG                        0.56\n7   PILYUGIN SS          2  BEAUCHEMIN CA                    0.50\n8   ROHANI P             2  LI Y                             0.50\n9   STALLKNECHT D        2  ROHANI P                         0.50\n10  TURNER SJ            2  ROZEN DE                         0.50\n\n\nTop manuscripts per citations\n\n                              Paper                                   DOI TC TCperYear NTC\n1  ZHENG N, 2014, PLOS ONE                   10.1371/JOURNAL.PONE.0105721  0         0 NaN\n2  HANDEL A, 2014, PROC BIOL SCI             10.1098/RSPB.2013.3051        0         0 NaN\n3  NGUYEN TH, 2014, J IMMUNOL                10.4049/JIMMUNOL.1303147      0         0 NaN\n4  LI Y, 2014, J THEOR BIOL                  10.1016/J.JTBI.2014.01.008    0         0 NaN\n5  HANDEL A, 2014, J R SOC INTERFACE         10.1098/RSIF.2013.1083        0         0 NaN\n6  CUKALAC T, 2014, PROC NATL ACAD SCI U S A 10.1073/PNAS.1323736111       0         0 NaN\n7  HANDEL A, 2013, PLOS COMPUT BIOL          10.1371/JOURNAL.PCBI.1002989  0         0 NaN\n8  THOMAS PG, 2013, PROC NATL ACAD SCI U S A 10.1073/PNAS.1222149110       0         0 NaN\n9  JACKWOOD MW, 2012, INFECT GENET EVOL      10.1016/J.MEEGID.2012.05.003  0         0 NaN\n10 DESAI R, 2012, CLIN INFECT DIS            10.1093/CID/CIS372            0         0 NaN\n\n\nCorresponding Author's Countries\n\n    Country Articles   Freq SCP MCP MCP_Ratio\n1 USA             14 0.7778  11   3     0.214\n2 AUSTRALIA        3 0.1667   2   1     0.333\n3 CANADA           1 0.0556   1   0     0.000\n\n\nSCP: Single Country Publications\n\nMCP: Multiple Country Publications\n\n\nTotal Citations per Country\n\n  Country      Total Citations Average Article Citations\n1    AUSTRALIA               0                         0\n2    CANADA                  0                         0\n3    USA                     0                         0\n\n\nMost Relevant Sources\n\n                                                                                                          Sources       \n1  JOURNAL OF THE ROYAL SOCIETY INTERFACE                                                                               \n2  JOURNAL OF THEORETICAL BIOLOGY                                                                                       \n3  JOURNAL OF IMMUNOLOGY (BALTIMORE MD. : 1950)                                                                         \n4  PLOS ONE                                                                                                             \n5  PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF AMERICA                                      \n6  BMC EVOLUTIONARY BIOLOGY                                                                                             \n7  BMC PUBLIC HEALTH                                                                                                    \n8  CLINICAL INFECTIOUS DISEASES : AN OFFICIAL PUBLICATION OF THE INFECTIOUS DISEASES SOCIETY OF AMERICA                 \n9  EPIDEMICS                                                                                                            \n10 INFECTION GENETICS AND EVOLUTION : JOURNAL OF MOLECULAR EPIDEMIOLOGY AND EVOLUTIONARY GENETICS IN INFECTIOUS DISEASES\n   Articles\n1         3\n2         3\n3         2\n4         2\n5         2\n6         1\n7         1\n8         1\n9         1\n10        1\n\n\nMost Relevant Keywords\n\n   Author Keywords (DE)      Articles Keywords-Plus (ID)     Articles\n1      HUMANS                      13  HUMANS                      13\n2      MODELS  BIOLOGICAL           8  MODELS  BIOLOGICAL           8\n3      ANIMALS                      7  ANIMALS                      7\n4      COMPUTER SIMULATION          5  COMPUTER SIMULATION          5\n5      BIOLOGICAL EVOLUTION         4  BIOLOGICAL EVOLUTION         4\n6      MODELS  IMMUNOLOGICAL        4  MODELS  IMMUNOLOGICAL        4\n7      FEMALE                       3  FEMALE                       3\n8      MICE                         3  MICE                         3\n9      MUTATION                     3  MUTATION                     3\n10     AMINO ACID SEQUENCE          2  AMINO ACID SEQUENCE          2\n\n\nTime period 2\n\nsummary(res_new, k = 10)\n\n\n\nMAIN INFORMATION ABOUT DATA\n\n Timespan                              2015 : 2020 \n Sources (Journals, Books, etc)        22 \n Documents                             29 \n Annual Growth Rate %                  -9.71 \n Document Average Age                  6.72 \n Average citations per doc             0 \n Average citations per year per doc    0 \n References                            1 \n \nDOCUMENT TYPES                     \n comparative study;journal article;research support, n.i.h., extramural;research support, non-u.s. gov't                             1 \n journal article                                               7 \n journal article;multicenter study;research support, n.i.h., extramural                                               1 \n journal article;research support, n.i.h., extramural                                               5 \n journal article;research support, n.i.h., extramural;research support, non-u.s. gov't                                               5 \n journal article;research support, n.i.h., extramural;research support, non-u.s. gov't;review                                        1 \n journal article;research support, n.i.h., extramural;research support, u.s. gov't, non-p.h.s.                                       1 \n journal article;research support, n.i.h., extramural;research support, u.s. gov't, non-p.h.s.;review                                1 \n journal article;research support, non-u.s. gov't                                               4 \n journal article;research support, non-u.s. gov't;research support, n.i.h., extramural                                               1 \n journal article;research support, non-u.s. gov't;research support, u.s. gov't, non-p.h.s.;research support, n.i.h., extramural      1 \n letter                                               1 \n \nDOCUMENT CONTENTS\n Keywords Plus (ID)                    198 \n Author's Keywords (DE)                198 \n \nAUTHORS\n Authors                               209 \n Author Appearances                    332 \n Authors of single-authored docs       1 \n \nAUTHORS COLLABORATION\n Single-authored docs                  1 \n Documents per Author                  0.139 \n Co-Authors per Doc                    11.4 \n International co-authorships %        41.38 \n \n\nAnnual Scientific Production\n\n Year    Articles\n    2015        5\n    2016        7\n    2017        3\n    2018        6\n    2019        5\n    2020        3\n\nAnnual Percentage Growth Rate -9.71 \n\n\nMost Productive Authors\n\n   Authors        Articles Authors        Articles Fractionalized\n1     HANDEL A          29    HANDEL A                      5.494\n2     WHALEN CC          7    ANTIA R                       0.810\n3     ANTIA R            5    SHEN Y                        0.723\n4     MARTINEZ L         5    WHALEN CC                     0.651\n5     SHEN Y             5    MCKAY B                       0.629\n6     LA GRUTA NL        4    EBELL MH                      0.571\n7     MCKAY B            4    THOMAS PG                     0.571\n8     THOMAS PG          4    LA GRUTA NL                   0.540\n9     ZALWANGO S         4    ROHANI P                      0.500\n10    DENHOLM JT         3    MARTINEZ L                    0.485\n\n\nTop manuscripts per citations\n\n                                                      Paper                                    DOI TC TCperYear NTC\n1  MCKAY B, 2020, PROC BIOL SCI                                      10.1098/RSPB.2020.0496         0         0 NaN\n2  MOORE JR, 2020, BULL MATH BIOL                                    10.1007/S11538-020-00711-4     0         0 NaN\n3  HANDEL A, 2020, NAT REV IMMUNOL                                   10.1038/S41577-019-0235-3      0         0 NaN\n4  MARTINEZ L, 2019, J INFECT DIS                                    10.1093/INFDIS/JIZ328          0         0 NaN\n5  WU T, 2019, NAT COMMUN                                            10.1038/S41467-019-10661-8     0         0 NaN\n6  MCKAY B, 2019, PLOS ONE                                           10.1371/JOURNAL.PONE.0217219   0         0 NaN\n7  DALE AP, 2019, J AM BOARD FAM MED SOCIOLOGICAL METHODS & RESEARCH 10.3122/JABFM.2019.02.180183   0         0 NaN\n8  WOLDU H, 2019, J APPL STAT                                        10.1080/02664763.2018.1470231  0         0 NaN\n9  HANDEL A, 2018, PLOS COMPUT BIOL                                  10.1371/JOURNAL.PCBI.1006505   0         0 NaN\n10 CASTELLANOS ME, 2018, INT J TUBERC LUNG DIS                       10.5588/IJTLD.18.0073          0         0 NaN\n\n\nCorresponding Author's Countries\n\n    Country Articles  Freq SCP MCP MCP_Ratio\n1 USA             16 0.696   7   9     0.562\n2 AUSTRALIA        5 0.217   1   4     0.800\n3 GEORGIA          2 0.087   0   2     1.000\n\n\nSCP: Single Country Publications\n\nMCP: Multiple Country Publications\n\n\nTotal Citations per Country\n\n  Country      Total Citations Average Article Citations\n1    AUSTRALIA               0                         0\n2    GEORGIA                 0                         0\n3    USA                     0                         0\n\n\nMost Relevant Sources\n\n                                                                                                                                       Sources       \n1  PLOS ONE                                                                                                                                          \n2  PLOS COMPUTATIONAL BIOLOGY                                                                                                                        \n3  THE INTERNATIONAL JOURNAL OF TUBERCULOSIS AND LUNG DISEASE : THE OFFICIAL JOURNAL OF THE INTERNATIONAL UNION AGAINST TUBERCULOSIS AND LUNG DISEASE\n4  THE LANCET. GLOBAL HEALTH                                                                                                                         \n5  THE LANCET. RESPIRATORY MEDICINE                                                                                                                  \n6  AMERICAN JOURNAL OF RESPIRATORY AND CRITICAL CARE MEDICINE                                                                                        \n7  BMC INFECTIOUS DISEASES                                                                                                                           \n8  BULLETIN OF MATHEMATICAL BIOLOGY                                                                                                                  \n9  ELIFE                                                                                                                                             \n10 EPIDEMICS                                                                                                                                         \n   Articles\n1         4\n2         2\n3         2\n4         2\n5         2\n6         1\n7         1\n8         1\n9         1\n10        1\n\n\nMost Relevant Keywords\n\n          Author Keywords (DE)      Articles           Keywords-Plus (ID)     Articles\n1  HUMANS                                 23 HUMANS                                 23\n2  ANIMALS                                 8 ANIMALS                                 8\n3  FEMALE                                  7 FEMALE                                  7\n4  MALE                                    7 MALE                                    7\n5  MICE                                    6 MICE                                    6\n6  ADULT                                   5 ADULT                                   5\n7  CHILD                                   5 CHILD                                   5\n8  ADOLESCENT                              4 ADOLESCENT                              4\n9  ANTIVIRAL AGENTS/THERAPEUTIC USE        4 ANTIVIRAL AGENTS/THERAPEUTIC USE        4\n10 CHILD  PRESCHOOL                        4 CHILD  PRESCHOOL                        4\n\n\nNote that some values are reported as NA, e.g.¬†the citations. Depending on which source you got the original data from, that information might be included or not. In my case, it is not.\n\n\nGetting a table of co-authors\nThis can be useful for NSF applications. For reasons nobody understands, that agency still asks for a list of all co-authors. An insane request in the age of modern science. If one wanted to do that, the following gives a table.\nUpdate: I have since created a short blog post describing how to do just that part in a bit more detail. It has a few additional components that might be useful, if interested check it out here.\nHere is the full table of my co-authors in the first period dataset.\n\n#removing the 1st one since that's me\nauthortable = data.frame(res_old$Authors[-1])\ncolnames(authortable) = c('Co-author name', 'Number of publications')\nknitr::kable(authortable)\n\n\n\n\nCo-author name\nNumber of publications\n\n\n\n\nANTIA R\n6\n\n\nDOHERTY PC\n3\n\n\nLA GRUTA NL\n3\n\n\nLONGINI IM JR\n3\n\n\nTHOMAS PG\n3\n\n\nPILYUGIN SS\n2\n\n\nROHANI P\n2\n\n\nSTALLKNECHT D\n2\n\n\nTURNER SJ\n2\n\n\nAKIN V\n1\n\n\nBEAUCHEMIN CA\n1\n\n\nBIRD NL\n1\n\n\nBROWN J\n1\n\n\nCHADDERTON J\n1\n\n\nCUKALAC T\n1\n\n\nDESAI R\n1\n\n\nDICKEY BW\n1\n\n\nFUNG IC\n1\n\n\nHALL AJ\n1\n\n\nHALL D\n1\n\n\nHEMBREE CD\n1\n\n\nJACKWOOD MW\n1\n\n\nKEDZIERSKA K\n1\n\n\nKJER-NIELSEN L\n1\n\n\nKOTSIMBOS TC\n1\n\n\nLEBARBENCHON C\n1\n\n\nLEON JS\n1\n\n\nLEVIN BR\n1\n\n\nLI Y\n1\n\n\nLOPMAN B\n1\n\n\nMARGOLIS E\n1\n\n\nMATTHEWS JE\n1\n\n\nMCDONALD S\n1\n\n\nMIFSUD NA\n1\n\n\nMOFFAT JM\n1\n\n\nNGUYEN TH\n1\n\n\nPARASHAR UD\n1\n\n\nPELLICCI DG\n1\n\n\nROWNTREE LC\n1\n\n\nROZEN DE\n1\n\n\nWHALEN CC\n1\n\n\nYATES A\n1\n\n\nZARNITSYNA V\n1\n\n\nZHENG N\n1\n\n\n\n\n\nSince I have many more co-authors in the second period, I‚Äôm not printing a table with all, instead I‚Äôm just doing those with whom I have more than 2 joint publications.\n\n#removing the 1st one since that's me\nauthortable = data.frame(res_new$Authors[-1])\nauthortable &lt;- authortable %&gt;% dplyr::filter(Freq&gt;2)\ncolnames(authortable) = c('Co-author name', 'Number of publications')\nknitr::kable(authortable)\n\n\n\n\nCo-author name\nNumber of publications\n\n\n\n\nWHALEN CC\n7\n\n\nANTIA R\n5\n\n\nMARTINEZ L\n5\n\n\nSHEN Y\n5\n\n\nLA GRUTA NL\n4\n\n\nMCKAY B\n4\n\n\nTHOMAS PG\n4\n\n\nZALWANGO S\n4\n\n\nDENHOLM JT\n3\n\n\nEBELL M\n3\n\n\nMCBRYDE ES\n3\n\n\nSUMNER T\n3\n\n\nTRAUER JM\n3\n\n\n\n\n\n\n\nMaking a table of journals\nIt can be useful to get a list of all journals in which you published. I‚Äôm doing this here for the second time period. With just the bibliometrix package, I can get a list of publications and how often I have published in each.\n\njournaltable = data.frame(res_new$Sources)\n#knitr::kable(journaltable) #uncomment this to print the table\n\nAs mentioned in part 1 of this series of posts, the Impact Factor feature from the scholar package doesn‚Äôt work anymore. I‚Äôm leaving the old code in there in case it ever comes back. For now, there is no Impact Factor information. (I haven‚Äôt tried to figure out if there is another way to get it.)\nIt might also be nice to get some journal metrics, such as impact factors. While this is possible with the scholar package, the bibliometrix package doesn‚Äôt have it.\nHowever, the scholar package doesn‚Äôt really get that data from Google Scholar, instead it has an internal spreadsheet/table with impact factors (according to the documentation, taken - probably not fully legally - from some spreadsheet posted on ResearchGate). We can thus access those impact factors stored in the scholar package without having to connect to Google Scholar. As long as the journal names stored in the scholar package are close to the ones we have here, we might get matches.\n\n#library(scholar)\n#ifvalues = scholar::get_impactfactor(journaltable[,1], max.distance = 0.1)\n#journaltable = cbind(journaltable, ifvalues$ImpactFactor)\n#colnames(journaltable) = c('Journal','Number of Pubs','Impact Factor')\ncolnames(journaltable) = c('Journal','Number of Pubs')\nknitr::kable(journaltable)\n\n\n\n\n\n\n\n\nJournal\nNumber of Pubs\n\n\n\n\nPLOS ONE\n4\n\n\nPLOS COMPUTATIONAL BIOLOGY\n2\n\n\nTHE INTERNATIONAL JOURNAL OF TUBERCULOSIS AND LUNG DISEASE : THE OFFICIAL JOURNAL OF THE INTERNATIONAL UNION AGAINST TUBERCULOSIS AND LUNG DISEASE\n2\n\n\nTHE LANCET. GLOBAL HEALTH\n2\n\n\nTHE LANCET. RESPIRATORY MEDICINE\n2\n\n\nAMERICAN JOURNAL OF RESPIRATORY AND CRITICAL CARE MEDICINE\n1\n\n\nBMC INFECTIOUS DISEASES\n1\n\n\nBULLETIN OF MATHEMATICAL BIOLOGY\n1\n\n\nELIFE\n1\n\n\nEPIDEMICS\n1\n\n\nEPIDEMIOLOGY AND INFECTION\n1\n\n\nFRONTIERS IN IMMUNOLOGY\n1\n\n\nJOURNAL OF APPLIED STATISTICS\n1\n\n\nJOURNAL OF THE AMERICAN BOARD OF FAMILY MEDICINE : JABFM\n1\n\n\nNATURE\n1\n\n\nNATURE COMMUNICATIONS\n1\n\n\nNATURE REVIEWS. IMMUNOLOGY\n1\n\n\nPHILOSOPHICAL TRANSACTIONS OF THE ROYAL SOCIETY OF LONDON. SERIES B BIOLOGICAL SCIENCES\n1\n\n\nPLOS BIOLOGY\n1\n\n\nPROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF AMERICA\n1\n\n\nPROCEEDINGS. BIOLOGICAL SCIENCES\n1\n\n\nTHE JOURNAL OF INFECTIOUS DISEASES\n1\n\n\n\n\n\nOk that worked somewhat. It couldn‚Äôt find several journals. The reported IF seem reasonable. But since I don‚Äôt know what year those IF are from, and if the rest is fully reliable, I would take this with a grain of salt.\n\n\nDiscussion\nThe bibliometrix package doesn‚Äôt suffer from the problems that I encountered in part 1 of this post when I tried the scholar package (and Google Scholar). The downside is that I can‚Äôt get some of the information, e.g.¬†my annual citations. So it seems there is not (yet) a comprehensive solution, and using both packages seems best.\nA larger overall problem is that a lot of this information is controlled by corporations (Google, Elsevier, Clarivate Analytics, etc.), which might or might not allow R packages and individual users (who don‚Äôt subscribe to their offerings) to access certain information. As such, R packages accessing this information will need to adjust to whatever the companies allow."
  },
  {
    "objectID": "posts/2020-01-08-resources-website/index.html",
    "href": "posts/2020-01-08-resources-website/index.html",
    "title": "Research and teaching resources website",
    "section": "",
    "text": "I have been collecting lists of links related to research, teaching and (academic) career topics in a Github repository. I finally got around to doing some cleanup. I turned the repository into a small website. You can find this site here."
  },
  {
    "objectID": "posts/2020-01-02-blogdown-website-1/index.html",
    "href": "posts/2020-01-02-blogdown-website-1/index.html",
    "title": "Creating a website in less than 1 hour - part 1",
    "section": "",
    "text": "Update: Hugo and the Academic (now Wowchemy) kept changing rapidly and became, in my opinion, rather complex and confusing. Once Quarto arrived, I switched this website over (see my post on that here), and I think it‚Äôs much easier to use. I therefore don‚Äôt really recommend the setup described here anymore, though it might still be useful for some.\nThe following are step-by-step instructions for creating your own website using blogdown, Hugo and Netlify.\nIn part 2, you will learn how to add GitHub to your workflow to make things even more automated and efficient."
  },
  {
    "objectID": "posts/2020-01-02-blogdown-website-1/index.html#get-a-netlify-account",
    "href": "posts/2020-01-02-blogdown-website-1/index.html#get-a-netlify-account",
    "title": "Creating a website in less than 1 hour - part 1",
    "section": "Get a Netlify account",
    "text": "Get a Netlify account\nGo to the Netlify website and sign up for an account. Follow the sign-up steps to set up your account."
  },
  {
    "objectID": "posts/2020-01-02-blogdown-website-1/index.html#install-r-and-rstudio",
    "href": "posts/2020-01-02-blogdown-website-1/index.html#install-r-and-rstudio",
    "title": "Creating a website in less than 1 hour - part 1",
    "section": "Install R and RStudio",
    "text": "Install R and RStudio\nIf you don‚Äôt already have it on your computer, install R first. You can pick any mirror you like. If you already have R installed, make sure it is a fairly recent version. If yours is old, I suggest you update (install a new R version).\nOnce you have R installed, install the free version of RStudio Desktop. Again, make sure it‚Äôs a recent version. If you have an older version of RStudio, you should update.\nInstalling R and RStudio should be fairly straightforward. If you want some more details or need instructions, see this page (which is part of an online course I teach)."
  },
  {
    "objectID": "posts/2020-01-02-blogdown-website-1/index.html#playing-with-widgets",
    "href": "posts/2020-01-02-blogdown-website-1/index.html#playing-with-widgets",
    "title": "Creating a website in less than 1 hour - part 1",
    "section": "Playing with widgets",
    "text": "Playing with widgets\nAll (or at least most) content goes into the content folder and its subfolders. Content is generally written in (R)Markdown. For this tutorial, you don‚Äôt need to know much Rmarkdown, but at some point you will have to learn it. Fortunately, (R)Markdown is very easy to learn. See e.g.¬†the RMarkdown section on this page, check out this nice interactive tutorial or this cheatsheet.\nThe Wowchemy/Academic theme, and many other modern websites, use a layout that employs widgets, which are components of a website that are individually formatted and styled. On the demo site you just created, you see many different sections, each is a widget. Which widgets you want is controlled by files in the /content/home/ folder. Go into that folder (from within RStudio) and open the demo.md file. You will see a bunch of text. Some commands are between +++ signs, this is called the TOML (or YAML if it‚Äôs 3 ---) header. These are instructions for the layout. The text below is what is actually shown on the site.\nAs you stare at the content of the file, you might recognize that it corresponds to the 2nd block of the demo website with the dark bubble content. Let‚Äôs say you don‚Äôt want this particular widget on your home page. The easiest way is to set active = false. (You can also delete the whole file if you are sure you don‚Äôt want it). Do that. You should notice 2 things. In the bottom-left RStudio window (the R console) you should see a bit of code showing that the website was re-built and updated in real time. If you re-load the page in your browser, the widget and its content should be gone. You can try turning off other sections of the main page using this approach."
  },
  {
    "objectID": "posts/2020-01-02-blogdown-website-1/index.html#making-things-personal",
    "href": "posts/2020-01-02-blogdown-website-1/index.html#making-things-personal",
    "title": "Creating a website in less than 1 hour - part 1",
    "section": "Making things personal",
    "text": "Making things personal\nNow let‚Äôs open the about.md file. You will notice that it doesn‚Äôt really contain any content. Instead, it pulls the content from another location, namely content in the authors folder. Go into /content/authors/admin/ and open the _index.md file. There you see the content that is displayed on the main page. Modify it with your personal information. Once you save your changes, you should see the website automatically being rebuilt. If you have, add a picture of yourself and replace the current avatar.jpg file. (Your picture needs to have that name). Also, while not required, you might want to rename the folder from admin to your name. Make sure this corresponds to the name you list in the _index.md file."
  },
  {
    "objectID": "posts/2020-01-02-blogdown-website-1/index.html#cleaning-up-for-now",
    "href": "posts/2020-01-02-blogdown-website-1/index.html#cleaning-up-for-now",
    "title": "Creating a website in less than 1 hour - part 1",
    "section": "Cleaning up for now",
    "text": "Cleaning up for now\nLet‚Äôs turn off all other parts of the main site apart from the about widget. The easiest way is to remove all files apart from the index.md and about.md files. You probably don‚Äôt want to completely delete them (since you might want to use them later), thus I recommend you move them to some other folder on your computer. For instance you can make a folder called myfiles as a subfolder of your website folder and move the files into that folder.\nIf all of this worked, there should be a main page containing only a brief description of yourself."
  },
  {
    "objectID": "posts/2020-01-02-blogdown-website-1/index.html#a-word-on-error-messages.",
    "href": "posts/2020-01-02-blogdown-website-1/index.html#a-word-on-error-messages.",
    "title": "Creating a website in less than 1 hour - part 1",
    "section": "A word on error messages.",
    "text": "A word on error messages.\nIf you make some changes that break the site, you will see an error message in the R console and the site won‚Äôt re-compile until you fix the problem. You often have to be careful to write things exactly as specified, and often with the right indentation, etc. Some fiddling is at times required. If you are stuck and think you broke it too badly, you can either look in the Wowchemy theme documentation or go into the themes/starter-academic/exampleSite folder and find the corresponding file you are editing there and see how it needs to look."
  },
  {
    "objectID": "posts/2020-01-02-blogdown-website-1/index.html#config.yaml",
    "href": "posts/2020-01-02-blogdown-website-1/index.html#config.yaml",
    "title": "Creating a website in less than 1 hour - part 1",
    "section": "config.yaml",
    "text": "config.yaml\nI mentioned above that TOML/YAML is a language/structure used by Hugo to control all kinds of things. Most files have some TOML/YAML part, a few files are nothing but TOML and control a lot of settings. Let‚Äôs look at the most important files. The first one is config.yaml (sometimes also called config.toml) located in the main website folder. Find and open it. You will see that it lists as title Academic. Change that to e.g.¬†Website of YOURNAME. You will see this change show up on the main site. You can try what happens if you write something in the copyright section. The rest of this file doesn‚Äôt need further editing for now."
  },
  {
    "objectID": "posts/2020-01-02-blogdown-website-1/index.html#menus.toml",
    "href": "posts/2020-01-02-blogdown-website-1/index.html#menus.toml",
    "title": "Creating a website in less than 1 hour - part 1",
    "section": "menus.toml",
    "text": "menus.toml\nLet‚Äôs go into the config/_default/ folder and open the menus.toml file. You‚Äôll see that those correspond to the menu buttons on the main page. Most of them don‚Äôt work since we removed the widgets. For now, let‚Äôs go ahead and disable (by placing # symbols in front) all entries apart from the Posts block of text."
  },
  {
    "objectID": "posts/2020-01-02-blogdown-website-1/index.html#params.toml",
    "href": "posts/2020-01-02-blogdown-website-1/index.html#params.toml",
    "title": "Creating a website in less than 1 hour - part 1",
    "section": "params.toml",
    "text": "params.toml\nOpen params.toml. This file lets you specify and control a lot of things. Try setting a different theme. Then read through the other parts. We won‚Äôt change them for now, but you might want to come back to them."
  },
  {
    "objectID": "posts/2020-01-02-blogdown-website-1/index.html#more-edits",
    "href": "posts/2020-01-02-blogdown-website-1/index.html#more-edits",
    "title": "Creating a website in less than 1 hour - part 1",
    "section": "More edits",
    "text": "More edits\nCongratulations, you have built a website and wrote a blog post! Of course there is a lot you can do next. Write more posts, look at all the different elements/widgets you can turn on and modify, etc. As mentioned, the Academic theme has a lot of features. If you like what you see, continue exploring. If you think you want something simpler, check out other Hugo themes until you find one you like, then customize it. A lot of things are very similar across all Hugo themes (e.g.¬†the TOML/YAML bits and the folder structure), but some details differ, so it‚Äôs good to pick a theme before you really start customizing it.\nBut for now, we‚Äôll leave it at this. There is one more crucial step missing though."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nReview Tracking\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 13, 2024\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nGlobal musical diversity is largely independent of linguistic and genetic histories\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 20, 2024\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Ordinal models\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 9, 2023\n\n\n20 min\n\n\n\n\n\n\n\n\n\n\n\n\nJournal Impact Factor & Google Scholar\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nTalks in Shandong and Korea\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 10, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nKinbank: A database of Kinship terminology media watch\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 7, 2023\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nCultural Evolution Society ACE ECR Awards\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 16, 2023\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nData Analysis Plan (DAP) Workshop 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay 11, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nABC Canberra: Language Diversity and Endangerment\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 26, 2023\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nSYNAPSE seminar\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 10, 2023\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nAncestral state reconstruction with INLA\n\n\n\n\n\n\n\n\n\n\n\nMar 3, 2023\n\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\n\nTo make a reference group, or to not make a reference group, that is the question this blog deals with.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 15, 2023\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nPhylogenetic Signal in INLA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJan 19, 2023\n\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\n\nThe Global Jukebox\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2022\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nFunding in Australia and New Zealand\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOct 11, 2022\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nPhylogenetic Signal in INLA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJul 15, 2022\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nThe problem with Galton‚Äôs problem\n\n\n\n\n\n\nOpinion\n\n\n\n\n\n\n\n\n\nMar 6, 2020\n\n\n3 min\n\n\n\n\n\n\n\n\n\n\n\n\nbayestraitr\n\n\n\n\n\n\nSoftware\n\n\n\n\n\n\n\n\n\nJun 6, 2019\n\n\n2 min\n\n\n\n\n\n\n\n\n\n\n\n\nCultured Scene\n\n\n\n\n\nResearch coverage in Cultured Scene\n\n\n\n\n\nDec 4, 2018\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nSookias, Passmore, and Atkinson (2018) - News coverage\n\n\n\n\n\nNews coverage of the Sookias, Passmore, and Atkinson paper\n\n\n\n\n\nApr 11, 2018\n\n\n3 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Sam Passmore - Brief professional bio",
    "section": "",
    "text": "Current position\nI currently work across two projects, both based within the College of Asia and the Pacific, at the Australian National Univeristy (ANU). I am a Post-doctoral Research Fellow with the Evolution of Cultural Diversity Initiative, and I am also Post-doctoral Research Fellow on the Pacific Creoles Project, working with the Australian Defence Science and Technology Group (DTSG).\n\n\nResearch focus\nMy research focuses on the application statistical inference and computational modelling to anthropological or linguistic data and questions. I use data driven approaches to analyze data from the humanities, with a focus on building a quantitative understanding of diversity. I have used a range statistical approaches to do this, including phylogenetics, machine-learning, and autoregressive regression models. I am particularly interested in the relationship between statistics and their assumptions and how they relate to the mechanics of the real world. Generally, I take an evolutionary approach to my research, which could also be labelled as cultural evolution.\nDetails of the projects I am currently working on can be seen in the projects page\n\n\nPrevious positions\nIn 2021, I worked for Associate Professor Patrick Savage as part of the CompMusic Lab, at Keio University in Fujisawa, Japan, and in 2020 I breifly worked for Russell Gray, at the Max Planck Institute for the Science of Human History, in Jena, Germany.\nBetween 2016 and 2020, I completed my PhD with Professor Fiona Jordan, as part of the excd.lab in the Department of Anthropology and Archaeology, at the University of Bristol, United Kingdom, and in 2014 I completed a Masters degree with Professor Quentin Atkinson in the School of Psychology, at the University of Auckland, New Zealand.\nBetween my Master‚Äôs and PhD, I was a Data Scientist at Pingar International."
  },
  {
    "objectID": "posts/2019-10-02-tidy-tuesday-exploration/index.html",
    "href": "posts/2019-10-02-tidy-tuesday-exploration/index.html",
    "title": "Analysis of pizza restaurants",
    "section": "",
    "text": "This analysis was performed as part of an exercise for my Modern Applied Data Analysis course.\nWhen I taught the course in fall 2019, one of the weekly assignments for the students was to participate in TidyTuesday. I did the exercise as well, this is my product. You can get the R Markdown/Quarto file to re-run the analysis here.\n\nIntroduction\nIf you are not familiar with TidyTuesday, you can take a quick look at the TidyTuesday section on this page.\nThis week‚Äôs data was all about Pizza. More on the data is here.\n\n\nLoading packages\n\nlibrary('readr')\nlibrary('ggplot2')\nlibrary(\"dplyr\")\nlibrary(\"cowplot\")\nlibrary(\"plotly\")\nlibrary(\"forcats\")\nlibrary(\"geosphere\")\nlibrary(\"emoji\")\n\n\n\nData loading\nLoad date following TidyTueday instructions.\n\npizza_jared &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-10-01/pizza_jared.csv\")\npizza_barstool &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-10-01/pizza_barstool.csv\")\npizza_datafiniti &lt;- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-10-01/pizza_datafiniti.csv\")\n\n\n\nAnalysis Ideas\nSee the TidyTuesday website for a codebook. These are 3 datasets. Looks like the 1st dataset is ratings of pizza places through some (online?) survey/poll, the 2nd dataset again has ratings of pizza places from various sources, and the 3rd dataset seems to have fairly overlapping information to the 2nd dataset.\nNote: When I looked at the website, the codebook for the 3rd dataset seemed mislabeled. Might be fixed by now.\nPossibly interesting questions I can think of:\n\nFor a given pizza restaurant, how do the different ratings/scores agree or differ?\nAre more expensive restaurants overall rated higher?\nIs there some systematic dependence of rating on location? Do restaurants located in a certain area in general get rated higher/lower compared to others?\n\nI think those are good enough questions to figure out, let‚Äôs see how far we get.\n\n\nInitial data exploration\nStart with a quick renaming and general check.\n\n#saves typing\nd1 &lt;- pizza_jared \nd2 &lt;- pizza_barstool \nd3 &lt;- pizza_datafiniti \nglimpse(d1)\n\nRows: 375\nColumns: 9\n$ polla_qid   &lt;dbl&gt; 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5‚Ä¶\n$ answer      &lt;chr&gt; \"Excellent\", \"Good\", \"Average\", \"Poor\", \"Never Again\", \"Ex‚Ä¶\n$ votes       &lt;dbl&gt; 0, 6, 4, 1, 2, 1, 1, 3, 1, 1, 4, 2, 1, 1, 0, 1, 1, 0, 3, 0‚Ä¶\n$ pollq_id    &lt;dbl&gt; 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5‚Ä¶\n$ question    &lt;chr&gt; \"How was Pizza Mercato?\", \"How was Pizza Mercato?\", \"How w‚Ä¶\n$ place       &lt;chr&gt; \"Pizza Mercato\", \"Pizza Mercato\", \"Pizza Mercato\", \"Pizza ‚Ä¶\n$ time        &lt;dbl&gt; 1344361527, 1344361527, 1344361527, 1344361527, 1344361527‚Ä¶\n$ total_votes &lt;dbl&gt; 13, 13, 13, 13, 13, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 5, 5, 5,‚Ä¶\n$ percent     &lt;dbl&gt; 0.0000, 0.4615, 0.3077, 0.0769, 0.1538, 0.1429, 0.1429, 0.‚Ä¶\n\nglimpse(d2)\n\nRows: 463\nColumns: 22\n$ name                                 &lt;chr&gt; \"Pugsley's Pizza\", \"Williamsburg ‚Ä¶\n$ address1                             &lt;chr&gt; \"590 E 191st St\", \"265 Union Ave\"‚Ä¶\n$ city                                 &lt;chr&gt; \"Bronx\", \"Brooklyn\", \"New York\", ‚Ä¶\n$ zip                                  &lt;dbl&gt; 10458, 11211, 10017, 10036, 10003‚Ä¶\n$ country                              &lt;chr&gt; \"US\", \"US\", \"US\", \"US\", \"US\", \"US‚Ä¶\n$ latitude                             &lt;dbl&gt; 40.85877, 40.70808, 40.75370, 40.‚Ä¶\n$ longitude                            &lt;dbl&gt; -73.88484, -73.95090, -73.97411, ‚Ä¶\n$ price_level                          &lt;dbl&gt; 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 1, ‚Ä¶\n$ provider_rating                      &lt;dbl&gt; 4.5, 3.0, 4.0, 4.0, 3.0, 3.5, 3.0‚Ä¶\n$ provider_review_count                &lt;dbl&gt; 121, 281, 118, 1055, 143, 28, 95,‚Ä¶\n$ review_stats_all_average_score       &lt;dbl&gt; 8.011111, 7.774074, 5.666667, 5.6‚Ä¶\n$ review_stats_all_count               &lt;dbl&gt; 27, 27, 9, 2, 1, 4, 5, 17, 14, 6,‚Ä¶\n$ review_stats_all_total_score         &lt;dbl&gt; 216.3, 209.9, 51.0, 11.2, 7.1, 16‚Ä¶\n$ review_stats_community_average_score &lt;dbl&gt; 7.992000, 7.742308, 5.762500, 0.0‚Ä¶\n$ review_stats_community_count         &lt;dbl&gt; 25, 26, 8, 0, 0, 3, 4, 16, 13, 4,‚Ä¶\n$ review_stats_community_total_score   &lt;dbl&gt; 199.8, 201.3, 46.1, 0.0, 0.0, 13.‚Ä¶\n$ review_stats_critic_average_score    &lt;dbl&gt; 8.8, 0.0, 0.0, 4.3, 0.0, 0.0, 0.0‚Ä¶\n$ review_stats_critic_count            &lt;dbl&gt; 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, ‚Ä¶\n$ review_stats_critic_total_score      &lt;dbl&gt; 8.8, 0.0, 0.0, 4.3, 0.0, 0.0, 0.0‚Ä¶\n$ review_stats_dave_average_score      &lt;dbl&gt; 7.7, 8.6, 4.9, 6.9, 7.1, 3.2, 6.1‚Ä¶\n$ review_stats_dave_count              &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n$ review_stats_dave_total_score        &lt;dbl&gt; 7.7, 8.6, 4.9, 6.9, 7.1, 3.2, 6.1‚Ä¶\n\nglimpse(d3)\n\nRows: 10,000\nColumns: 10\n$ name            &lt;chr&gt; \"Shotgun Dans Pizza\", \"Sauce Pizza Wine\", \"Mios Pizzer‚Ä¶\n$ address         &lt;chr&gt; \"4203 E Kiehl Ave\", \"25 E Camelback Rd\", \"3703 Paxton ‚Ä¶\n$ city            &lt;chr&gt; \"Sherwood\", \"Phoenix\", \"Cincinnati\", \"Madison Heights\"‚Ä¶\n$ country         &lt;chr&gt; \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", ‚Ä¶\n$ province        &lt;chr&gt; \"AR\", \"AZ\", \"OH\", \"MI\", \"MD\", \"MD\", \"CA\", \"CA\", \"FL\", ‚Ä¶\n$ latitude        &lt;dbl&gt; 34.83230, 33.50927, 39.14488, 42.51667, 39.28663, 39.2‚Ä¶\n$ longitude       &lt;dbl&gt; -92.18380, -112.07304, -84.43269, -83.10663, -76.56698‚Ä¶\n$ categories      &lt;chr&gt; \"Pizza,Restaurant,American restaurants,Pizza Place,Res‚Ä¶\n$ price_range_min &lt;dbl&gt; 0, 0, 0, 25, 0, 0, 0, 0, 0, 0, 25, 25, 25, 25, 0, 0, 0‚Ä¶\n$ price_range_max &lt;dbl&gt; 25, 25, 25, 40, 25, 25, 25, 25, 25, 25, 40, 40, 40, 40‚Ä¶\n\n\nThe first question I have is if the pizza places in the 3 datasets are the same or at least if there is decent overlap. If not, then one can‚Äôt combine the data.\n\nd1names = unique(d1$place)\nd2names = unique(d2$name)\nd3names = unique(d3$name)\nsum(d1names %in% d2names) #check how many restaurants in d1 are also in d2. Note that this assumes exact spelling.\n\n[1] 22\n\nsum(d1names %in% d3names) #check how many restaurants in d1 are also in d2. Note that this assumes exact spelling.\n\n[1] 9\n\nsum(d2names %in% d3names)\n\n[1] 66\n\n\n22 restaurants out of 56 in dataset 1 are also in dataset 2. Only 9 overlap between dataset 1 and 3. 66 are shared between datasets 2 and 3.\nThe last dataset has no ratings, and if I look at the overlap of dataset 1 and 2, I only get a few observations. So I think for now I‚Äôll focus on dataset 2 and see if I can address the 3 questions I posed above with just that dataset. Maybe I‚Äôll have ideas for the other 2 datasets as I go along (would be a shame to not use them.)\n\n\nRatings agreement analysis\nOk, I‚Äôll focus on dataset 2 now and look closer at the scores/rating. From the codebook, it‚Äôs not quite clear to me what the different scores and counts in dataset 2 actually mean, so let‚Äôs look closer to try and figure that out.\nFrom the glimpse function above, I can‚Äôt see much of a difference between average and total score. Let‚Äôs look at that. Here are a few plots comparing the different score-related variables.\n\nplot(d2$review_stats_community_total_score,d2$review_stats_community_average_score)\n\n\n\n\n\n\n\nplot(d2$review_stats_community_total_score - d2$review_stats_community_average_score* d2$review_stats_community_count)\n\n\n\n\n\n\n\nplot(d2$review_stats_critic_total_score-d2$review_stats_critic_average_score)\n\n\n\n\n\n\n\nplot(d2$review_stats_dave_total_score-d2$review_stats_dave_average_score)\n\n\n\n\n\n\n\nplot(d2$review_stats_all_total_score- (d2$review_stats_community_total_score+d2$review_stats_critic_total_score+d2$review_stats_dave_total_score))  \n\n\n\n\n\n\n\n\nOk, so based on the plots above, and a few other things I tried, it seems that average score is total score divided by number of counts, and the all score is just the sum of dave, critic and community.\nSo to address my first question, I‚Äôll look at correlations between average scores for the 3 types of reviewers, namely dave, critic and community.\nHowever, while playing around with the data in the last section, I noticed a problem. Look at the counts for say critics and the average score.\n\ntable(d2$review_stats_critic_count)\n\n\n  0   1   5 \n401  61   1 \n\ntable(d2$review_stats_critic_average_score)\n\n\n   0    4  4.3  4.5  4.8    5  5.1  5.4  5.5  5.7  5.8  5.9 5.96    6  6.2 6.31 \n 401    1    1    1    1    3    1    1    1    1    1    1    1    1    1    1 \n 6.5  6.6  6.7 6.76  6.8  6.9    7  7.2  7.3  7.4  7.6 7.76  7.8  7.9    8  8.1 \n   3    1    1    1    2    1    5    2    2    1    1    1    2    1    4    2 \n 8.5  8.7  8.8    9  9.3  9.4  9.8   10   11 \n   3    1    1    1    1    2    1    4    1 \n\n\nA lot of restaurants did not get reviewed by critics, and the score is coded as 0. That‚Äôs a problem since if we take averages and such, it will mess up things. This should really be counted as NA. So let‚Äôs create new average scores such that any restaurant with no visits/reviews gets an NA as score.\n\nd2 &lt;- d2 %&gt;% mutate( comm_score = ifelse(review_stats_community_count == 0 ,NA,review_stats_community_average_score)) %&gt;%\n             mutate( crit_score = ifelse(review_stats_critic_count == 0 ,NA,review_stats_critic_average_score)) %&gt;%\n             mutate( dave_score = ifelse(review_stats_dave_count == 0 ,NA,review_stats_dave_average_score)) \n\nNow let‚Äôs plot the 3.\n\np1 &lt;- d2 %&gt;% ggplot(aes(x=comm_score, y = crit_score)) + geom_point() + geom_smooth(method = \"lm\")\np2 &lt;- d2 %&gt;% ggplot(aes(x=comm_score, y = dave_score)) + geom_point() + geom_smooth(method = \"lm\")\np3 &lt;- d2 %&gt;% ggplot(aes(x=crit_score, y = dave_score)) + geom_point() + geom_smooth(method = \"lm\")\ncowplot::plot_grid(p1, p2, p3, labels = c('A', 'B','C'), label_size = 12)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nLooks like there is some agreement between Dave, the critics and the community on the ratings of various pizza places, though there is a good bit of variation.\nI think it would be fun to be able to click on specific points to see for a given score which restaurant that is. For instance I‚Äôm curious which restaurant has a close to zero score from both the community and Dave (bottom left of plot B).\nI think that can be done with plotly, let‚Äôs google it.\nOk, figured it out. This re-creates the 3 scatterplots from above and when one moves over the dots, it shows restaurant name.\n\nplotly::plot_ly(d2, x = ~comm_score, y = ~crit_score, type = \"scatter\", mode = 'markers', text = ~paste('Restaurant: ', d2$name))\n\n\n\n\nplotly::plot_ly(d2, x = ~comm_score, y = ~dave_score, type = \"scatter\", mode = 'markers', text = ~paste('Restaurant: ', d2$name))\n\n\n\n\nplotly::plot_ly(d2, x = ~crit_score, y = ~dave_score, type = \"scatter\", mode = 'markers', text = ~paste('Restaurant: ', d2$name))\n\n\n\n\n\nSo apparently the lousy restaurant that got a 1 from the community and almost 0 from Dave is called Amtrak. I‚Äôm wondering if that refers to pizza on Amtrak trains? Just for the heck of it and because I‚Äôm curious, let‚Äôs look at that entry.\n\nd2 %&gt;% filter(name == \"Amtrak\") %&gt;% knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\naddress1\ncity\nzip\ncountry\nlatitude\nlongitude\nprice_level\nprovider_rating\nprovider_review_count\nreview_stats_all_average_score\nreview_stats_all_count\nreview_stats_all_total_score\nreview_stats_community_average_score\nreview_stats_community_count\nreview_stats_community_total_score\nreview_stats_critic_average_score\nreview_stats_critic_count\nreview_stats_critic_total_score\nreview_stats_dave_average_score\nreview_stats_dave_count\nreview_stats_dave_total_score\ncomm_score\ncrit_score\ndave_score\n\n\n\n\nAmtrak\n234 W 31st St\nNew York\n10001\nUS\n40.74965\n-73.9934\n0\n3\n345\n0.54\n2\n1.08\n1\n1\n1\n0\n0\n0\n0.08\n1\n0.08\n1\nNA\n0.08\n\n\n\n\n\nI googled the address, and it seems to be indeed Amtrak. Note to self: Never order pizza on an Amtrak train.\n\n\nPrice vs ratings analysis\nNext, let‚Äôs look at possible impact of restaurant price level on rating.\n\ntable(d2$price_level)\n\n\n  0   1   2   3 \n 21 216 218   8 \n\n\nThere isn‚Äôt much spread, most pizza places are in the middle. Maybe not too surprising. Let‚Äôs look at a few plots to see if there is a pattern. First, we should recode price level as a factor.\n\nd2 &lt;- d2 %&gt;% mutate(price = as.factor(price_level))\n\n\np1 &lt;- d2 %&gt;% ggplot(aes(x=price, y=comm_score)) + geom_violin() + geom_point()\np2 &lt;- d2 %&gt;% ggplot(aes(x=price, y=crit_score)) + geom_violin() + geom_point()\np3 &lt;- d2 %&gt;% ggplot(aes(x=price, y=dave_score)) + geom_violin() + geom_point()\ncowplot::plot_grid(p1, p2, p3, labels = c('A', 'B','C'), label_size = 12)\n\n\n\n\n\n\n\n\nHard to tell if there‚Äôs a trend. Could do some stats to look in more detail, but since this exercise focuses on exploring, I won‚Äôt do that. Instead I‚Äôll leave it at that.\n\n\nRating versus location\nOk, on to the last of the questions I started out with. Maybe there are some areas where restaurants are in general better? Or maybe an area where diners are more critical? Let‚Äôs see if there is some correlation between ratings and location.\n\ntable(d2$country)\n\n\n US \n463 \n\nsort(table(d2$city))\n\n\n        Alpharetta            Augusta             Austin         Austintown \n                 1                  1                  1                  1 \n        Blacksburg          Braintree           Brockton            Buffalo \n                 1                  1                  1                  1 \n        Charleston          Charlotte      Chestnut Hill           Chilmark \n                 1                  1                  1                  1 \n        Clearwater            Clifton         Coralville      Daytona Beach \n                 1                  1                  1                  1 \n          Dearborn        Dennis Port              DUMBO        East Meadow \n                 1                  1                  1                  1 \n             Edina          Elizabeth             Elmont         Gansevoort \n                 1                  1                  1                  1 \n              Gary       Hampton Bays          Hopkinton       Howard Beach \n                 1                  1                  1                  1 \n        Huntington          Iowa City            Jackson Jacksonville Beach \n                 1                  1                  1                  1 \n       Jersey City        Kew Gardens          Lakeville      Lawrenceville \n                 1                  1                  1                  1 \n              Lynn    Manhattan Beach       Mashantucket              Miami \n                 1                  1                  1                  1 \n    Middle Village       Mount Vernon      New Hyde Park      New York City \n                 1                  1                  1                  1 \n   North Arlington             Nutley         Oak Bluffs           Oak Lawn \n                 1                  1                  1                  1 \n     Oklahoma City             Orange         Palm Beach           Pembroke \n                 1                  1                  1                  1 \n         Princeton             Ramsey           Randolph             Revere \n                 1                  1                  1                  1 \n        Rutherford      San Francisco           Sandwich        Southampton \n                 1                  1                  1                  1 \n         Stoughton              Tampa     Vineyard Haven     West Melbourne \n                 1                  1                  1                  1 \n   West Palm Beach       West Roxbury             Woburn            Yonkers \n                 1                  1                  1                  1 \n   East Rutherford          Edgartown       Elmwood Park            Hyannis \n                 2                  2                  2                  2 \n       Miami Beach       Philadelphia         Saint Paul       Santa Monica \n                 2                  2                  2                  2 \n          Stamford              Bronx       Indianapolis          Lexington \n                 2                  3                  3                  3 \n        Morgantown        San Antonio          San Diego          Ann Arbor \n                 3                  3                  3                  4 \n        Louisville          New Haven      Staten Island         Youngstown \n                 4                  4                  4                  4 \n           Atlanta            Chicago           Columbus            Hoboken \n                 6                  6                  6                  6 \n         Nantucket   Saratoga Springs        Minneapolis          Las Vegas \n                 6                  6                  8                 11 \n            Boston           Brooklyn           New York \n                13                 20                251 \n\n\nOk so all restaurants are in the US, and most are in New York. We could look at NY versus ‚Äúrest of the cities‚Äù. Though isn‚Äôt Brooklyn (the 2nd largest entry) basically a part of New York? I‚Äôm not enough of an expert on all things NY to be sure (for any real analysis, you need to know a good bit about the subject matter, or work closely with a subject matter expert. If not, more likely than not something dumb will happen).\nFor now, I assume that it‚Äôs different enough, and make 2 categories, NY and ‚Äúother‚Äù and see if there are differences. Let‚Äôs try.\n\np1 &lt;- d2 %&gt;% dplyr::mutate(newcity = forcats::fct_lump(city, n = 1)) %&gt;%\n              ggplot(aes(x=newcity, y = comm_score)) + geom_violin() + geom_point()\np2 &lt;- d2 %&gt;% dplyr::mutate(newcity = forcats::fct_lump(city, n = 1)) %&gt;%\n              ggplot(aes(x=newcity, y = crit_score)) + geom_violin() + geom_point()\np3 &lt;- d2 %&gt;% dplyr::mutate(newcity = forcats::fct_lump(city, n = 1)) %&gt;%\n              ggplot(aes(x=newcity, y = dave_score)) + geom_violin() + geom_point()\ncowplot::plot_grid(p1, p2, p3, labels = c('A', 'B','C'), label_size = 12)\n\n\n\n\n\n\n\n\nLooks like the community in NY gives lower scores compared to other locations, less noticeable difference for critics and Dave.\nOk, the next analysis might not make much sense, but why not check if there is a North-South or East-West trend related to ratings. Maybe restaurants are better in one of those directions? Or people in the South are more polite and give better scores? üòÅ. I‚Äôm mostly doing this because longitude and latitude are continuous variables, so I can make a few more scatterplots. I don‚Äôt have any real goal for this otherwise.\n\np1 &lt;- d2 %&gt;%  ggplot(aes(x=longitude, y = comm_score)) + geom_point() + geom_smooth(method = 'lm')\np2 &lt;- d2 %&gt;%  ggplot(aes(x=longitude, y = crit_score)) + geom_point() + geom_smooth(method = 'lm')\np3 &lt;- d2 %&gt;%  ggplot(aes(x=longitude, y = dave_score)) + geom_point() + geom_smooth(method = 'lm')\ncowplot::plot_grid(p1, p2, p3, labels = c('A', 'B','C'), label_size = 12)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nSo as we go from the west (-120) to the east (-70), there is a trend in restaurants getting higher scores, by all 3 groups. I guess as we are moving closer to Italy, the pizza quality goes up? üòÉ.\nNext, let‚Äôs look at latitude.\n\np1 &lt;- d2 %&gt;%  ggplot(aes(x=latitude, y = comm_score)) + geom_point() + geom_smooth(method = 'lm')\np2 &lt;- d2 %&gt;%  ggplot(aes(x=latitude, y = crit_score)) + geom_point() + geom_smooth(method = 'lm')\np3 &lt;- d2 %&gt;%  ggplot(aes(x=latitude, y = dave_score)) + geom_point() + geom_smooth(method = 'lm')\ncowplot::plot_grid(p1, p2, p3, labels = c('A', 'B','C'), label_size = 12)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nSo doesn‚Äôt seem as much of a trend going from South (25) to North (45). That finding of course fully confirms our ‚Äúcloser to Italy‚Äù theory!\nOk, I was going to leave it at that with location, but since I‚Äôm already going down a crazy rabbit hole regarding Italy, let‚Äôs do it for real: We‚Äôll take both longitude and latitude of each restaurant and use it compute the distance of each location to Naples, the home of Pizza. And then we‚Äôll plot that and see.\nSince I have no idea how to do that, I need Google. Fortunately, the first hit worked, found this one: https://stackoverflow.com/questions/32363998/function-to-calculate-geospatial-distance-between-two-points-lat-long-using-r\nLet‚Äôs try.\n\ncoord_naples=cbind(rep(14.2,nrow(d2)),rep(40.8,nrow(d2)))  #location of naples\ncoord_restaurants = cbind(d2$longitude,d2$latitude)\ndistvec = rep(0,nrow(d2))\nfor (n in 1:nrow(d2))\n{\n  distvec[n] = distm( coord_restaurants[n,], coord_naples[n,], fun = distGeo)\n}\nd2$distvec = distvec / 1609 #convert to miles since we are in the US :)\n\nIt‚Äôs not tidyverse style, which I tried first but couldn‚Äôt get it to work. The trusty old for-loop seems to always work for me. I checked the numbers in distvec, they look reasonable.\nOk, let‚Äôs redo the plots above, now with distance to Naples.\n\np1 &lt;- d2 %&gt;%  ggplot(aes(x=distvec, y = comm_score)) + geom_point() + geom_smooth(method = 'lm')\np2 &lt;- d2 %&gt;%  ggplot(aes(x=distvec, y = crit_score)) + geom_point() + geom_smooth(method = 'lm')\np3 &lt;- d2 %&gt;%  ggplot(aes(x=distvec, y = dave_score)) + geom_point() + geom_smooth(method = 'lm')\ncowplot::plot_grid(p1, p2, p3, labels = c('A', 'B','C'), label_size = 12, nrow = 3)\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nHm ok, no smoking gun. Looks like there is a bit of a trend that the further away you are from Naples, the lower the score. But really not much.\n\n\nHyping our result\nBut since this distance-from-Naples makes such a good story, let‚Äôs see if I can hype it.\nFirst, to increase potential statistical strength, I‚Äôll combine all 3 scores into an overall mean, i.e.¬†similar ot the all variable in the original. I don‚Äôt trust that one since I don‚Äôt know if they averaged over 0 instead of properly treating it as NA. Of course I could check, but I‚Äôm just re-creating it here.\n\nd2$all_score = rowMeans(cbind(d2$dave_score,d2$crit_score,d2$comm_score),na.rm=TRUE)\n\nOk, let‚Äôs check if correlation between this new score and distance is significant!\n\n#compute a linear fit and p-value (it's significant!)\nfit=lm(d2$all_score ~ d2$distvec, data = d2)\nsummary(fit)\n\n\nCall:\nlm(formula = d2$all_score ~ d2$distvec, data = d2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.7854 -0.5866  0.3027  0.9612  2.3686 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  8.9895014  0.7008802  12.826  &lt; 2e-16 ***\nd2$distvec  -0.0004772  0.0001525  -3.129  0.00187 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.478 on 459 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.02089,   Adjusted R-squared:  0.01875 \nF-statistic: 9.791 on 1 and 459 DF,  p-value: 0.001865\n\npval=anova(fit)$`Pr(&gt;F)`[1]\nprint(pval)\n\n[1] 0.001865357\n\n\nIt is signficant, p&lt;0.05! We hit pay dirt! Let‚Äôs make a great looking figure and go tell the press!\n\n#make final plot\np1 &lt;- d2 %&gt;%  ggplot(aes(x=distvec, y = all_score)) + geom_point(shape = 21, colour = \"black\", fill = \"red\",  size = 2 ) + geom_smooth(method = 'lm', se = TRUE, color = \"darkgreen\", size = 2) + xlab('Distance from Naples (miles)') + ylab('Pizza Quality (score)') + ylim(c(2.5,max(d2$all_score))) + theme_bw() +theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face=\"bold\")) + annotate(\"text\", x=6000, y=9, label= paste(\"p =\",round(pval,4)),size = 12) \nggsave('pizzadistance.png')\nknitr::include_graphics(\"pizzadistance.png\")\n\n\n\n\n\n\n\n\n\n\nThe ‚Äúpress release‚Äù\nA novel study of pizza restaurants in the US found a clear, statistically significant correlation between the distance of the restaurant to Naples and the quality of the pizza as determined by the community and expert restaurant critics. The study authors attribute the finding to the ability of restaurants that are closer to Naples to more easily get genuine fresh and high quality ingredients, such as the famous San Marzano tomatoes.\n\n\n\n\n\n\n\n\n\n\n\nSummary\nThat was a fun exploration. It was the first time I played with the tidyverse data. I had no idea which direction it was going to go, and ideas just came as I was doing it. I‚Äôm sure there is interesting stuff in datasets 1 and 3 as well, but I already spent several hours on this and will therefore call it quits now.\nWhile the exercise was supposed to focus on cleaning/wrangling and visualizing, I couldn‚Äôt resist going all the way at the end and producing a statistically significant and somewhat plausible sounding finding. If this were a ‚Äúreal‚Äù study/analysis, such a nice result would be happily accepted by most analysts/authors, hyped by a university press release and - if the result is somewhat interesting/cute, picked up by various media outlets.\nI had no idea at the beginning what I was going to analyze, I did that longitude/latitude analysis on a whim, and if I hadn‚Äôt found this correlation and had that crazy distance to Italy idea, nothing would have happened. But now that I have a significant result and a good story to go with, I can publish! It‚Äôs not really much sillier than for instance the Chocolate and Nobel Laureates paper paper.\nWhat I illustrated here (without having had any plan to do so), is a big, general problem in secondary data analysis. It‚Äôs perfectly ok to do secondary analyses, and computing significance is also (kinda) ok, but selling exploratory (fishing) results as inferential/causal/confirmatory is wrong - and incredibly widespread. If you want to sharpen your critical thinking skills related to all those supposed significant and real findings in science we see a lot, a great (though at times sobering) read is Andrew Gelman‚Äôs blog where he regularly picks apart studies/results like the one I did here or the chocolate and Nobel laureates one. And now I‚Äôll go eat some chocolate so I can increase my chances for a Nobel prize."
  },
  {
    "objectID": "posts/2020-01-04-blogdown-website-2/index.html",
    "href": "posts/2020-01-04-blogdown-website-2/index.html",
    "title": "Creating a website in less than 1 hour - part 2",
    "section": "",
    "text": "This is part 2/2 of the website development posts, where we‚Äôll move the website to GitHub. This provides a nice and more automated workflow for editing and deploying your site.\nIf you haven‚Äôt seen part 1 yet, I suggest you read through that first. In that part, I covered how to create a website using blogdown, Hugo, and Netlify.\n\nRequired skills\nI assume that you have general computer literacy, but no experience with any of the tools that will be used. Also, no coding, web-development or related experience is expected.\nI assume your website is at a stage as described at the end of part 1.\n\n\nQuick tool overview\nThe only new tool for this part is GitHub. GitHub is a very powerful and common way of developing projects like code, research projects or a website. Github is a great tool to be familiar with in general, and most importantly, it very nicely integrates with the other tools to make things seamless and automated. As you learned in part 1, it is not strictly needed, but it makes updating automatic and is such an overall useful tool to be exposed to that I‚Äôm including it in the setup.\n\n\nGet a GitHub account\nIf you do not already have a GitHub account, create one. Note that GitHub is widely used professionally, and you might want to allow other people to see your GitHub presence. I, therefore, recommend using a future-proof, professional user name.\nNote that Git and GitHub (which are technically different, here I‚Äôm using GitHub to refer to both) can be initially confusing, mainly because they use a lot of specialized terminology. I will try and walk you through all steps for getting a website up and running in detail, but you might have to look up a few things on the way. If you are completely new to GitHub, I recommend you take a quick look at this page (and links provided there) so you can get some idea of what it‚Äôs all about.\n\n\nGet Gitkraken\nDownload and install Gitkraken, link it with your GitHub account. Gitkraken is a graphical Git/GitHub client that makes a lot of tasks related to GitHub easier. It‚Äôs not strictly needed, and if you already have your own way of using Git/GitHub (e.g.¬†with another client or the command line) you can stick with that. There is also an option to use Git/GitHub through RStudio, which is fine for most things, but overall Gitkraken is more powerful. So if you plan to use GitHub more in the future, I recommend using it. In the following, I assume you are using Gitkraken. If you interface with GitHub some other way, you will have to adjust those specific instructions accordingly.\n\n\nRecommended, but optional: Upgrade GitHub (& Gitkraken)\nOn GitHub, by default, all repositories are public (a repository is the collection of files and folders that make up a specific GitHub project, such as your website.) If you have public repositories, you need to be careful about files that shouldn‚Äôt be shared publicly (e.g.¬†because of copyright restrictions or because it might violate data privacy). Normally, if you want private repositories, you have to pay. As student or educator, you can get private repositories for free.\nIf you are a student, I strongly recommend you get the GitHub student developer pack. This gives you access to private repositories. You also get 1 year of free Gitkraken Pro access. While the free version of Gitkraken works well, you can‚Äôt access private repositories with it. Often, being able to use private repositories is useful.\nEducators can also get a free GitHub Pro account here. As far as I‚Äôm aware, there is no free Gitkraken Pro for educators, but it‚Äôs fairly cheap. So if you want to use a private repository for your website (I don‚Äôt know why you would), you need to pay for Gitkraken Pro or use a different way to manage your GitHub repositories.\n\n\nCreate a GitHub repository\n\nLog into GitHub, click on ‚ÄòRepositories‚Äô and create a new repository (green button). Choose a repository (repo) name that tells you what‚Äôs in the repo (e.g.¬†YOURNAME-website). You can give it the same name as you named your main website directory/project in part 1, but that‚Äôs not required. Check the box Initialize this repository with a README. Set the .gitignore option to R, you can leave the license at none. Click create repository.\nClone the repository from GitHub to your local computer (using Gitkraken or whatever software/method you decided to use). Place the local folder in a location on your computer where it is not synced with some other software (e.g., Dropbox, OneDrive).\n\n\n\nMove your website folder to the GitHub repo\n\nFind the main folder of your website and move all of it into your newly created GitHub repository. To make sure everything is up-to-date, close RStudio before doing so.\nOpen the repository you just created in Gitkraken. In the top right of Gitkraken, there should be a notification about changed files. Click View changes, then Stage. Write a commit message, commit the changes. You should see the master with the computer symbol above the one with some random logo. That means your local repository is ahead of the one on github.com. To get them in sync, you click the push button. If things work, the two symbols should now be in the same line.\nIf your code cannot sync you will likely receive an option from GitKraken to perform a force push. A force push will overwrite the remote repo with the local repo forcibly. This means that the remote will be updated with any changes that exist on your local computer however, if any additional changes have been made to the code since you began editing (i.e.¬†someone else has made a commit to the repo while you were working on it) they will be destroyed by the force push since they are not on your local repo. For this project, you are the only person working on your introduction so it is unlikely you will encounter any issues with force push, but it is good to be aware of this action.\nGo back to GitHub.com and to your repository. You should see all your files in there.\n\n\n\nConnecting GitHub to Netlify\n\nThe last step is to set up Netlify so it can automatically monitor your GitHub repository and process any changes into an updated website. To do so, log into your Netlify account. Select your website. Under Site Settings find the Build and deploy menu. Under Continuous deployment click on Link site to Git. Choose GitHub and the follow the steps to link your webpage repository. Once finished, Netlify will monitor that repository and automatically pull any updates and rebuild your website.\n\n\n\nThe new workflow\nThe workflow for your website using GitHub now works as follows:\n\nOpen your website in Rstudio by clicking on the .Rproj file. Load blogdown with library(blogdown). Make any edits you want. While you make your edits, you can run serve_site() to see your updates.\n\nOnce you are done with your updates, restart R (to shut down serve_site()) and run build_site(). While Netlify will automatically run Hugo on your files, it won‚Äôt do any processing of Rmarkdown files, therefore you need to run the blogdown build command.\nUse Gitkraken to push/pull and therefore sync changes between your local computer (or multiple computers if you work on more than one machine) and Github.com.\n\nNetlify will monitor your Github repository and when it sees changes, automatically rebuild your website. Note that this means that if you start working on a document and don‚Äôt finish it, and then push to GitHub, your half-finished document shows up. To avoid that, you can set draft: true in your TOML/YAML heading for the document in progress.\n\n\nMore Information\nFor the non-GitHub aspects of this, see the resources mentioned at the end of part 1. For some more GitHub related information, see e.g.¬†here."
  },
  {
    "objectID": "posts/2020-02-01-publications-analysis-1/index.html",
    "href": "posts/2020-02-01-publications-analysis-1/index.html",
    "title": "Using R to analyze publications - part 1",
    "section": "",
    "text": "Overview\nI needed some information on all my publications for ‚Äúbean counting‚Äù purposes related to preparing my promotion materials. In the past, I also needed similar information for NSF grant applications.\nInstead of doing things by hand, there are nicer/faster ways using R. The following shows a few things one can do with the scholar package. I describe an alternative approach using the bibliometrix package in part 2 of this post.\n\n\nNotes\n\nAs of this writing, the scholar R package seems semi-dormant and not under active development. If Google changes their API for Scholar and the package isn‚Äôt updated, the below code might stop working.\nA problem I keep encountering with Google Scholar is that it starts blocking requests, even after what I consider are not that many attempts to retrieve data. I notice that when I try to pull references from Google Scholar using JabRef and also with the code below. If that happens to you, try a different computer, or clear cookies. This is a well known problem and if you search online, you find others complaining about it. I haven‚Äôt found a great solution yet, other than not using the Google Scholar data. I describe such an approach in part 2 of this post. However, some analyses are only able with Google Scholar information.\nTo minimize chances of getting locked out by Google, I wrote the code below such that it only sends a request if there isn‚Äôt a local file already containing that data. To refresh data, delete the local files.\n\n\n\nRequired packages\n\nlibrary(scholar)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(knitr)\nlibrary(ggplot2)\n\n\n\nGet all citations for an individual\nFirst, I‚Äôm using Google Scholar to get all citations for a specific author (in this case, myself).\n\n#Define the person to analyze\nscholar_id=\"bruHK0YAAAAJ\" \n#either load existing file of publications\n#or get a new one from Google Scholar\n#delete the file to force an update\nif (file.exists('citations.Rds'))\n{\n  cites &lt;- readRDS('citations.Rds')\n} else {\n  #get citations\n  cites &lt;- scholar::get_citation_history(scholar_id) \n  saveRDS(cites,'citations.Rds')\n}\n\n\n\nCompare citations for different time periods\nFor my purpose, I want to compare citations between 2 time periods (my Assistant Professor time and my Associate Professor time). I‚Äôm splitting them into 2. I‚Äôm doing this analysis at the beginning of 2020 and want only full years. The code snippets below give me what I need, two time periods 2009-2014 and 2014-2019.\n\nperiod_1_start = 2009\nperiod_2_start = 2015\ncites_1 &lt;- cites %&gt;% dplyr::filter((year&gt;=period_1_start & year&lt;period_2_start ))\n#remove last year since it's not a full year\ncites_2 &lt;- cites %&gt;% dplyr::filter((year&gt;=period_2_start & year&lt;2020 )) \n\nFitting a linear model to both time segments to look at increase in citations over both periods.\n\nfit1=lm(cites ~ year, data = cites_1)\nfit2=lm(cites ~ year, data = cites_2)\ninc1 = fit1$coefficients[\"year\"]\ninc2 = fit2$coefficients[\"year\"] \nprint(sprintf('Annual increase for periods 1 and 2 are %f, %f',inc1,inc2))\n\n[1] \"Annual increase for periods 1 and 2 are 22.257143, 43.100000\"\n\n\nMaking a figure to show citation count increases\n\n# combine data above into single data frame\n#add a variable to indicate period 1 and period 2\ncites_1$group = \"1\"\ncites_2$group = \"2\"\ncites_df = rbind(cites_1,cites_2)\nxlabel = cites_df$year[seq(1,nrow(cites_df),by=2)]\n#make the plot and show linear fit lines\np1 &lt;- ggplot(data = cites_df, aes(year, cites, colour=group, shape=group)) + \n      geom_point(size = I(4)) + \n      geom_smooth(method=\"lm\",aes(group = group), se = F, size=1.5) + \n      scale_x_continuous(name = \"Year\", breaks = xlabel, labels = xlabel) +     scale_y_continuous(\"Citations according to Google Scholar\") +\n      theme_bw(base_size=14) + theme(legend.position=\"none\") + \n      geom_text(aes(NULL,NULL),x=2010.8,y=150,label=\"Average annual \\n increase 22%\",color=\"black\",size=5.5) +\n      geom_text(aes(NULL,NULL),x=2017,y=150,label=\"Average annual \\n increase 43%\",color=\"black\",size=5.5) \n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n\n#open a new graphics window\n#note that this is Windows specific. Use quartz() for MacOS\nww=5; wh=5; \nwindows(width=ww, height=wh)                    \nprint(p1)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\ndev.print(device=png,width=ww,height=wh,units=\"in\",res=600,file=\"citations.png\")\n\npng \n  2 \n\n\n\n\nGetting list of publications\nAbove I got citations, but not publications. This function retrieves all publications for a specific author and returns it as a data frame.\n\n#get all pubs for an author (or multiple)\nif (file.exists('publications.Rds'))\n{\n  publications &lt;- readRDS('publications.Rds')\n} else {\n  #get citations\n  publications &lt;- scholar::get_publications(scholar_id) \n  saveRDS(publications,'publications.Rds')\n}\n\n\n\nQuick peek at publications\n\nglimpse(publications)\n\nRows: 90\nColumns: 8\n$ title   &lt;fct&gt; \"Severe outcomes are associated with genogroup 2 genotype 4 no‚Ä¶\n$ author  &lt;fct&gt; \"R Desai, CD Hembree, A Handel, JE Matthews, BW Dickey, S McDo‚Ä¶\n$ journal &lt;fct&gt; \"Clinical infectious diseases\", \"BMC public health\", \"Journal ‚Ä¶\n$ number  &lt;fct&gt; \"55 (2), 189-193\", \"11 (S1), S7\", \"7 (42), 35-47\", \"3 (12)\", \"‚Ä¶\n$ cites   &lt;dbl&gt; 163, 158, 129, 124, 123, 115, 105, 89, 71, 71, 55, 53, 52, 49,‚Ä¶\n$ year    &lt;dbl&gt; 2012, 2011, 2010, 2007, 2006, 2012, 2006, 2017, 2016, 2008, 20‚Ä¶\n$ cid     &lt;fct&gt; 1979732925283755485, 10982184786304722425, 1038596204985444772‚Ä¶\n$ pubid   &lt;fct&gt; 5nxA0vEk-isC, _FxGoFyzp5QC, 9yKSN-GCB0IC, d1gkVwhDpl0C, u5HHmV‚Ä¶\n\n\nThis shows the variables obtained in the data frame. One thing I notice is that this contains more entries than I have peer-reviewed publications. Since most people‚Äôs Google Scholar profile (including my own) list items beyond peer-reviewed journal articles, one likely needs to do some manual cleaning before analysis. That is not ideal. I‚Äôll do/show a few more possible analyses, but decided to do the analyses below using the approach in part 2.\n\n\nMaking a table of journals and impact factors\nThis used to work, but as of 2022-09-10 when I tried to re-run, it failed. Seems like get_impactfactor() doesn‚Äôt exist anymore. Maybe they got in trouble with the owners of ImpactFactor? Leaving it here, but code chunk below doesn‚Äôt run.\nThe scholar package has a function that allows one to get impact factors for journals. This data doesn‚Äôt actually come from Google Scholar, instead the package comes with an internal spreadsheet/table with impact factors. Looking a bit into the scholar package indicates that the data was taken from some spreadsheet posted on ResearchGate (probably not fully legal). Either way, let‚Äôs give it a try.\n\n#here I only want publications since 2015\npub_reduced &lt;- publications %&gt;% dplyr::filter(year&gt;2014)\n# my guess is they got in trouble with the owners of ImpactFactor?\nifdata &lt;- scholar::get_impactfactor(pub_reduced$journal) \n#Google SCholar collects all kinds of 'publications'\n#including items other than standard peer-reviewed papers\n#this sorts and removes some non-journal entries  \niftable &lt;- ifdata %&gt;% dplyr::arrange(desc(ImpactFactor) ) %&gt;% tidyr::drop_na()\nknitr::kable(iftable)\n\nOK so this doesn‚Äôt quite work. I know for instance that I didn‚Äôt publish anything in Cancer Journal for Clinicians and the 2 Rheumatology entries are workshop presentations. Oddly, when I look at publications$journal there is no Cancer Journal listed. Somehow this is a bug created by the get_impactfactor() function. I could fix that by hand. The bigger problem is what to do with all those publications that are not peer-reviewed papers. I could remove them from my Google scholar profile. But I kind of want to keep them there since some of them link to useful stuff. I could alternatively manually clean things at this step. This somewhat defeats the purpose of automation.\n\n\nGetting list of co-authors\nAnother useful piece of information to have, e.g.¬†for NSF grants, is a table with all co-authors. Unfortunately, get_publications() only pulls from the main Google Scholar page, which cuts off the author list. To get all authors, one needs to run through each paper using get_complete_authors(). The problem is that Google cuts off access if one sends too many queries. If you get error messages, it might be that Google blocked you. See the Notes section.\n\nallauthors = list()\nif (file.exists('allauthors.Rds'))\n{\n  allauthors &lt;- readRDS('allauthors.Rds')\n} else {\n  for (n in 1:nrow(publications)) \n  {\n    allauthors[[n]] = get_complete_authors(id = scholar_id, pubid = publications[n,]$pubid)\n  }\n  saveRDS(allauthors,'allauthors.Rds')\n}\n\nTheoretically, if the above code runs without Google blocking things, I should end up with a list of all co-authors which I could then turn into a table. The problem is still that it pulls all entries on my Google Scholar profile, and not just peer-reviewed papers. With a bit of cleaning I could get what I need. But overall I don‚Äôt like this approach too much.\n\n\nDiscussion\nWhile the scholar package has some nice features, it has 2 major problems:\n\nGoogle blocking the script if it decides too many requests are made (that can happen quickly).\nSince most people‚Äôs Google Scholar profile (including my own) list items beyond peer-reviewed journal articles, one likely needs to do some manual cleaning before analysis.\n\nI do keep all my published, peer-reviewed papers in a BibTeX bibliography file in my reference manager (I‚Äôm using Zotero and/or Jabref). I know that file is clean and only contains peer-reviewed papers. Unfortunately, the scholar package can‚Äôt read in such data. In part 2 of this post series, I‚Äôll use a different R package to produce the journal and author tables I tried making above.\nThe one feature only available through Google Scholar is the citation record and the analysis I did at the beginning if this post."
  },
  {
    "objectID": "posts/2020-02-15-MADA-survey-analysis/index.html",
    "href": "posts/2020-02-15-MADA-survey-analysis/index.html",
    "title": "Text analysis of a mid-semester course survey",
    "section": "",
    "text": "Our center for teaching and learning administered a mid-semester survey to the students in my fall 2019 online Modern Applied Data Analysis course. I figured it would make for a nice and topical exercise if I performed some analysis of the survey results. Students agreed to have the - fully anonymous - results posted publicly. This is my quick and simple text analysis."
  },
  {
    "objectID": "posts/2020-02-15-MADA-survey-analysis/index.html#some-cleaning-actions",
    "href": "posts/2020-02-15-MADA-survey-analysis/index.html#some-cleaning-actions",
    "title": "Text analysis of a mid-semester course survey",
    "section": "Some cleaning actions",
    "text": "Some cleaning actions\n\nd &lt;- d %&gt;% clean_names() #clean column names, which are the full questions\norig_quest &lt;- data.frame(Number = paste0('Q',1:11), Question = names(d)) #save names and replace with simpler ones for now\nnames(d) = paste0('Q',1:11) #just call each column as Q1, Q2,... originallly asked question is stored in orig_quest\nkable(orig_quest) %&gt;% kable_styling() #print them here for further reference  \n\n\n\n\n\nNumber\nQuestion\n\n\n\n\nQ1\nwhats_working_well_in_this_class_what_are_the_strengths_of_the_class_and_which_aspects_are_having_a_positive_impact_on_your_learning\n\n\nQ2\nwhat_aos_not_working_so_well_in_this_class_what_aspects_are_having_a_less_positive_impact_on_your_learning\n\n\nQ3\nwhat_specific_changes_do_you_think_should_be_made_to_improve_your_experience_in_this_class\n\n\nQ4\ni_think_the_pace_of_this_class_is\n\n\nQ5\nare_there_specific_modules_that_should_be_adjusted_and_how_5\n\n\nQ6\nthe_quantity_of_material_covered_in_each_module_is\n\n\nQ7\nare_there_specific_modules_that_should_be_adjusted_and_how_7\n\n\nQ8\nthe_level_of_difficult_of_each_module_is\n\n\nQ9\nare_there_specific_modules_that_should_be_adjusted_and_how_9\n\n\nQ10\non_average_i_spend_this_many_hours_per_week_doing_work_for_this_course\n\n\nQ11\nfinally_what_is_your_gold_star_top_choice_number_one_recommendation_for_a_constructive_change_your_instructor_can_make_in_this_course\n\n\n\n\n\n\n\n\nMore cleaning\n\nvisdat::vis_dat(d) #missing values\n\n\n\n\n\n\n\n#looks like a few students left some entries blank. Should be ok. One student only answered 1 question. Quick look at entry.\nprint(d[12,2])\n\n# A tibble: 1 √ó 1\n  Q2                                       \n  &lt;chr&gt;                                    \n1 Elc system not work well for online class\n\n#ok, not too useful (though I agree with the statement). Let's remove that student/observation.\nd&lt;- d[-12,]\n# most questions were free text, but some were specific choices, so should be grouped as factor.\nd &lt;- d %&gt;% dplyr::mutate_at(c(\"Q4\", \"Q6\",\"Q8\"), factor)\n#Q10 is number, should be numeric but was text field so different entries exist\n#small enough to print here\nprint(d$Q10)\n\n [1] \"20\"             \"15-20\"          \"15 or more\"     \"15\"            \n [5] \"14-16\"          \"12\"             \"15\"             \"10\"            \n [9] \"30\"             \"20\"             \"10 to 12 hours\" \"&gt;10 hours\"     \n[13] \"2\"              \"20\"            \n\n#ok, this is kinda bad style, but the dataset is so small that it's easiest to replace the non-numeric values by hand. I'll set them to their mean or the specified limit.\nd$Q10[c(2,3,5,11,12)] &lt;- c(17.5,15,15,11,10)\nd$Q10 &lt;- as.numeric(d$Q10)\nprint(d$Q10)\n\n [1] 20.0 17.5 15.0 15.0 15.0 12.0 15.0 10.0 30.0 20.0 11.0 10.0  2.0 20.0"
  },
  {
    "objectID": "posts/2020-02-15-MADA-survey-analysis/index.html#drawing-first-conclusions",
    "href": "posts/2020-02-15-MADA-survey-analysis/index.html#drawing-first-conclusions",
    "title": "Text analysis of a mid-semester course survey",
    "section": "Drawing first conclusions",
    "text": "Drawing first conclusions\n\nkable(orig_quest[c(4,6,8),]) %&gt;% kable_styling()\n\n\n\n\n\n\nNumber\nQuestion\n\n\n\n\n4\nQ4\ni_think_the_pace_of_this_class_is\n\n\n6\nQ6\nthe_quantity_of_material_covered_in_each_module_is\n\n\n8\nQ8\nthe_level_of_difficult_of_each_module_is\n\n\n\n\n\n\n\nd %&gt;% dplyr::select(Q4, Q6, Q8) %&gt;% summary()\n\n          Q4                Q6                Q8    \n just right:10   right amount:4   just right   :10  \n too fast  : 4   too much    :9   too difficult: 3  \n                 NA's        :1   NA's         : 1  \n\nplot(1:14,d$Q10, ylab = 'Time spent per week')\nlines(1:14,rep(12,14))\n\n\n\n\n\n\n\n\nBased on answers to questions 4,6 and 8, the majority of students think the pace and level of difficulty of the course is right but the amount of material covered is too much. Based on answer to Q10, students spend more time than my target (12 hours, solid line). Even accounting for some ‚Äúinflation factor‚Äù (people generally over-estimate the time they spend on tasks like these, counting all the other things they do at the same time e.g., texting/email/FB/drinknig coffee/‚Ä¶), the overall amount seems too high, and it agrees with Q6 answers about too much material.\nFirst conclusion: Reduce weekly workload, probably best by reducing assigned reading (see text answers which I already glimpsed at üòÉ)."
  },
  {
    "objectID": "posts/2020-02-15-MADA-survey-analysis/index.html#manual-text-analysis",
    "href": "posts/2020-02-15-MADA-survey-analysis/index.html#manual-text-analysis",
    "title": "Text analysis of a mid-semester course survey",
    "section": "Manual text analysis",
    "text": "Manual text analysis\n\n#dropping the question/variables analyzed above\nd &lt;- d %&gt;% dplyr::select( -c(\"Q4\", \"Q6\", \"Q8\", \"Q10\") )\n\nQuestions 5, 7 and 9 ask how modules should be adjusted regarding pace, quantity and difficulty, so it‚Äôs worth looking at those questions on their own.\n\nd2 &lt;- d %&gt;% dplyr::select( Q5, Q7, Q9)\nis.na(d2) #some students didn't write anything for any of those questions, remove before printing content.\n\n         Q5    Q7    Q9\n [1,] FALSE  TRUE FALSE\n [2,] FALSE FALSE FALSE\n [3,]  TRUE  TRUE  TRUE\n [4,] FALSE FALSE FALSE\n [5,] FALSE FALSE FALSE\n [6,]  TRUE  TRUE  TRUE\n [7,] FALSE FALSE FALSE\n [8,]  TRUE  TRUE  TRUE\n [9,]  TRUE  TRUE  TRUE\n[10,]  TRUE  TRUE  TRUE\n[11,] FALSE FALSE FALSE\n[12,]  TRUE  TRUE  TRUE\n[13,] FALSE  TRUE  TRUE\n[14,]  TRUE FALSE  TRUE\n\nd2 &lt;- d2[which(rowSums(is.na(d2)) != 3),] #remove all rows/obersvations that have NA to all 3 questions\nnames(d2) &lt;- c('too fast','too much','too hard')\nknitr::kable(d2) %&gt;% kable_styling() ##show rest\n\n\n\n\n\ntoo fast\ntoo much\ntoo hard\n\n\n\n\nAll of them are to long except for module 2.\nNA\nThe quizzes are becoming ridiculously difficult.\n\n\nNone, I have enjoyed the pace.\nNone, I think the content is appropriate.\nThe toughest module thus far in the course was the strings module. Though it should be noted, I think the content was very appropriate to cover the concepts of this module as they are quite difficult. I really benefited from completing some of the Regex crosswords while working through this chapter.\n\n\nI think all modules need to be adjusted.\nConsidering we have only done the first half of the class and the shortest amount of time I have spent in one week on this class was the first module and I still did more than 9 hours of work for what should have been a half week seems unreasonable. For all other modules I spend over 12 hours each week on this class sometimes upwards of 25 hours in a single week.\nI don't think the modules themselves are difficult just the content is being squeezed in and the time it takes doesn't correlate to the amount I feel like I'm learning, where I should be learning/proficient in much more than I currently am based on the amount of time spent.\n\n\nThe visualization module is pretty hefty in terms of how much time is spent. But it's all the same topic so I don't know how that would be split up.\nsee above\nThe modules are just difficult enough that the first couple times I try I struggle and it's hard, but after giving it a go it becomes much easier and I get it. So its enjoyably difficult.\n\n\nI feel like I like the order and pace of the course materials. We can learn all of the material in the module in one week - it's just exhausting to do so.\nTrim the reading in the modules and add more exploratory exercises.\nI feel like the modules scale up in difficulty each week, but proportionally to the growth in our skills. I feel like I am really learning R for the first time! (After three other courses in R...)\n\n\nIn the tidyverse module, we used ggplot, but then we actually learned about how to do ggplot which would have been helpful before tidyverse. The R module was definitely a lot of material and the coding exercise was a steep learning curve for the first true coding exercise of the course. I understand sometimes the best way to learn something is struggling through, but I think it can be difficult for new students to make the sudden leap from follow a specific script in a book chapter to make up your own code. Maybe something as in altering existing scripts first?\nWhile now there is a distinction on what is going to be on the quiz, there are multiple chapters and tutorials I haven‚Äö√Ñ√¥t read through yet. I‚Äö√Ñ√¥m actively searching to see what to prioritize first in a module. Module 4/5 also had a lot of (useful) material to work through.\nI think working through the material for the quizzes themselves is just the right difficulty but the assessments range from not so difficult (Tidyverse) to spending a couple of hours googling and troubleshooting (first R and Visualization). While you said to spend no more than 1 or 2 hours on Visualization, it still took me 4 hours to figure out.\n\n\nNot that I see so far.\nNA\nNA\n\n\nNA\nMost modules waver between just the right amount and a little too much. However, this goes hand in hand with what I have identified also as a strength of the course (great curation of resources), so perhaps adjustment isn't necessary... maybe it would be useful to have a more organized list of required and optional readings.\nNA\n\n\n\n\n\n\n\n\nConclusions from anwers to those questions: Overall too much material (see above), level of difficulty overall ok but too fast/crowded. Again, solution is to reduce (required) material.\nNext, let‚Äôs look at ‚Äúwhats working/not working‚Äù questions.\n\nd2 &lt;- d %&gt;% dplyr::select( Q1, Q2)\nnames(d2) &lt;- c('good','bad')\nknitr::kable(d2) %&gt;% kable_styling() \n\n\n\n\n\ngood\nbad\n\n\n\n\nThe positive impact would be that I am learning completely new things I have had no exposure too.\nThis class is extremely overwhelming. It is online and the professor is the worst time estimator I have ever seen. He assigns way to much with absolutely no emphasis on what is important. As he is the ‚Äö√Ñ√∫expert‚Äö√Ñ√π in this subject he should be able to narrow it down and make this course more reasonable. I should not be spending 20-25 hours a week on one course that is 3 credits.\n\n\nI really enjoy the exercises assigned for each module. I am receiving a good amount of background information from the modules/reading, however I feel I really start to understand the material once I have used it in practice. Additionally, a good portion of the exercises thus far are directly applicable to the analysis I will be using for my personal research which is excellent practice. I also really appreciate that the structure of the course. I really struggled with the basic work flow of R scripts and GitHub initially as I am newer to the program. However, the progressive flow that this course has created has allowed me to learn the process step-by-step and frequently connected back to information that was previously used in an earlier module. As a result, I have become very comfortable with the basic usage/flow of R and I am excited to move into the more detailed functions of the program in the latter half of the semester!\nThe only small issue I have encountered thus far with this course is with the quiz time limits. I am sometimes having trouble sifting through all of my notes quickly enough to properly answer the quiz questions. I take approximately 10-20 pages of notes on each module (depending on the size and amount of exercises) and I can sometime struggle to locate the specific material in my notes within these time constraints. I absolutely understand the need for a time limit with an online class structure, however 20 minutes can be a bit tight for some of the quizzes. A simple bump from a 20-30 minute time range to 30-40 minute time range would be more than enough to address all questions adequately, while still ensuring students review the module content beforehand.\n\n\nI love the tidyverse package and thanks to instructions on R primer. I love R primer, because it raise the problem and then have a space to write code to try, and available solution.\nSome materials (R data science, IDS) are useful too. However, I do not like them. The reason is that they give me the knowledge, and explain some simple code. Then, I have exercise part with more complicated questions. I do not know how to do it sometimes and got stuck. One example is chapter about regular expressions. Another thing is that this class took me so much time per week. It is one third of my week. I still have other three classes, and research duties. I expect that taking class is the quicker and better way to learn than learning by myself. This class is not what I expect. It is too time-consuming because I mainly have to learn by myself.\n\n\nThe R primers are very useful for understanding the material, the exercises you give us, and some of the exercises in the IDS book (however, they need to be narrowed down to what is actually important).\nHaving 5+ chapters of reading each week isn't useful. The readings in general aren't useful for coding, at least for me. They should supplement and be a reference for the actual coding we are doing but not be the entire basis of the quizzes. I spend so much time reading and I don't actually understand any of it unless I'm doing the coding. However, expecting to do every exercise that comes with some of the reading isn't working because I currently spend at least 12 hours every week just doing the exercises that go with readings and I end up not retaining much because I'm overwhelmed.\n\n\nI really enjoy having a structured way to learn r for data analysis instead of just learning it on my own. It makes it much more manageable and mentally-forgiving when there are other people learning/struggling at the same time as you. All of the data/resources are in one place and present in a timeline that makes it easier to understand and learn. I also really enjoyed how he makes us go back to the other student's repositories and work with them. It helps foster the feeling/attitude that we're classmates and get to know each other more even though it's an online course.\nThe resources are a lot to work through. I don't have previous R experience but this class is taking a lot of my time to work through the R for data science book.\n\n\nThe amount of information available. Strengths of the class are being able to work with other students to complete assignments and get ideas from one another.\nThe amount of work we have to do every week, including reading, quizzes, and lengthy assignments. Someone new to coding might find this extremely overwhelming considering they have other classes to work on as well as their own laboratory work. The quizzes make me feel like I have not learned anything because they are very weird, specific questions that I have to spend a long time going back through the reading to hunt for.\n\n\nThe course website is very thorough. It is very clear that Dr. Handel has put a lot of thought and effort into building the materials and course for this class. The writing is very clear and accessible. The site and materials are very consistent, which is helpful. The RPrimers, IDS, and R4DS chapters are helpful (in that order, respectively). The class exercises are very helpful. I am finding myself coding more often and drafting unique codes, which was my goal from the start. This is the most well-rounded course in R I have taken. Dr. Handel's exercises are by far the most helpful. We get to think through the material and use resources as-needed. These are exercises I will need in my own analysis. I have been using these codes as guidelines for working through my own data sets. I love the R-Primers as an introduction, and then the Exercises. I think we could take a stab at the exercises without the IDS and R4DS readings, or have those readings embedded into the exercise. Such as...\"Try to make this figure\" If you are stuck, Chapter 4.5 might be a good a reference.\nI'm lucky in that there are a five or six students in my department all taking this course. Since we see each other regularly, we can help each other along and make sense of the material. I can't imagine working through this course alone, with no face-to-face interaction. The group dynamic helps me read into what is important and helps trouble-shoot when there are problems. The eLC discussion board is not great for connecting with others... Outside of these few people that I am lucky to see in person, I think it is hard to connect with the other students to gauge if I am \"on the right track\" or \"falling behind\" or plain bad at coding. We don't engage much with other students, so it's hard to tell if they're succeeding or struggling as much as we are. I keep thinking - \"am I the only one feeling this way?\" The modules seem to be thick/dense for just one week's worth of work. As soon as I finish a module, I am exhausted by the material, but then have to start right back up again. I don't feel like I have enough time to play around with all of the functions we just learned before it's time to start learning new ones. I'd like more time to apply them to data sets in example exercises. I've been to a few meetings with my advisor now where I say that most of my week has been taken up with this course. Perhaps I need to work on my own time management and expectations for this course work. Of course, I do think the time investment will pay off! I am excited to start analyzing my own data set! To get by, I feel like I have started skimming the readings, or reading them \"diagonally\" as some people may say. I have \"CTRL-F\" through the readings just for the quizzes. I don't think this is ideal, and I'm sure Dr. Handel wouldn't want us to resort to this.\n\n\nI like the exercises in R. While my lack of previous experience means I have to spend more time trying to understand how to put complex displays together, what I learn from this stays with me a lot better.\nI know that I review the material quite a bit for the quizzes yet continue to be surprised at what actually shows up on the quiz. This may be a personal problem as the class distribution shows that other individuals are doing well. However, I know that it continues to be frustrating. Also, the MADA course on GitHub is still a bit difficult to navigate and the info given for each module is a lot to absorb without there being an application element.\n\n\nThe supplementary readings are useful, but that's about all that's having a positive impact on my learning in this class.\nThe class being online is convenient, but if the class were in person and I could see step by step instructions, and everything would make more sense. Yes I understand you learn by making mistakes but taking 20+ hours every single time I do an assignment or reading is absolutely ridiculous due to the fact that all of my instructions are typed out, and there's no \"teaching\" in my opinion. This class structure is not conducive to my learning style. There aren't even any videos of the professor lecturing and walking us through lessons. I'm not absorbing any material by reading over 50+ pages then being thrown an assignment and an overly detailed quiz that doesn't even focus on main points but extremely detailed findings that DO NOT showcase what I've learned.\n\n\nThe class is quite well structured with nice use of different video resources.\nSome of the assignments are very much like busy work and are time consuming without being particularly helpful and just end up putting one under pressure during the week whenever there are so many other draws on ones time at graduate school.\n\n\nI do like the material we cover since it‚Äö√Ñ√¥s mostly relevant, I feel like I‚Äö√Ñ√¥m learning more of the ins and outs of R each week! I think the set up of the class with readings then an assessment works well.\nThere is a lot of material to cover each week. While I enjoy it and I‚Äö√Ñ√¥m interested, it is all mostly new to me and I feel like I have to rush through the readings to do the quiz by Friday.\n\n\nThis class requires a lot of coding, and I had no previous experience with coding, but we were provided with a lot of resources and exercises to start from the beginning.\nThere are too much reading assignments. It really takes a lot of time for this class. I cold barely finish these assignments every week, not alone to digest them and use them.\n\n\nI think this course is set up very well. The modules are constructed in a manner that makes the content accessible and the exercises are well guided.\nI would say that nothing related to the class is having a negative impact on my learning.\n\n\nWe receive a ton of excellent resources. Dr. Handel is very responsive to my feedback, which I greatly appreciate. I was very apprehensive about taking an online course, but my fears have been completely assuaged. This class has already been incredibly beneficial to me.\nAs I've already shared with Dr. Handel, the online quizzes are sometimes frustrating in that we are only asked 6 to 7 very specific questions on a great deal of material. I don't feel we're given enough time to complete it, and partial credit isn't awarded. However, as I mentioned above, Dr. Handel is very receptive to feedback and I have already spoken with him about my concerns with this issues.\n\n\n\n\n\n\n\n\nConclusions from anwers to those questions: Overall too much material, especially too much reading. R primers are good. Other resources are hit or miss. Quizzes are not working, need to be ditched or altered. Maybe more exercises. Find better alternative to eLC.\nFinally, the 2 remaining questions are about improvements, phrased in 2 different ways. Let‚Äôs look at them together.\n\nd2 &lt;- d %&gt;% dplyr::select( Q3, Q11)\nd2 &lt;- d2[-12,] #this student didn't provide answers to either question\nnames(d2) &lt;- c('specific suggested changes','number one recommendation')\nknitr::kable(d2) %&gt;% kable_styling() \n\n\n\n\n\nspecific suggested changes\nnumber one recommendation\n\n\n\n\nLess information per module. Most of these modules should be split into two weeks.\nYou need to put yourself in the eyes of a graduate student who is new to R and has a minimal stats background. As a graduate student in general public health field, what are the most important things to prepare is for publishing papers, understanding data, etc. We should have spent two weeks on data wrangling and two weeks on data visualization. These are EXTREMELY important topics, and they were rushed through in one week. I do not think my understanding of how to do these things is good, and instead we are moving onto other topics that are less important.\n\n\nAs mentioned above, just a bit more time on the quizzes would be great!\nA small increase in the time allotted to take the weekly quizzes.\n\n\nI suggest that you can give the solution for this chapter ahead. We can learn through solution if we get stuck. To reduce time to learn by myself, I suggest that professor can create R scripts with coding and explain what is the purpose of the code by text or better having a video to show how the code works in purpose.I think that this way will be better than the way that students have to go through many chapters of reading. If going through many chapters, students have to learn the new things and be able to finalize the knowledge at the same. This process is hard. So, I suggest that professor finalize the knowledge, then show it to us. You can indicate book in case they are still confused or do not understand, and need to read further.\nFor the knowledge that I learn through the R primer, I am happy with them. However, for other chapters that have to read through R4DS, and IDS, it is too time consuming and not an efficient way to learn. If you can have a better resources for the knowledge covered in R4DS and IDS, I am happy with this course.\n\n\nMore homework assignments that mimic the material that is in the readings, where we can use the readings as a reference/starting point. I feel like if we replace the quizzes/readings with an assignment that goes over what is generally important from the chapters you assign it would be more worthwhile.\nNo quizzes - replace with short assignments that reinforce the material presented in the reading each week. Still makes it so you have to read but not that you have to spend hours upon hours doings so. Also as a side note - maybe introduce folders in the class github so that its organized by assignment so we can easily find things.\n\n\nSpecifically not much.\nEstablishing a working discussion board from the beginning would probably be the best thing.\n\n\nI think that there should be more time allotted for the quizzes, simply because of the complexity of the questions as well as the vast amount of reading required.\nBe a little more lenient on quizzes and giving out massive assignments. For an online course in coding, you're asking a lot from us.\n\n\nIt might be helpful to have \"coding drop in sessions\" where students can meet in a room, bring snacks, and discuss the course as a group. It would be helpful if each modules was spread out over two weeks. It would be helpful to have more of Dr. Handel's exercises (not necessarily the IDS R4DS ones).\nTrim the modules\n\n\nI know that I do significantly better on the exercise portions of the class as I can see where the material is applicable but, again, this may be a personal preference. One blanket recommendation would be improving the navigational ability of the GitHub course site and breaking down the sections within each module to highlight where outside links need to be used.\nNA\n\n\nIf this class is going to remain online I think the professor should record himself lecturing and have a split screen showing him using R instead of reading everything to get instructions. I think the supplementary reading/interactive learnings are helpful, but they should be in addition to teaching, not the only mode of learning. At this point I don't even feel like my professor is teaching me, a website and book are (barely).\nEither change back to in person instruction or have video lectures with a computer split screen to show examples, THEN supplement with readings and interactive learning. The entire course just can't be a written instruction list and a website with definitions.\n\n\nMake the assignments more relevant rather than busy work or at least have a required part and then then optional parts because to have to do a time consuming assignment each week isn't beneficial\nNA\n\n\nI liked the quiz this week (Module 7) because you specifically said that the exercises were optional in the reading so it did take the pressure off of having to go through multiple chapters. If the purpose of the quizzes are to make sure we are reading the material, then it was nice to focus on the content of the chapters. (And not lose 30 minutes to solving one exercise)\nexercises from chapters not be covered on quizzes. :)\n\n\nAgain, the class is great. The only improvements are needed by me.\nI do think the tidyverse is so massive an undertaking for a beginner that it could be split into two modules.\n\n\nNothing in particular comes to mind at the moment, as I've felt very comfortable voicing recommendations to Dr. Handel as they arise :)\nAlternative way of testing for our reading each week? As of right now, the quizzes are precarious (one wrong answer can doom a grade) and time-constrained, and a pretty significant source of stress for me. However, as mentioned earlier, Dr. Handel and I are in communication about this!\n\n\n\n\n\n\n\n\nConlusions from these answers: Reduce content per module (or alternatively increase time). Adjust or drop quizzes. More exercises. Record some lectures or provide links to recordings."
  },
  {
    "objectID": "posts/2020-02-15-MADA-survey-analysis/index.html#automated-text-analysis",
    "href": "posts/2020-02-15-MADA-survey-analysis/index.html#automated-text-analysis",
    "title": "Text analysis of a mid-semester course survey",
    "section": "Automated text analysis",
    "text": "Automated text analysis\nSo this is likely not too useful, but I wanted to play around with some automated text analysis. Maybe the computer can figure out things I can‚Äôt?\nI don‚Äôt actually know how to do text analysis, so I‚Äôll have to peek at the tidytext tutorial. Getting some ideas from this tutorial and the Text Mining with R book.\nTurn all answers into a long dataframe of words\n\nd2 &lt;- d %&gt;% tidyr::pivot_longer(cols = starts_with('Q'), names_to =\"question\", values_to = \"answers\") %&gt;% \n            drop_na() %&gt;%\n            unnest_tokens(word, answers, token = \"words\")\n\nLook at most frequent words.\n\nd2 %&gt;%  count(word, sort = TRUE) \n\n# A tibble: 835 √ó 2\n   word      n\n   &lt;chr&gt; &lt;int&gt;\n 1 the     215\n 2 i       134\n 3 to      126\n 4 and      90\n 5 of       81\n 6 a        75\n 7 is       59\n 8 in       57\n 9 that     48\n10 are      47\n# ‚Ñπ 825 more rows\n\n\nThe usual words are the most frequent."
  },
  {
    "objectID": "posts/2020-02-15-MADA-survey-analysis/index.html#sentiment-analysis",
    "href": "posts/2020-02-15-MADA-survey-analysis/index.html#sentiment-analysis",
    "title": "Text analysis of a mid-semester course survey",
    "section": "Sentiment analysis",
    "text": "Sentiment analysis\nSentiment analysis, look at most frequent positive and negative words.\n\nbing &lt;- get_sentiments(\"bing\")\npositive &lt;- bing %&gt;% filter(sentiment == \"positive\")\nd2 %&gt;% semi_join(positive) %&gt;% nrow()\n\n[1] 158\n\nnegative &lt;- get_sentiments(\"bing\") %&gt;% filter(sentiment == \"negative\")\nd2 %&gt;% semi_join(negative) %&gt;% nrow()\n\n[1] 72\n\nbing_word_counts &lt;- d2 %&gt;%\n  inner_join(bing) %&gt;%\n  count(word, sentiment, sort = TRUE) %&gt;%\n  mutate(n = ifelse(sentiment == \"negative\", -n, n)) %&gt;%\n  mutate(word = reorder(word, n)) \n\nPlot positive and negative words.\n\nbing_word_counts %&gt;% ggplot(aes(word, n, fill = sentiment)) +\n  geom_bar( stat = \"identity\") +\n  coord_flip() +\n  labs(y = \"Counts\")\n\n\n\n\n\n\n\n\nAbout twice as many positive as negative words, i guess that‚Äôs good üòÉ. And the most frequent negative words do reflect that things are ‚Äútoo much‚Äù.\nLet‚Äôs look at sentiment per question. Higher values are more positive.\n\nquestion_sentiment &lt;- d2 %&gt;%\n      inner_join(bing) %&gt;%\n      count(question, sentiment) %&gt;%\n  spread(sentiment, n, fill = 0) %&gt;%\n  mutate(sentiment = positive - negative)\n\nggplot(question_sentiment, aes(question, sentiment)) +\n  geom_bar(stat = \"identity\", show.legend = FALSE) \n\n\n\n\n\n\n\n\nNot surprising, the 1st question ‚Äúwhat is working well‚Äù has lots of positive. Surprisingly, question 2, ‚Äúwhat‚Äôs not working well‚Äù has fairly high positive sentiment. One problem could be that what I‚Äôm plotting is total counts, but I should probably normalize by total words written per question. Let‚Äôs try:\n\nwords_per_q &lt;- d2 %&gt;% group_by(question) %&gt;% count()\nprint(words_per_q)\n\n# A tibble: 7 √ó 2\n# Groups:   question [7]\n  question     n\n  &lt;chr&gt;    &lt;int&gt;\n1 Q1         799\n2 Q11        393\n3 Q2        1304\n4 Q3         582\n5 Q5         193\n6 Q7         202\n7 Q9         240\n\n\nYep, looks like most words were written by far for Q2. Maybe not a good sign? But maybe ok, since this specifically solicited feedback on all aspects. So let‚Äôs replot sentiment, normalized by number of words.\n\nquestion_sentiment &lt;- question_sentiment %&gt;% mutate(sent_per_word = sentiment / words_per_q$n)\nggplot(question_sentiment, aes(question, sent_per_word)) +\n  geom_bar(stat = \"identity\", show.legend = FALSE) \n\n\n\n\n\n\n\n\nOk, changed things a bit but not a lot. Q2 drop (expected) is most noticable change. Still, even for the ‚Äúwhat‚Äôs not good‚Äù section, positive words dominate. That either means the course is quite good, or students are very optimistic or polite, or it might mean nothing at all."
  },
  {
    "objectID": "posts/2020-02-15-MADA-survey-analysis/index.html#wordclouds",
    "href": "posts/2020-02-15-MADA-survey-analysis/index.html#wordclouds",
    "title": "Text analysis of a mid-semester course survey",
    "section": "Wordclouds",
    "text": "Wordclouds\nWhy not? Everyone loves a wordcloud, even if they are just fun to look at, right?\n\nd2 %&gt;%\ninner_join(bing) %&gt;%\n  count(word, sentiment, sort = TRUE) %&gt;%\n  acast(word ~ sentiment, value.var = \"n\", fill = 0) %&gt;%\n  wordcloud::comparison.cloud(colors = c(\"#F8766D\", \"#00BFC4\"),\n                   max.words = 100)\n\n\n\n\n\n\n\n\nAt this point, I ran out of ideas for further text analysis. I didn‚Äôt think analysis by word pairs, or sentences, or such alternatives would lead to any further interesting results. I looked in the Text Mining with R book for some more ideas of what kind of analyses might be useful, but can‚Äôt come up with anything else. Not that the above ones are that useful either, but it was fun to try some text analysis, which is a type of data analysis I‚Äôm not very familiar with. So, I‚Äôll stop this here. Feel free to play around yourself, you have access to the raw data and this script in the GitHub repository."
  },
  {
    "objectID": "posts/2020-10-07-custom-word-format/index.html",
    "href": "posts/2020-10-07-custom-word-format/index.html",
    "title": "Custom Word formatting using R Markdown",
    "section": "",
    "text": "Background\nA good while back (around 2 years as of this writing), I needed the feature to turn an R Markdown document into a Word document (that‚Äôs easy) and to apply custom styles to specific parts of the Word document (that was trickier). I found some good information in this RStudio article, but it didn‚Äôt quite address everything I was looking for.\nSpecifically, I wanted to find a way to format certain parts of the R Markdown document in a specific style. I asked online and got some help from JJ Allaire (yes, the guy who started RStudio ü§©). Then recently, Dean Attali (yes, the guy who does a lot of cool Shiny stuff ‚ú®) posted a reply asking for an example. I tried to dig out the 2 year old project (it turns out I ended up not needing it for that project and haven‚Äôt used it since).\nAs I was trying to contemplate how to best share the example with Dean, I figured I‚Äôll write a brief blog post, which might benefit others, too. So here it goes.\n\n\nWord template preparation\n\nCreate a new word document (either through RMarkdown -&gt; Word, or just open Word and create a new empty one).\nOpen the word document. Write a bit of text. It doesn‚Äôt matter what you write, it‚Äôs just meant so you can create and apply new styles to it. For instance you can write Text for mystyle1.\nMark the text you wrote, click on the arrow to the left of the Styles box (see the red ‚Äúcircle‚Äù in the figure) and choose Create a style. Depending on your version of Word, this might be somewhere else. Create the formatting the way you want.\nRepeat to create as many custom styles as you want, save the word document into the folder of your RMarkdown file.\n\n\n\n\n\n\n\n\n\n\n\n\nRMarkdown setup\nStart a new Rmarkdown document. Your YAML header should look something like this:\n---\ntitle: \"An example of formatting text blocks in Word\"\nauthor: \"Andreas Handel\"\ndocumentclass: article\nsite: bookdown::bookdown_site\noutput:\n  bookdown::word_document2: \n    toc: false\n    reference_docx: wordstyletemplate.docx\n---\nNote that I‚Äôm using bookdown as the output format here, but any others that can produce word output, e.g.¬†the standard R Markdown format, should work equally well. The important part is the last line, which specifies the word document with the custom styles you created in the previous step.\n\n\nRMarkdown input content\nYou can now assign text blocks in your R markdown file specific styles. Here I created 3 styles called mystyle1/mystyle2/mystyle3 in the Word doc, and assign them to specific parts of the text. This example markdown text shows how to use the styles.\n# A regular section\n\nThis text is not getting a special format.\n\n# A formatted Section\n\n:::{custom-style=\"mystyle1\"}\nThis is formatted according to the _mystyle1_ format.\n:::\n\n# Another formatted block of text\n\nSome more regular text.\n\n:::{custom-style=\"mystyle2\"}\nNow text formatted based on _mystyle2_.\n:::\n\nMore regular text.\n\n:::{custom-style=\"mystyle3\"}\nThis format includes a border and it also works with an equation.\n$$Y = bX + c$$\n:::\n\nRegular text again.\n\n::: {custom-style=\"mystyle1\"}\n# With a header\n\nNote that the header formatting is overwritten.\n:::\n\n\nWord output\nThe resulting word document looks like this:\n\n\n\n\n\n\n\n\n\n\n\nSome notes\nOne thing you see in this example is that your own styles overwrite all others, so headers inside your custom style will just be formatted like your custom style. Some other quirks I noticed is that you seem to need empty lines before and after your custom style block. I seem to remember that formatting of R chunks works ok, but I also seem to recall that sometimes manual intervention is required.\nOverall, this approach gives you a good deal of flexibility for applying styling to your Word documents when writing in R Markdown, but there might still be some kinks. As I mentioned in the beginning, I ended up not using it for the project I had intended to use it (a review paper I wrote), so I don‚Äôt have a lot of real world experience beyond what I‚Äôm describing here.\n\n\nFurther Resources\nYou can get the Word template and the example R Markdown file I used to illustrate this here and here.\nI recently saw that the new R Markdown cookbook has a section describing word styling. I expect that more content will be added to the cookbook, so it‚Äôs worth checking regularly."
  },
  {
    "objectID": "posts/2021-01-11-simple-github-website/index.html",
    "href": "posts/2021-01-11-simple-github-website/index.html",
    "title": "Create a simple Markdown/GitHub website in less than 30 minutes",
    "section": "",
    "text": "Update 2022-09-25: Hosting a website on GitHub is still a good idea. With the new Quarto framework available, I recommend using that setup. This page provides good instructions on how to start a Quarto website, and this page explains how to publish it on GitHub. Maybe I‚Äôll get around to updating this post for Quarto soon. In the meantime, here is my short Hugo to Quarto transition guide.\nThe following blog post provides step-by-step instructions for creating a simple (and free) website using (R)Markdown and Github."
  },
  {
    "objectID": "posts/2021-01-11-simple-github-website/index.html#install-r-and-rstudio",
    "href": "posts/2021-01-11-simple-github-website/index.html#install-r-and-rstudio",
    "title": "Create a simple Markdown/GitHub website in less than 30 minutes",
    "section": "Install R and RStudio",
    "text": "Install R and RStudio\nIf you don‚Äôt already have it on your computer, install R first. You can pick any mirror you like. If you already have R installed, make sure it is a fairly recent version. If yours is old, I suggest you update (install a new R version).\nOnce you have R installed, install the free version of RStudio Desktop. Again, make sure it‚Äôs a recent version. If you have an older verion of RStudio, you should update.\nInstalling R and RStudio should be fairly straightforward. If you want some more details or need instructions, see this page (which is part of an online course I teach)."
  },
  {
    "objectID": "posts/2021-01-11-simple-github-website/index.html#get-github-up-and-running",
    "href": "posts/2021-01-11-simple-github-website/index.html#get-github-up-and-running",
    "title": "Create a simple Markdown/GitHub website in less than 30 minutes",
    "section": "Get Github up and running",
    "text": "Get Github up and running\nIf you are new to Github, you need to create an account. At some point, it would also be useful to learn more about what Git/Github is and how to use it, but for this purpose you actually don‚Äôt need to know much. If you want to read a bit about Git/Github, see e.g.¬†this document, which I wrote for one of my courses.. But for now, you don‚Äôt need to know much about Git/Github."
  },
  {
    "objectID": "posts/2021-01-11-simple-github-website/index.html#install-gitkraken-optional-but-assumed",
    "href": "posts/2021-01-11-simple-github-website/index.html#install-gitkraken-optional-but-assumed",
    "title": "Create a simple Markdown/GitHub website in less than 30 minutes",
    "section": "Install Gitkraken (optional but assumed)",
    "text": "Install Gitkraken (optional but assumed)\nThere are many ways you can interact with Git/Github. I mosty use the fairly user-friendly and full-featured Gitkraken. You can get a basic version for free, if you are a student, you can get the Pro version through the Github developer pack, teachers can get it through the Github teacher toolbox. If you qualify for either, I highly recommend signing up. But you don‚Äôt need it for our purpose.\nI assume for the rest of the post that you are using Gitkraken. If you have your own preferred Git/Github client (e.g.¬†the one that comes with RStudio), you can of course use that one too. Make sure you connect Gitkraken to your Github account."
  },
  {
    "objectID": "posts/2021-09-30-tidy-tuesday-exploration/index.html",
    "href": "posts/2021-09-30-tidy-tuesday-exploration/index.html",
    "title": "Analysis of economic papers",
    "section": "",
    "text": "This analysis was performed as part of an exercise for my Modern Applied Data Analysis course taught in fall 2021.\nOne of the weekly assignments for the students is to participate in Tidy Tuesday. I did the exercise as well, this is my product. You can get the R Markdown/Quarto file to re-run the analysis here.\n\nIntroduction\nIf you are not familiar with Tidy Tuesday, you can take a quick look at the TidyTuesday section on this page.\nThis week‚Äôs data was about an analysis of economic working papers catalogued by NBER. More on the data is here.\n\n\nLoading packages\nMake sure they are installed. Note: I don‚Äôt like loading meta-packages, such as the tidyverse. Doing so makes it really hard to figure our which packages are actually used. So I prefer to only load what I need.\n\nlibrary('ggplot2')\nlibrary('readr')\nlibrary('dplyr')\nlibrary('stringr')\nlibrary('tidytuesdayR')\n\n\n\nData loading\nApparently there is now a tidytuesdayR package that makes data loading very easy!\n\n#tuesdata &lt;- tidytuesdayR::tt_load('2021-09-28')\n\nWell, that command above failed, claiming that date didn‚Äôt exist. So loading the data manually after all.\n\npapers &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-09-28/papers.csv')\n\nRows: 29434 Columns: 4\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (2): paper, title\ndbl (2): year, month\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nauthors &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-09-28/authors.csv')\n\nRows: 15437 Columns: 4\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (4): author, name, user_nber, user_repec\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nprograms &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-09-28/programs.csv')\n\nRows: 21 Columns: 3\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (3): program, program_desc, program_category\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npaper_authors &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-09-28/paper_authors.csv')\n\nRows: 67090 Columns: 2\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (2): paper, author\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npaper_programs &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-09-28/paper_programs.csv')\n\nRows: 53996 Columns: 2\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (2): paper, program\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nUnderstanding the data\nThe explanations on the Tidy Tuesday website were a bit confusing. Looks like each CSV file only has a few variables, and papers.csv does not contain all the variables listed in the readme.\n\ncolnames(papers)\n\n[1] \"paper\" \"year\"  \"month\" \"title\"\n\ncolnames(authors)\n\n[1] \"author\"     \"name\"       \"user_nber\"  \"user_repec\"\n\ncolnames(programs)\n\n[1] \"program\"          \"program_desc\"     \"program_category\"\n\ncolnames(paper_authors)\n\n[1] \"paper\"  \"author\"\n\ncolnames(paper_programs)\n\n[1] \"paper\"   \"program\"\n\n\nPapers seem to be linked to authors by the paper_authors file and to programs (areas of work) by the paper_programs file. Probably best to combine all into one data frame. The Tidy Tuesday website has some code for that already, let‚Äôs see if it works\n\ndf &lt;- left_join(papers, paper_authors) %&gt;% \n  left_join(authors) %&gt;% \n  left_join(paper_programs) %&gt;% \n  left_join(programs)%&gt;% \n  mutate(\n    catalogue_group = str_sub(paper, 1, 1),\n    catalogue_group = case_when(\n      catalogue_group == \"h\" ~ \"Historical\",\n      catalogue_group == \"t\" ~ \"Technical\",\n      catalogue_group == \"w\" ~ \"General\"\n    ),\n    .after = paper\n  ) \n\nJoining with `by = join_by(paper)`\nJoining with `by = join_by(author)`\nJoining with `by = join_by(paper)`\n\n\nWarning in left_join(., paper_programs): Detected an unexpected many-to-many relationship between `x` and `y`.\n‚Ñπ Row 111 of `x` matches multiple rows in `y`.\n‚Ñπ Row 2 of `y` matches multiple rows in `x`.\n‚Ñπ If a many-to-many relationship is expected, set `relationship =\n  \"many-to-many\"` to silence this warning.\n\n\nJoining with `by = join_by(program)`\n\nglimpse(df)\n\nRows: 130,081\nColumns: 12\n$ paper            &lt;chr&gt; \"w0001\", \"w0002\", \"w0003\", \"w0004\", \"w0005\", \"w0006\",‚Ä¶\n$ catalogue_group  &lt;chr&gt; \"General\", \"General\", \"General\", \"General\", \"General\"‚Ä¶\n$ year             &lt;dbl&gt; 1973, 1973, 1973, 1973, 1973, 1973, 1973, 1973, 1973,‚Ä¶\n$ month            &lt;dbl&gt; 6, 6, 6, 7, 7, 7, 8, 9, 9, 9, 9, 9, 10, 10, 10, 10, 1‚Ä¶\n$ title            &lt;chr&gt; \"Education, Information, and Efficiency\", \"Hospital U‚Ä¶\n$ author           &lt;chr&gt; \"w0001.1\", \"w0002.1\", \"w0003.1\", \"w0004.1\", \"w0005.1\"‚Ä¶\n$ name             &lt;chr&gt; \"Finis Welch\", \"Barry R Chiswick\", \"Swarnjit S Arora\"‚Ä¶\n$ user_nber        &lt;chr&gt; \"finis_welch\", \"barry_chiswick\", \"swarnjit_arora\", NA‚Ä¶\n$ user_repec       &lt;chr&gt; NA, \"pch425\", NA, \"pli669\", \"psm28\", NA, NA, NA, \"pli‚Ä¶\n$ program          &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ program_desc     &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n$ program_category &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N‚Ä¶\n\n\nThat seems to have worked, now all data is in one data frame.\n\n\nExploration 1\nMy first idea is very idiosyncratic. A friend of mine is an Econ/Finance professor at Bocconi University. I‚Äôm going to see if he has any papers in that dataset.\n\nsmalldf &lt;- df %&gt;% filter(grepl(\"Wagner\",name))\nprint(unique(smalldf$name))\n\n [1] \"Joachim Wagner\"       \"Alexander F Wagner\"   \"Todd H Wagner\"       \n [4] \"Stefan Wagner\"        \"Mathis Wagner\"        \"Ulrich J Wagner\"     \n [7] \"Wolf Wagner\"          \"Gernot Wagner\"        \"Zachary Wagner\"      \n[10] \"Katherine R H Wagner\" \"Myles Wagner\"         \"Kathryn L Wagner\"    \n\n\nNot seeing him in there. Just to make sure, a check on first name.\n\nsmalldf &lt;- df %&gt;% filter(grepl(\"Hannes\",name))\nprint(unique(smalldf$name))\n\n[1] \"Hannes Schwandt\"\n\n\nOk, nothing. Might be that his area, finance, is not indexed by NBER. I don‚Äôt know enough about the econ/business/finance fields enough to know what is and isn‚Äôt part of NBER. So I guess moving on.\n\n\nExploration 2\nIn most areas of science and when looking at publication records, one finds that most people publish very little (e.g.¬†a student who is a co-author on a paper, then goes into the ‚Äúreal world‚Äù and never publishes again) and a few people publish a lot (super-star and/or old faculty). One usually sees an 80/20 pattern or a distribution that follows a power law. Let‚Äôs see what we find here.\nFirst, I‚Äôm doing a few more checks.\n\n#looking at missing data for each variable\nnas &lt;- colSums(is.na(df))\nprint(nas)\n\n           paper  catalogue_group             year            month \n               0                0                0                0 \n           title           author             name        user_nber \n               0                0                0             2112 \n      user_repec          program     program_desc program_category \n           47158              530              530             1516 \n\n\nMissing data pattern seems ok. To be expected that some users don‚Äôt have those NBER or REPEC IDs.\nLet‚Äôs look at number of unique papers and unique authors.\n\nn_authors = length(unique(df$author))\nn_papers = length(unique(df$title))\nprint(n_authors)\n\n[1] 15437\n\nprint(n_papers)\n\n[1] 29419\n\n\nComparing those numbers to the original data frames, we see that the number of authors is same as in original authors data frame, that‚Äôs good. Number of papers (or at least unique titles) is less. Seems like some papers have the same titles? Non-unique titles is confirmed by checking the ID for each paper, which is the same as the one in the original papers CSV file.\nLet‚Äôs look at those titles that show up more than once. Note that we need to do that with the original papers data frame, since the merged one contains many duplicates since each author gets their own row.\n\n#using base R here, can of course also do that with tidyverse syntax\ndfdup &lt;- papers[duplicated(papers$title),]\nprint(dfdup$title)\n\n [1] \"The Wealth of Cohorts: Retirement Saving and the Changing Assets of Older Americans\"        \n [2] \"Empirical Patterns of Firm Growth and R&D Investment: A Quality Ladder Model Interpretation\"\n [3] \"The Market for Catastrophe Risk: A Clinical Examination\"                                    \n [4] \"Taxation and Corporate Financial Policy\"                                                    \n [5] \"Asset Pricing with Heterogeneous Consumers and Limited Participation: Empirical Evidence\"   \n [6] \"Tax Incidence\"                                                                              \n [7] \"Liquidity Shortages and Banking Crises\"                                                     \n [8] \"Legal Institutions and Financial Development\"                                               \n [9] \"Inequality\"                                                                                 \n[10] \"Predictive Systems: Living with Imperfect Predictors\"                                       \n[11] \"Corruption\"                                                                                 \n[12] \"The Dynamic Properties of Financial-Market Equilibrium with Trading Fees\"                   \n[13] \"Forward Guidance\"                                                                           \n[14] \"Labor Market Integration Before the Civil War\"                                              \n[15] \"The Impact of Globalization on Pre-Industrial, Technologically Quiescent Economies\"         \n\n\nSome titles I can easily seen being used more than once, e.g.¬†a paper called ‚ÄúCorruption‚Äù. Others sound very unique, so not sure why they show up as duplicates. If this were a serious analysis, I would look more closely into that. But for this exercise, and since it‚Äôs just a few titles, I‚Äôll ignore and move on.\nI want to look at publications per author. Since names might not be unique but NBER ID should be, I‚Äôm just going to remove those authors that don‚Äôt have an NBER ID (likely most of them have co-authored very few papers) and focus on the remaining authors. For each, I‚Äôll count their total papers by counting how often they show up.\n\ndfnew &lt;- df %&gt;% filter(!is.na(user_nber)) %&gt;% \n                group_by(user_nber) %&gt;% \n                summarise(n_papers = n() ) %&gt;%\n                arrange(desc(n_papers)) %&gt;%\n                mutate(allpapers = cumsum(n_papers)) \ndfnew$id = 1:nrow(dfnew) #add an ID variable for plotting\n\nLooking at the histogram of number of papers for each author.\n\np1 &lt;- dfnew %&gt;% ggplot(aes(n_papers)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nLooks like expected, most authors have published only a few papers, a few have a lot.\nAnother way to look at this is with a violin plot\n\np1a &lt;- dfnew %&gt;% ggplot(aes(x=1, y=n_papers)) + geom_violin() \nplot(p1a)\n\n\n\n\n\n\n\n\nThat‚Äôs a very flat violin plot, with almost all the density close to 1.\nDoing a quick numerical summary\n\nsummary(dfnew$n_papers)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   2.000   3.000   8.983   7.000 359.000 \n\n\nOf course one needs to have at least 1 paper published to be in there, so that‚Äôs the minimum. The median is 3, so half of individuals have published 3 or less. The mean is higher, as expected for skewed data, and the max is at 359 papers, confirming the histogram which shows a few individuals wrote a lot of papers.\nJust because, let‚Äôs look at the 10 most published authors\n\nhead(dfnew,n=10)\n\n# A tibble: 10 √ó 4\n   user_nber        n_papers allpapers    id\n   &lt;chr&gt;               &lt;int&gt;     &lt;int&gt; &lt;int&gt;\n 1 jonathan_gruber       359       359     1\n 2 james_heckman         331       690     2\n 3 daron_acemoglu        308       998     3\n 4 janet_currie          306      1304     4\n 5 michael_bordo         297      1601     5\n 6 edward_glaeser        291      1892     6\n 7 joshua_aizenman       284      2176     7\n 8 martin_feldstein      272      2448     8\n 9 andrei_shleifer       242      2690     9\n10 alan_taylor           239      2929    10\n\n\nSince this is not my field, I don‚Äôt know any of those individuals. But looks like the 1st one, Gruber, is somewhat famous and also worked at NBER, so maybe not surprising that his papers are in there. Not sure, I don‚Äôt know how exactly NBER works, but it‚Äôs a quick consistency check and no red flag.\nThis looks at the accumulation of papers for the first N authors, with number of authors on the x-axis and total papers on the y axis. If every author were to contribute the same number of papers, we‚Äôd see a straight line up the diagonal. The fact that some authors write more papers, and most just a few, pushes the curve to the upper left corner. I‚Äôm also plotting a few lines that show the 80/20 idea, i.e.¬†the vertical line indicates 20% of authors, the horizontal indicates 80% of all published papers.\n\nnobs = nrow(dfnew)\ntotpapers = max(dfnew$allpapers)\np2 &lt;- dfnew %&gt;% ggplot(aes(x=id, y=allpapers)) + \n                geom_point() + \n                theme(legend.position=\"none\") + \n               geom_segment(aes(x = floor(nobs*0.2), y = 1, xend = floor(nobs*0.2), yend = floor(totpapers*0.8) ), colour = \"red\", linetype=\"dashed\", size=1) +\n geom_segment(aes(x = 1, y = floor(totpapers*0.8), xend = floor(nobs*0.2), yend = floor(totpapers*0.8) ), colour = \"red\", linetype=\"dashed\", size=1) \n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n\nplot(p2)\n\n\n\n\n\n\n\n\nLooks like the NBER papers are fairly close to that 80/20 distribution, with few authors contributing the bulk, and most authors contributing little.\nNote that this does not account for co-authorship, just doing a per-author count.\n\n\nFurther explorations\nI‚Äôm going to leave it at this for now. In contrast to my 2019 Tidy Tuesday exploration I won‚Äôt try a fake statistical analysis here.\nBut I can think of a few other ideas and things to explore Here are a few:\n\nThe Tidy Tuesday website had a link to this article which looks at gender representation among the papers/authors. We could do that here too, e.g.¬†follow their approach to try and guess gender for authors, then could stratify number of papers by gender of author.\nAnother possible exploration would be to look at the numbers of papers per author based on the area of research, i.e.¬†the programs variable.\nYet another analysis one could do is to look at the pattern of publication for those that publish a good bit (say over 50 papers) and see how numbers of papers per year changes, or how number of co-authors changes over the years.\n\n\n\nSummary\nPatterns of authorship have been explored often. Sometimes they lead to useful information. At other times, one just needs more or less meaningful numbers for career advancement purposes. See e.g.¬†an analysis of my own papers I did and wrote up in this post covering Google scholar data and this post using the bibliometrix package.\nMy exploration here was not too thorough, but some expected patterns showed up, namely the 80/20 skewed distribution in authorship."
  },
  {
    "objectID": "posts/2022-10-01-hugo-to-quarto-migration/index.html",
    "href": "posts/2022-10-01-hugo-to-quarto-migration/index.html",
    "title": "Migration from Hugo/blogdown/Wowchemy to Quarto",
    "section": "",
    "text": "Motivation\nLike many folks who use R, RMarkdown and related products a lot, I was excited about the blogdown package and used it to build my website with Hugo. Also as many did, I used the Wowchemy (back then called Academic) theme. I wrote about that setup here.\nThe results were nice, but both Hugo and Wowchemy kept changing all the time and stuff kept breaking. It wasn‚Äôt robust, it got rather complex and fiddly, etc. I started dreading making changes to the website. Enter Quarto. I first watched from the sidelines thinking that my current setup was ok and I didn‚Äôt need to change (yet). But after a few online presentations, I was convinced that Quarto was the new way forward and would in the long run provide a much better experience.\nI therefore decided to tackle the website conversion project. I had a lot of help from others who already did this (I‚Äôll provide links below).\nI wanted the new website to be as close in content/functionality as the old website. Since I didn‚Äôt have 100s of pages to convert, I was ok doing some manual adjustments. Here is a brief summary of what I did and some pointers to a few things I learned that might help others.\n\n\nWho this is (not) for\nThis most directly targets folks who want to switch from blogdown/Hugo to Quarto. I‚Äôm hosting things on Netlify, but it should work for other hosting platforms too. Some of the tips might also be useful for folks who plan to build a Quarto website from scratch.\nThis is not a detailed walk-through. For that, see the Quarto documentation or for instance this blog post. I‚Äôll describe a lot of steps only briefly, and make comments on some topics that might be not yet commonly known.\n\n\nSetting up the website\nI started by creating a new Quarto website. Either the Quarto documentation or these blog posts by Danielle Navarro and Albert Rapp worked well.\nFor the main page, I simply used the about page template that is built into Quarto. (While my about.qmd page is just a regular page.)\nI structured the new website to be as similar to my old as possible. For me, that meant folders and subfolders for posts, presentations and projects, and all other files (e.g.¬†about.qmd) in the top level.\nNote that while I have posts and presentations separately, and under the old setup those pages were somewhat different, with Quarto there is (currently) no separate styling for presentations, thus it is basically another collection of posts. Only in this case, each post just contains the basic information of the presentation and a link to the slides and other relevant material. The same is true for the projects, they follow again the same structure as the posts, just somewhat differently formatted.\nIt would be possible to tune more, and make the presentations entries display differently. But I wanted to keep it simple, I learned that too much customization is just a time suck for me üòÅ.\nFor the projects, I made a change. They are also set up like posts, but I wanted the image that‚Äôs shown as thumbnail to also show on each page explaining a project. That was the way it was on my old site. I was thinking of messing with the project pages, but realized I could just use one of the ‚Äòabout‚Äô page layout templates and that would do the trick. So I just added the solona template into the YAML of each file and got a layout that looked good enough for what I wanted.\n\n\nConfiguring the site\nThe main file for setting configurations is the _quarto.yml file. Additional configurations can be done in _metadata.yml inside the posts and presentations folders. I followed a mix of the Quarto documentation and the blog posts mentioned above and below to configure those files. You can check out my setup on the GitHub repo of this page.\n\n\nConverting YAML sections\nI had to do a bunch of manual conversions of all the index.qmd files that go with each post and presentation. It was a bit tedious but not too bad. Basically, I had to remove anything from the YAML that was specific for Hugo/Wowchemy and format it to have the fields supported by Quarto. I found Danielle‚Äôs post to be helpful for a quick orientation. (Note that she converts from Distill, so her starting point is slightly different, but the new Quarto entries are the same.)\n\n\nSetting up a 404/not found page\nThis is not really required, but I liked this approach from Lucy D‚ÄôAgostino McGowan and decided to mostly copy it and adjust a bit for my purposes.\n\n\nTurning on comments\nI used utterances before to allow folks to comment on certain sites. This can be done easily as explained in this Quarto documentation section. To prevent comments on certain pages, one can turn those off in the YAML, also also described on the Quarto documentation page. An alternative is to place the utterances information into the _metadata.yml files, which then means comments are only on for those specific files/folders, in my case the blog posts and the presentations.\n\n\nHTML form embedding\nUpdate 2023-01-09: At the end of 2022, I decided to stop doing my newsletter for the time being and turned off the newsletter subscription section. Leaving this here since it might still be of general interest to figure out how to embed HTML.\nFor my newsletter subscription page, I embed HTML code produced by Mailchimp. It took me a little bit to figure out how to do it. This tip explained it, basically the setup looks like this:\n```{=html}\nALL THE HTML STUFF FROM MAILCHIMP GOES HERE\n```\n\n\nRedirects 1\nMy previous website used slug: labels in the YAML to provide shortened URLs. I wanted to keep those old URLs so breaking links are minimized. First I was thinking of following this example by Danielle Navarro and adjusting the code such that it parses the YAML of all my index.qmd files, pulls out the slug part and builds the paths and redirect table. It should have worked for me since I‚Äôm also hosting this website on Netlify, like Danielle.\nBut then I found a simpler option, using the aliases: setting, here is a an explanation/example. I tried to do an automated search and replace, but it didn‚Äôt quite work, so I ended up doing it manually. I basically replaced this:\nslug: publications-analysis-1\nwith this:\naliases: \n  - ../publications-analysis-1/\nin the YAML of all my posts (my presentations didn‚Äôt use slugs).\nNote on this: I ran into some problems initially. Since I was mapping into the same file structure, just a different file name, I wrote this:\naliases: \n  - publications-analysis-1\nThat didn‚Äôt work. On contemplation, the alias is referenced to the current document. So if my file is https://www.andreashandel.com/posts/2020-02-01-publications-analysis-1/ I need to move up a folder and place the alias folder/file publications-analysis-1 there. That was confusing on first try. I only figured it out once I looked into the _site folder to see what Quarto was producing, and from that deduced the right setup.\n\n\nRedirects 2\nAfter I got the aliases bit to work, I realized that I needd further redirects. On my old blog, I had andreashandel.com/talk redirect to andreashandel.com/presentations. The first URL is on many of my presentation slides and I didn‚Äôt want to change them all. So I figured I should use Danielle‚Äôs approach after all, and make a small _redirects file that contained these. Basically it looks like this:\n/talk /presentations\n/talks /presentations\n/post /posts\nI followed her instructions of placing this bit of code into the main index.qmd file.\nUpdate 2023-07-06: I noticed that sometimes when I build the website, the whole _site folder gets erased and rebuilt (which also deletes the _redirects file). But the index.qmd file might not get re-executed since I‚Äôm using the execute: freeze option in my _quarto.yml file to prevent running everything each time. This means the code snippet inside index.qmd which makes the _redirects file isn‚Äôt run. The solution I found was to use Quarto‚Äôs Post-render functionality. Basically I placed the few lines of code that I got from Danielle‚Äôs example into a separate makeredirects.R script and added that to the main _quarto.yml file as\nproject:\n  post-render:\n    - makeredirects.R\nIt now runs each time after the rendering and produces the needed redirect file.\n\n\nWidgets\nOn my previous website, I had several sections (widgets) on the main page under the static welcome text. Those showed my Twitter feed and most recent posts and presentations. I was trying to see if I can reproduce that with Quarto. Based on this example it seems one could do something like that. I contemplated giving it a try. But then I decided to keep it simple, and let interested readers klick on my Twitter/Posts/Presentations sections if they want. No need to complicate things üòÅ.\n\n\nCustom footer\nI had a footer with copyright text that I wanted to keep. Albert Rapp‚Äôs post has an example of using an html file. I had that setup on my previous site, but I didn‚Äôt quite like the inclusion of html. I instead added the footer using the page-footer section in _quarto.yml as described here. I basically copied this statement from one of Andrew Heiss‚Äô courses. To get the alignment right, I also had to grab a bit of code out of his custom.scss file and stick it int my custom.css file (I don‚Äôt really know the difference between scss and css, seemed easier to place it into the css file.)\n\n\nExtensions\nTo get the nice icons in the footer that Andrew has, one needs to install an extension. In this case, it‚Äôs the fontawsome extension. This is easily done, as described here. or here. I expect many more cool extensions to show up soon.\n\n\nOther settings\nI‚Äôm using Google Analytics, it is easy to include that in a Quarto website. I also turned on the Cookies setting notification (I don‚Äôt use any cookies directly, but Google Analytics likely does, and probably Mailchimp that I use for my newsletter too?).\n\n\nPublishing\nI host my website on Netlify. I followed the Quarto documentation. First I did a quarto publish netlify. That placed my website onto Netlify and gave it a temporary URL. That was great for testing it online (as opposed to testing locally with Quarto‚Äôs preview option, which of course I used a lot as well). Once online, I ran a link checker (I like using Dr Link Check but there are many others.) Of course there were broken internal links, so I went ahead and fixed them. I decided that I like the somewhat manual publishing of the site instead of doing it automatically with GitHub integration (also an option described in the Quarto documentation, and what I was using). The manual approach means I can mess around with new blog posts and sync with GitHub and don‚Äôt have to worry about using the main branch or not and then only once things look good do I publish with Quarto. So I changed my setup to that. That was done easily by setting the information in the _publish.yml file to point to my actual website URL, and changing the settings on the Netlify side as described in the Quarto documentation.\n\n\nSummary\nOverall, it wasn‚Äôt too hard. The one item that got me stuck for a bit was the aliases issue as described above. The conversion did require some manual changing, which I‚Äôm sure I could have written R scripts for it, but since my webpage isn‚Äôt that large, it seemed easier to just do things by hand. The new site is slightly different, some parts are simplified, but I got pretty much the same functionality and content back. And it is a much simpler setup compared to the - in my opinion - fairly convoluted setup of Hugo/blogdown/Wowchemy. Overall I‚Äôm happy with the results. My research group website is still using blogdown/Hugo/Wowchemy. That one has a few more custom layout features, which would likely require some fiddling before they work in Quarto. However, I have high hopes for those extensions and I‚Äôm pretty sure soon someone will have made new layouts, and then I might just be able to use some of them. So research group website conversion will happen, though probably not this year üòÑ.\n\n\nFurther resources\nIn addition to the main Quarto website and the blogs mentioned above, the a Quarto tip a day by Mine √áetinkaya-Rundel and Thomas Mock‚Äôs materials (look for his Quarto training repositories) are great resources. This repo has materials for one of his workshops, here is another one. This tutorial by Isabella Vel√°squez is another nice resource describing specifically Quarto blogs.\n\n\nAcknowledgments\nIt should be obvious that I owe a lot of these ideas to the blog posts and other resources I cite above. So thanks to those who tried it before me and wrote about it!"
  },
  {
    "objectID": "posts/2023-08-04-time-impact-AI/index.html",
    "href": "posts/2023-08-04-time-impact-AI/index.html",
    "title": "Time, Impact and AI",
    "section": "",
    "text": "Introduction\nI don‚Äôt know why, but for some reason I was recently thinking about the relation between time invested in a task or project and the return one gets from it. At the same time, I have been exploring how to use AI tools productively. Those two strands of thinking ended up merging in my head and I had the idea for this maybe somewhat strange post.\n\n\nTime versus Impact\nI was contemplating the impact of various projects I‚Äôm working on, and how that is related to the time dedicated to each project. I ended up conceptualizing them as a sigmoid curve (see below). The next question I asked myself is how to optimize those curves. This thought merged with my current interest in exploration of AI tools and how they might be able to help in that optimization. So I decided to write a post about it, and use AI to help me do it.\n\n\nAI as helper\nFirst, I asked ChatGPT 3.5 to make a ggplot2 figure using R code that shows the sigmoid relation between time and imact I had in mind. Here is the prompt I used and the code I got.\n\n\n\n\n\n\nNote\n\n\n\nChatGPT 3.5 prompt\nWrite well-documented R code that creates a ggplot2 figure Time from 0 to 100 on the x-axis and Impact from 0 to 1 on the y-axis. Model the relation between Time and Impact as a 4-parameter sigmoid function. Give the model parameters meaningful names. Do not include any model fitting.\n\n\nThe following code chunk is the un-edited return from ChatGPT.\n\n# Load required libraries\nlibrary(ggplot2)\n\n# Function to calculate 4-parameter sigmoid function\nsigmoid_function &lt;- function(time, a, b, c, d) {\n  1 / (1 + exp(-a * (time - b))) - d + c\n}\n\n# Define model parameters with meaningful names\n# a: controls the slope of the sigmoid function\n# b: controls the midpoint of the sigmoid function on the x-axis\n# c: controls the vertical shift of the sigmoid function on the y-axis\n# d: controls the horizontal shift of the sigmoid function on the y-axis\na &lt;- 0.5\nb &lt;- 50\nc &lt;- 0.2\nd &lt;- 0.1\n\n# Generate data for Time and Impact (replace with your actual data if available)\ntime &lt;- seq(0, 100, length.out = 100)\nimpact &lt;- sigmoid_function(time, a, b, c, d)\n\n# Create a data frame\ndata_df &lt;- data.frame(Time = time, Impact = impact)\n\n# Create the ggplot2 figure\nggplot(data_df, aes(x = Time, y = Impact)) +\n  geom_line() +\n  geom_point() +\n  labs(x = \"Time\", y = \"Impact\") +\n  ggtitle(\"4-Parameter Sigmoid Function\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nThe prompt and result I‚Äôm showing here was not my first try. I had to rephrase a few times. Initially it always wanted to give me code that involved fitting a model, which is not what I was after. But it still took only a few minutes to get what I needed. I estimate if I had hand-written the code, it would have taken around 10-20 minutes.\n\n\nTime-Impact optimization\nWhile that figure above shows the general pattern I had in mind relating time to impact, I wanted a few more curves to illustrate different scenarios. Again I asked an AI system for help, this time using Bing (which is using a version of ChatGPT).\n\n\n\n\n\n\nNote\n\n\n\nBing AI prompt using ‚ÄòBalanced‚Äô Mode\nWrite well-documented R code that creates a ggplot2 figure with Time from 0 to 100 on the x-axis and Impact from 0 to 1 on the y-axis. Model the relation between Time and Impact as a 4-parameter sigmoid function. Give the model parameters meaningful names. Do not include any model fitting. Show 4 different curves in the plot with different values for the model parameters. Add legend labels to the plot.\n\n\nThe code I got (see further below) was almost what I wanted, but I did want the parameter values for the 4 curves to have specific values to illustrate the ideas I describe below. Therefore, I manually edited to values. And I changed the labels for the curves to what I wanted them to say. The rest is code directly returned from the AI.\nNote that despite my request for meaningful parameter names, I only got letters. In some other iterations of me playing with it, I got it to produce more meaningful names (but other parts of the return weren‚Äôt what I wanted).\nThis code also didn‚Äôt quite parameterize the function the way I would do it, e.g. I would write it in a way that it‚Äôs easier to quickly identify the max, zero, slope and 50% parameters. But I figured this was good enough and the best single try I was able to get.\nIt would be very easy to combine the results from a few iterations into a version of the code that does everything I want. In fact, I found it rather useful to ask it to update the code. I think I‚Äôll explore an interactive session in another post. Here, I used the one return that was closest to what I wanted following a single prompt.\n\nlibrary(ggplot2)\n\n# Define the sigmoid function\nsigmoid &lt;- function(x, a, b, c, d) {\n  d + (a - d) / (1 + exp(-c * (x - b)))\n}\n\n# Define the parameters for the sigmoid function\na &lt;- 0.1\nb &lt;- 50\nc &lt;- 0.1\nd &lt;- 0\n\n# Create a data frame with Time values from 0 to 100\ndf &lt;- data.frame(Time = seq(0, 100))\n\n# Add columns for Impact values using different parameter values\ndf$Impact1 &lt;- sigmoid(df$Time, a = a, b = b, c = c, d = d)\ndf$Impact2 &lt;- sigmoid(df$Time, a = a, b = b * 0.5, c = c , d = d)\ndf$Impact3 &lt;- sigmoid(df$Time, a = 10*a, b = b , c = c, d = d)\ndf$Impact4 &lt;- sigmoid(df$Time, a = 10*a, b = b * 0.5, c = c, d = d)\n\n# Melt the data frame into long format for plotting\ndf.melted &lt;- reshape2::melt(df, id.vars = \"Time\")\n\n# Create the plot using ggplot2\nggplot(data = df.melted,\n       aes(x = Time,\n           y = value,\n           color = variable)) +\n  geom_line() +\n  scale_color_manual(values = c(\"red\", \"blue\", \"green\", \"purple\"),\n                     labels = c(\"Low Impact, slow\", \"Low Impact, quick\", \n                                  \"High Impact, slow\", \"High Impact, quick\")) +\n  labs(x = \"Time\",\n       y = \"Impact\",\n       title = \"Sigmoid Function with Four Curves\")\n\n\n\n\n\n\n\n\nBack to the topic at hand, time and impact.\nWhat I want to illustrate with these curves is the idea that some projects can be done faster or slower, and some have inherently more impact than others. Even if you can do a low-impact project fairly quickly, it‚Äôs still a low-impact project. Figuring out which projects to work on for the most impact is something that AI tools currently can‚Äôt help with, and it will likely take a while before they can be of substantial help on that end (though they might provide some useful pointers if one engages in a dialogue with the AI to try and reflect what one should work on).\nIt seems to me what AI tools currently can do is shift the curve along the x-axis, meaning they can potentially help reduce the time it takes to reach a certain level of outcome/impact.\nIn the figure above, that means the human has to figure out how to pick high-impact projects, while AI can be used to speed up the execution of projects, especially for manual and tedious tasks.\nFor instance, if had written this - arguably fairly low impact - blog post fully manually, including the coding, it would have probably taken me at least an hour. Using AI tools, I got it a good bit under an hour, and that includes me fiddling with the AI prompts to figure out how to best phrase things.\n\n\nFurther musings\nIn addition to writing code to illustrate my thought, I decided to ask the AI tools to explore the time-impact topic themselves. Here are my prompts\n\n\n\n\n\n\nNote\n\n\n\nChatGPT 3.5 prompt\nWrite a blog post discussing the relation between time and impact under the assumption of a sigmoid relationship and explain how AI can and cannot help to optimize this relation.\n\n\nHere is what I got.\n\nTitle: Unraveling Time-Impact Relationships with the Sigmoid Function: Leveraging AI for Optimization\nIntroduction:\nTime and impact are often interconnected in various scenarios, especially in fields such as business, healthcare, and social sciences. Understanding the relationship between these two variables can be critical for decision-making and strategic planning. One way to model this relationship is by using the sigmoid function. In this blog post, we will delve into the concept of the sigmoid function, its implications, and explore how Artificial Intelligence (AI) can assist in optimizing this critical relationship.\nThe Sigmoid Function:\nThe sigmoid function is a mathematical function that exhibits an S-shaped curve. It has four parameters: baseline (a), maximum impact (b), time to reach 50% impact (c), and steepness (d). The function smoothly transitions from the baseline value to the maximum impact as time progresses. The steepness parameter controls how rapidly this transition occurs.\nModeling Time-Impact with the Sigmoid Function:\nAssuming a sigmoid relationship between time and impact implies that as time increases, the impact will initially increase gradually, and at some point, it will reach a maximum value, leveling off afterward. The time to reach 50% impact (c) represents the inflection point, indicating the time at which the impact grows most rapidly.\nApplications in Real-Life Scenarios:\nBusiness: In marketing, understanding how a product‚Äôs impact changes over time can help strategize product launches, promotional campaigns, and identify peak impact periods. AI can analyze past sales data and customer behavior to identify patterns and forecast future impacts, aiding in better resource allocation.\nHealthcare: In pharmacokinetics, the sigmoid model is used to describe the relationship between drug concentration and its effect in the body over time. AI-powered drug dosing algorithms can optimize therapeutic outcomes while minimizing side effects based on this relationship.\nSocial Sciences: Social media engagement and information propagation exhibit sigmoid behavior. AI algorithms can analyze user engagement data to predict trends and virality of content, enabling targeted dissemination of information.\nAI‚Äôs Role in Optimizing Time-Impact Relationship:\nAI can play a significant role in optimizing the time-impact relationship in various ways:\nData Analysis: AI algorithms can efficiently analyze vast amounts of historical data to identify patterns and trends in time-impact relationships. This analysis can inform decision-making and resource allocation strategies.\nPredictive Analytics: AI can create predictive models based on historical data to forecast future impacts and their timelines. Such models help organizations plan for potential scenarios and make data-driven decisions.\nParameter Optimization: AI-driven optimization techniques can determine the best parameter values for the sigmoid function, enabling better fit to real-world data and more accurate predictions.\nAutomation: AI can automate repetitive tasks related to data collection, analysis, and reporting, allowing professionals to focus on higher-level decision-making.\nLimitations of AI in Optimizing Time-Impact Relationships:\nWhile AI can provide valuable insights and predictions, it does have some limitations:\nData Quality: AI models heavily depend on the quality and representativeness of the data. Biased or incomplete data can lead to inaccurate predictions and optimization.\nEthical Considerations: AI optimization must adhere to ethical standards, especially in areas like healthcare, where decisions can significantly impact individuals‚Äô lives.\nInterpretability: AI models, such as deep learning neural networks, can be complex and challenging to interpret, making it difficult to understand the underlying reasoning for certain predictions.\nConclusion:\nModeling the relationship between time and impact using the sigmoid function offers valuable insights into various real-life scenarios. Leveraging AI to optimize this relationship can lead to better decision-making, resource allocation, and improved outcomes in fields like business, healthcare, and social sciences. However, AI‚Äôs use must be mindful of data quality, ethical considerations, and the need for human interpretability to ensure responsible and beneficial optimization. As AI continues to advance, it holds great promise in unraveling the complexities of the time-impact relationship and enhancing our ability to make informed decisions for a better future.\n\nThe first 3 sections are fairly decent. After that, things get wonky and don‚Äôt make much sense.\nHere is another try, this time using Bing. It also runs ChatGPT but is connected to the internet, and thus behaves somewhat differently.\n\n\n\n\n\n\nNote\n\n\n\nBing AI prompt\nLook up online articles discussing the relation between time and impact, then write a post on that topic under the assumption of a sigmoid relation between the two quantities, and explain how AI can and cannot help to optimize this relation.\n\n\nHere is what I got.\n\nTime and impact are two important factors that affect the success of any project, goal, or endeavor. Time is the duration or period that is available or required for something to happen. Impact is the effect or influence that something has on another thing or situation. The relation between time and impact can be complex and nonlinear, depending on the context and the variables involved. One possible way to model this relation is using a sigmoid function, which is a mathematical function that has an S-shaped curve.\nA sigmoid function can be used to describe how the impact of an action or intervention changes over time. For example, suppose you want to improve your health by exercising regularly. At first, you may not see much impact from your exercise, as your body adapts to the new routine. This is the lower part of the S-curve, where the impact is low and increases slowly with time. However, as you continue to exercise, you may start to see more noticeable results, such as increased stamina, strength, and well-being. This is the middle part of the S-curve, where the impact is high and increases rapidly with time. Finally, as you reach your optimal level of fitness, you may not see much further improvement from your exercise, as your body reaches a plateau. This is the upper part of the S-curve, where the impact is high but increases slowly with time.\nThe sigmoid function can also be used to describe how the impact of an action or intervention varies with different levels of time investment. For example, suppose you want to learn a new skill by taking an online course. If you invest too little time in the course, you may not learn much from it, as you miss out on important concepts and practice opportunities. This is the lower part of the S-curve, where the time investment is low and the impact is low. However, if you invest a moderate amount of time in the course, you may learn a lot from it, as you grasp the key ideas and apply them effectively. This is the middle part of the S-curve, where the time investment is optimal and the impact is high. Finally, if you invest too much time in the course, you may not gain much more from it, as you encounter diminishing returns and opportunity costs. This is the upper part of the S-curve, where the time investment is high and the impact is low.\nAI can help to optimize the relation between time and impact in various ways. For example, AI can help to:\n\nAnalyze data and identify patterns that reveal how time and impact are related in different scenarios.\nPredict the optimal amount of time that should be invested in an action or intervention to maximize its impact.\nProvide feedback and guidance that help to adjust the time and impact levels according to changing goals and circumstances.\nAutomate or streamline tasks that are repetitive, tedious, or low-impact, saving time and resources for more impactful activities.\n\nHowever, AI cannot help to optimize the relation between time and impact in every situation. For example, AI cannot help to:\n\nAccount for human factors that affect how time and impact are perceived and valued by different individuals and groups.\nCapture the complexity and uncertainty that often characterize real-world problems and solutions.\nReplace human creativity and intuition that are essential for generating novel and innovative ideas and approaches.\nEnsure ethical and responsible use of time and impact data and decisions.\n\nTherefore, while AI can be a powerful tool for optimizing the relation between time and impact, it should not be used without human oversight and input. AI should complement rather than replace human judgment and agency in determining how best to use our time and make an impact.\n\nI find that second result pretty good. While some points miss the mark, others are spot-on. I could see myself editing this and incorporating good chunks of it in further musings on the topic (maybe with or without telling the world that AI helped).\n\n\nConclusion\nI‚Äôm certain I‚Äôm not the first one to think of the time versus impact relation. I didn‚Äôt bother searching around for similar write-ups. I‚Äôm sure they exist. Many are probably better than what I scribbled down. For me, the point of this blog post was to quickly write down my (likely not very original) thoughts on this idea, AND use AI tools to help me write the post. It worked surprisingly well. This is my first time using AI for something productive, instead of just playing with it. It certainly looks promising.\n\n\nFurther resources\nI‚Äôm just starting to learn about the new large-language model based AI tools and how to use them productively. I found some of the posts by Ethan Mollick to be very useful and informative.\n\n\nAcknowledgments\nAs always, I rarely come up with completely new stuff, instead I cobble it together from other resources. I‚Äôm sure the ideas of time versus impact have been floating around for a long time (e.g., as the 80/20 principle) and I likely read ideas of that sort many times in various books and online sources. I just can‚Äôt think of a specific one to point to that might have most likely inspired my thoughts. For the AI bits, I very much followed ideas by Ethan Mollink that he describes in several of his blog posts on the website linked above.\nOf course, thanks also goes to the creators of these pretty impressive AI tools, and all those folks whose work the AI engineers used (stole?) to train these AI systems. The ethics of this whole endeavor are certainly dicey, but not a topic I feel qualified to talk about."
  },
  {
    "objectID": "posts/2024-02-16-longitudinal-multilevel-bayes-6/index.html",
    "href": "posts/2024-02-16-longitudinal-multilevel-bayes-6/index.html",
    "title": "Bayesian analysis of longitudinal multilevel data - part 6",
    "section": "",
    "text": "This is an extension of a cmdstanr/Stan model to fit longitudinal data using Bayesian multilevel/hierarchical/mixed-effects models. It is a continuation of a prior series of posts. You should start at the beginning.\nHere is the Stan code for this example and this is the R script that runs everything."
  },
  {
    "objectID": "posts/2024-02-16-longitudinal-multilevel-bayes-6/index.html#full-model-structure",
    "href": "posts/2024-02-16-longitudinal-multilevel-bayes-6/index.html#full-model-structure",
    "title": "Bayesian analysis of longitudinal multilevel data - part 6",
    "section": "Full model structure",
    "text": "Full model structure\nThe rest of the model follow the previous one, just with more parameters. They all get their own equations. Here are the main components of the the new model.\n\n\\[\n\\begin{aligned}\n\\textrm{Outcome} \\\\\nY_{i,t}   \\sim \\mathrm{Normal}\\left(\\mu_{i,t}, \\sigma\\right) \\\\\n\\\\\n\\textrm{main model describing the virus trajectory} \\\\\n\\mu_{i,t} = \\log\\left( \\frac{2 \\exp(\\alpha_{i})}{e^{-\\exp(\\beta_{i})  (t_i - \\exp(\\gamma_{i}))} + e^{\\exp(\\eta_{i})  (t_i - \\exp(\\gamma_{i}))}}\\right).\\\\\n\\\\\n\\textrm{Deterministic models for main parameters} \\\\\n\\alpha_{i}   =  a_{0,i} + a_1 \\left(\\log (D_i) - \\log (D_m)\\right)  \\\\\n\\beta_{i}   =  b_{0,i} + b_1 \\left(\\log (D_i) - \\log (D_m)\\right) \\\\\n\\gamma_{i}   =  g_{0,i} + g_1 \\left(\\log (D_i) - \\log (D_m)\\right) \\\\\n\\eta_{i}   =  e_{0,i} + e_1 \\left(\\log (D_i) - \\log (D_m)\\right) \\\\\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/2024-02-16-longitudinal-multilevel-bayes-6/index.html#parameter-distributions",
    "href": "posts/2024-02-16-longitudinal-multilevel-bayes-6/index.html#parameter-distributions",
    "title": "Bayesian analysis of longitudinal multilevel data - part 6",
    "section": "Parameter distributions",
    "text": "Parameter distributions\nTo fully specify the model, we need to give all parameters distributions. Here are the distributions for the population-level parameters. These do not vary between individuals. I‚Äôm choosing values for the prior distributions assuming that dose will lead to an increase in peak, time to peak and growth rate, and a reduction in decay rate. I‚Äôm only indicating the values with an X below since I keep changing them in the code and then they are out of sync here. Check the R script to see the chosen values.\n\\[\n\\begin{aligned}\n\\textrm{population-level priors} \\\\\n\\sigma  \\sim \\mathrm{Exponential}(X)  \\\\\na_1 \\sim \\mathrm{Normal}(X,X) \\\\\nb_1 \\sim \\mathrm{Normal}(X,X) \\\\\ng_1 \\sim \\mathrm{Normal}(X,X) \\\\\ne_1 \\sim \\mathrm{Normal}(X,X) \\\\\n\\end{aligned}\n\\]\nIn addition, we allow some parameters to differ between individuals, and we‚Äôll implement hyper-parameters to allow these values to inform each other across individuals. This is again the adaptive pooling concept discussed previously.\nI‚Äôm setting values for the prior distributions such that the virus load curve looks somewhat reasonable, based on the quick exploration of the model above. Again, not showing exact values here to not create confusion between what I write here and potentially different values I end up using in the code (and forgetting to update here). See the code for the actual values.\n\\[\n\\begin{aligned}\n\\textrm{individal-level priors} \\\\\na_{0,i} \\sim \\mathrm{Normal}(\\mu_a, \\sigma_a) \\\\\nb_{0,i}  \\sim \\mathrm{Normal}(\\mu_b, \\sigma_b) \\\\\ng_{0,i} \\sim \\mathrm{Normal}(\\mu_g, \\sigma_g) \\\\\ne_{0,i}  \\sim \\mathrm{Normal}(\\mu_e, \\sigma_e) \\\\\n\\\\\n\\textrm{hyper priors} \\\\\n\\mu_a  \\sim \\mathrm{Normal}(X, X) \\\\\n\\mu_b  \\sim \\mathrm{Normal}(X, X) \\\\\n\\mu_g  \\sim \\mathrm{Normal}(X, X) \\\\\n\\mu_e  \\sim \\mathrm{Normal}(X, X) \\\\\n\\sigma_a  \\sim \\mathrm{Exponential}(X)  \\\\\n\\sigma_b  \\sim \\mathrm{Exponential}(X)  \\\\\n\\sigma_g  \\sim \\mathrm{Exponential}(X)  \\\\\n\\sigma_e  \\sim \\mathrm{Exponential}(X)  \\\\\n\\end{aligned}\n\\]\nAnd that‚Äôs the full model. The basic structure is the same as before, but the model is bigger because I‚Äôm now modeling the virus trajectory (given by \\(\\mu_{i,t}\\)) with 4 main parameters."
  },
  {
    "objectID": "posts/2024-02-16-longitudinal-multilevel-bayes-6/index.html#model-fitting-settings",
    "href": "posts/2024-02-16-longitudinal-multilevel-bayes-6/index.html#model-fitting-settings",
    "title": "Bayesian analysis of longitudinal multilevel data - part 6",
    "section": "Model fitting settings",
    "text": "Model fitting settings\nThese are the settings for the model fitting routine. Basically the same as before, only more initial conditions now because we have more parameters. And of course different values, since our model changed.\n\n#settings for fitting\nfs_m1 = list(warmup = 1500,\n             sampling = 2000, \n             max_td = 20, #tree depth\n             adapt_delta = 0.99999,\n             chains = 5,\n             cores  = 5,\n             seed = rngseed,\n             save_warmup = TRUE)\n\n\n# separate definition of initial values, added to fs_m1 structure \n# a different sample will be drawn for each chain\n# there's probably a better way to do that than a for loop\nset.seed(rngseed) #make inits reproducible\ninit_vals_1chain &lt;- function() (list(mu_a = runif(1,1,1.5), \n                                     mu_b = runif(1,0.8,1.2),\n                                     mu_g = runif(1,2,3),\n                                     mu_e = runif(1,0.2,0.6),\n                                     sigma_a = runif(1,0,1),\n                                     sigma_b = runif(1,0,1),\n                                     sigma_g = runif(1,0,1),\n                                     sigma_e = runif(1,0,1),\n                                     a0 = runif(Nind,1,1.5),\n                                     b0 = runif(Nind,0.8,1.5),\n                                     g0 = runif(Nind,1.5,2.5),\n                                     e0 = runif(Nind,0,1),\n                                     a1 = runif(1,0.5,0.6),\n                                     b1 = runif(1,0.1,0.1),\n                                     g1 = runif(1,0.1,0.1),\n                                     e1 = runif(1,-0.1,-0.1),\n                                     sigma = runif(1,0,1))\n                                )\ninits = NULL\nfor (n in 1:fs_m1$chains)\n{\n  inits[[n]] = init_vals_1chain()\n}\nfs_m1$init = inits"
  },
  {
    "objectID": "posts/2025-03-05-custom-quarto-divs/index.html",
    "href": "posts/2025-03-05-custom-quarto-divs/index.html",
    "title": "Custom formatting of components in Quarto with divs",
    "section": "",
    "text": "I want to create custom styling for different components of a Quarto based (teaching) website. I know Quarto comes with callout blocks, which is similar to what I want. I was searching online for some information, and happened to come across my own blog post from about 2 years ago - aparently I previously had the same question and figured it out, but since then completely forgot üôÑ.\nAs I was reading through my old post and looked at a few online sources, I decided that A) I was somewhat conceptually confused when I did the original blog post, and B) I needed things a bit tweaked. This new post are my notes on my latest thinking and attempt of styling."
  },
  {
    "objectID": "posts/2025-03-05-custom-quarto-divs/index.html#using-your-new-callout",
    "href": "posts/2025-03-05-custom-quarto-divs/index.html#using-your-new-callout",
    "title": "Custom formatting of components in Quarto with divs",
    "section": "Using your new callout",
    "text": "Using your new callout\nWell, it works. I can now write\n::: {.newgoals}\n## Learning Goals\n* Know how to make your own callouts.\n* Be able to mess with some SCSS/CSS styling.\n:::\nto get"
  },
  {
    "objectID": "posts/2025-03-05-custom-quarto-divs/index.html#learning-goals",
    "href": "posts/2025-03-05-custom-quarto-divs/index.html#learning-goals",
    "title": "Custom formatting of components in Quarto with divs",
    "section": "Learning Goals",
    "text": "Learning Goals\n\nKnow how to make your own callouts.\nBe able to mess with some SCSS/CSS styling."
  },
  {
    "objectID": "posts/2025-03-05-custom-quarto-divs/index.html#summary",
    "href": "posts/2025-03-05-custom-quarto-divs/index.html#summary",
    "title": "Custom formatting of components in Quarto with divs",
    "section": "Summary",
    "text": "Summary\nTo sum it all up, here is what you need to do:\n\nIf needed, create a new SCSS (or CSS) file and add the SCSS/CSS code chunks above to it.\nEdit your _quarto.yml file to include your new SCSS file.\nWrite .qmd documents using the fenced div (:::{}) approach shown above.\nKeep adding as many new styles as you want, and adjust them to however style you like."
  },
  {
    "objectID": "presentations/2019-07-hangzhou/index.html",
    "href": "presentations/2019-07-hangzhou/index.html",
    "title": "Introduction to Infectious Disease Modeling",
    "section": "",
    "text": "Preparation\n\nInstall R and (optional) Rstudio.\nInstall the DSAIDE R package and take a brief look. This website provides instructions.\nOptional: Read this paper which provides background reading for the ‚ÄúID Control of multiple outbreaks‚Äù app in DSAIDE.\n\n\n\nOutline\nThe following topics are covered in this workshop:\n\nIntroduction to infectious disease modeling\nSome example models\nHow to use simulation models\nTypes of models\nSources of uncertainty\nHow to build (good) models\nHow to assess modeling studies\nActive learning of infectious disease epidemiology\n\n\n\nPresentation Slides\nAll pdf slides as zip file"
  },
  {
    "objectID": "presentations/2019-10-paris/index.html",
    "href": "presentations/2019-10-paris/index.html",
    "title": "Model-based optimization of Vaccines inoculum dose",
    "section": "",
    "text": "We described some of our past and ongoing work in using a combination of models and data to understand the role of inoculum dose on vaccination and infection outcomes. The presentation slides (as pdf) are here."
  },
  {
    "objectID": "presentations/2020-02-reading-managing-publishing-papers/index.html",
    "href": "presentations/2020-02-reading-managing-publishing-papers/index.html",
    "title": "Reading, managing and publishing papers",
    "section": "",
    "text": "These are slides for a talk/discussion I led with our graduate students on how to efficiently read the (peer reviewed) literature, how to manage your references, and how to write and publish papers. The presentation slides are here."
  },
  {
    "objectID": "presentations/2020-04-gasouthern-covid/index.html",
    "href": "presentations/2020-04-gasouthern-covid/index.html",
    "title": "Some recent analysis and modeling applied to COVID-19",
    "section": "",
    "text": "These are slides for a talk discussing some recent COVID-19 projects. I gave the talk (online) at GA Southern. The presentation slides are here."
  },
  {
    "objectID": "presentations/2020-08-uga-ghi/index.html",
    "href": "presentations/2020-08-uga-ghi/index.html",
    "title": "Studying COVID-19 Spread and Control",
    "section": "",
    "text": "These are slides for a talk discussing some recent COVID-19 projects. I gave the talk (online) as part of UGA‚Äôs Global Health Institute seminar series. The presentation slides are here."
  },
  {
    "objectID": "presentations/2020-09-ubc/index.html",
    "href": "presentations/2020-09-ubc/index.html",
    "title": "Modeling COVID-19",
    "section": "",
    "text": "These are slides for a talk discussing some COVID-19 modeling projects. I gave the talk (online) at UBC.\nThe presentation slides are here.\nThe presentation slides are here."
  },
  {
    "objectID": "presentations/2020-10-vt/index.html",
    "href": "presentations/2020-10-vt/index.html",
    "title": "COVID-19: Modeling, Visualization and Data Analysis",
    "section": "",
    "text": "These are slides for a talk discussing some COVID-19 projects. I gave the talk (online) at Virginia Tech. The presentation slides are here."
  },
  {
    "objectID": "presentations/2020-11-pudong/index.html",
    "href": "presentations/2020-11-pudong/index.html",
    "title": "Simulation modeling to inform COVID-19 control and vaccination strategies",
    "section": "",
    "text": "These are slides for a talk discussing some recent projects on modeling COVID-19 spread and control. I gave a pre-recorded online talk as part of the International Symposium on the Prevention and Control of Infectious Diseases during the Pandemic of COVID-19 - The 4th Pudong Health Forum and the Yangtze River Delta Regional Public Health Symposium. The presentation slides are here."
  },
  {
    "objectID": "presentations/2021-01-building-your-brand/index.html",
    "href": "presentations/2021-01-building-your-brand/index.html",
    "title": "Building and curating your brand (online presence)",
    "section": "",
    "text": "These are slides for a discussion I led with our graduate students on the topic of online brand/presence building. The presentation slides are here."
  },
  {
    "objectID": "presentations/2021-02-U01/index.html",
    "href": "presentations/2021-02-U01/index.html",
    "title": "The Role of Inoculum Dose Following Infection or Vaccination",
    "section": "",
    "text": "These are slides for a talk discussing some of my past and present work on the role of inoculum dose for infection and vaccination. The presentation slides are here."
  },
  {
    "objectID": "presentations/2021-04-CIVIC-meeting/index.html",
    "href": "presentations/2021-04-CIVIC-meeting/index.html",
    "title": "A Comparison of High-Dose and Regular-Dose Seasonal Influenza Vaccines Toward Eliciting Homologous and Heterologous Immunity",
    "section": "",
    "text": "These are slides for a talk discussing the role of dose for the FluZone Influenza Vaccines.The presentation slides are here."
  },
  {
    "objectID": "presentations/2021-04-mentorship/index.html",
    "href": "presentations/2021-04-mentorship/index.html",
    "title": "How to be a good mentee and mentor",
    "section": "",
    "text": "These are slides for a talk/discussion I led with our graduate students discussing mentorship, both how to be a good mentee and mentor. The presentation slides are here."
  },
  {
    "objectID": "presentations/2021-07-useR/index.html",
    "href": "presentations/2021-07-useR/index.html",
    "title": "An R package to flexibly generate simulation model flow diagrams",
    "section": "",
    "text": "Since the package has changed a lot since this presentation, the recorded video is more confusing than helpful. I thus removed it. For the latest on the package, check out the package website!"
  },
  {
    "objectID": "presentations/2021-10-Metrum/index.html",
    "href": "presentations/2021-10-Metrum/index.html",
    "title": "Modeling the role of dose for Vaccines & some other projects",
    "section": "",
    "text": "These are the slides for a presentation I gave to team members of Metrum Research Group."
  },
  {
    "objectID": "presentations/2021-11-GASouthern/index.html",
    "href": "presentations/2021-11-GASouthern/index.html",
    "title": "Adventures in Data Analytics and Modeling - COVID-19 and Influenza",
    "section": "",
    "text": "These are the slides for a presentation I gave at GA Southern University."
  },
  {
    "objectID": "presentations/2022-04-York-University/index.html",
    "href": "presentations/2022-04-York-University/index.html",
    "title": "Assessing the impact of dose on infection and vaccination outcomes",
    "section": "",
    "text": "These are the slides for a presentation I gave at York University."
  },
  {
    "objectID": "presentations/2022-09-ECMTB/index.html",
    "href": "presentations/2022-09-ECMTB/index.html",
    "title": "The impact of seasonal Influenza Vaccines dose on homologous and heterologous immunity",
    "section": "",
    "text": "These are the slides for a presentation I gave at the 2022 ECMTB conference.\nNote: I had to cancel the trip at the last minute and thus was not able to deliver the presentation. I decide to leave the slides here anyway."
  },
  {
    "objectID": "presentations/2023-01-idp/index.html",
    "href": "presentations/2023-01-idp/index.html",
    "title": "Introduction to Individual Development Plans and the AAAS myIDP",
    "section": "",
    "text": "These are slides for a talk/discussion I led with our graduate students discussing Individual Development Plans, and specifically the AAAS myIDP plan. This is the kick-off presentation/discussion for a semester-long IDP development project done as part of the seminar class. The presentation slides are here.\nThis is a slightly updated version of a similar presentation from a few years ago."
  },
  {
    "objectID": "presentations/2023-05-CIVR/index.html",
    "href": "presentations/2023-05-CIVR/index.html",
    "title": "Drivers of heterologous influenza Vaccines antibody responses",
    "section": "",
    "text": "These are the slides for this presentation."
  },
  {
    "objectID": "presentations/2023-07-sismid/index.html",
    "href": "presentations/2023-07-sismid/index.html",
    "title": "Infectious Diseases, Immunology, and Within-Host Models",
    "section": "",
    "text": "The workshop website, is here. This is mainly for archival reasons. All the useful material is on a dedicated, stand-alone website called Simulation Modeling in Immunology."
  },
  {
    "objectID": "presentations/2023-10-Drake-FYOS/index.html",
    "href": "presentations/2023-10-Drake-FYOS/index.html",
    "title": "Introduction to infectious disease modeling",
    "section": "",
    "text": "These are the slides for this presentation."
  },
  {
    "objectID": "presentations/2023-10-MtSinai/index.html",
    "href": "presentations/2023-10-MtSinai/index.html",
    "title": "Analysis and Modeling to evaluate influenza Vaccines responses",
    "section": "",
    "text": "These are the slides for this presentation."
  },
  {
    "objectID": "presentations/2024-01-idp/index.html",
    "href": "presentations/2024-01-idp/index.html",
    "title": "Introduction to Individual Development Plans and the AAAS myIDP",
    "section": "",
    "text": "These are slides for a talk/discussion I led with our graduate students discussing Individual Development Plans, and specifically the AAAS myIDP plan. This is the kick-off presentation/discussion for a semester-long IDP development project done as part of the seminar class. The presentation slides are here.\nThis is a very slightly updated version of a similar presentation from past years ago."
  },
  {
    "objectID": "presentations/2024-05-UGA-fluday/index.html",
    "href": "presentations/2024-05-UGA-fluday/index.html",
    "title": "Assessing homologous and heterologous influenza Vaccines responses",
    "section": "",
    "text": "These are the slides for this presentation."
  },
  {
    "objectID": "presentations/2024-07-sismid/index.html",
    "href": "presentations/2024-07-sismid/index.html",
    "title": "Infectious Diseases, Immunology, and Within-Host Models",
    "section": "",
    "text": "For the 16th time, my colleague Paul Thomas and I taught our annual SISMID workshop. For the first time in Atlanta. The workshop website is here. Most materials are found on a dedicated, stand-alone website called Simulation Modeling in Immunology."
  },
  {
    "objectID": "presentations/2025-01-idp/index.html",
    "href": "presentations/2025-01-idp/index.html",
    "title": "Introduction to Individual Development Plans and the AAAS myIDP",
    "section": "",
    "text": "These are slides for a talk/discussion I led with our graduate students discussing Individual Development Plans, and specifically the AAAS myIDP plan. This is the kick-off presentation/discussion for a semester-long IDP development project done as part of the seminar class. The presentation slides are here.\nThis is a very slightly updated version of similar past presentations."
  },
  {
    "objectID": "presentations/2025-03-pudong-workshop/index.html",
    "href": "presentations/2025-03-pudong-workshop/index.html",
    "title": "Modeling the role of dose for infection and vaccination outcomes",
    "section": "",
    "text": "These are the slides for this presentation."
  },
  {
    "objectID": "projects/IDEMAbook/index.html",
    "href": "projects/IDEMAbook/index.html",
    "title": "Infectious Disease Epidemiology - a Model-based Approach",
    "section": "",
    "text": "An online book (perpetually under construction) convering infectious disease epidemiology using a model-based approach. Some parts of the book are fairly readable and complete enough that I use it when I teach a course on that topic. Other sections are currently only templates/outlines. While I try to ensure that what I write is correct, the whole book is not thoroughly fact-checked, error-corrected, properly referenced, etc. While I have been considering the idea of turning this into a full/real book, in my current thinking the cost-benefit analysis doesn‚Äôt pan out. I thus plan to leave it online for free as is, with occasional updates and fixes, but without an attempt to make it polished and complete enough for a printed book."
  },
  {
    "objectID": "projects/MADAcourse/index.html",
    "href": "projects/MADAcourse/index.html",
    "title": "Modern Applied Data Analysis",
    "section": "",
    "text": "Modern Applied Data Analysis (MADA) is a course I regularly teach online. All course materials are available in the form of a simple GitHub website and can be used by anyone for self-learning. You can find the course on this site."
  },
  {
    "objectID": "projects/dsaide/index.html",
    "href": "projects/dsaide/index.html",
    "title": "DSAIDE - Dynamical Systems Approach to Infectious Disease Epidemiology",
    "section": "",
    "text": "We developed an R package that teaches a modern, model-based approach to infectious disease epidemiology, without the need to write computer code. Learn more about it on the package website."
  },
  {
    "objectID": "projects/flowdiagramr/index.html",
    "href": "projects/flowdiagramr/index.html",
    "title": "Flowdiagramr",
    "section": "",
    "text": "We developed the R package flowdiagramr to allow anyone to easily create high-quality ggplot2 based flow diagrams of simulation models (and other flowcharts) with just a few lines of R code. Learn more about it on the package website."
  },
  {
    "objectID": "projects/modelbuilder/index.html",
    "href": "projects/modelbuilder/index.html",
    "title": "Modelbuilder",
    "section": "",
    "text": "We are developing an R package called modelbuilder that allows users to build and analyze compartmental, dynamical, mechanistic models (implemented as differential equations, discrete-time or stochastic), without the need to write computer code. Learn more about it on the package website."
  },
  {
    "objectID": "projects/resourcelist/index.html",
    "href": "projects/resourcelist/index.html",
    "title": "Resource list website",
    "section": "",
    "text": "A simple website with a collection of lists with links to various resources that are related to my research and teaching, as well as some general (academic) career content."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "",
    "text": "This is a tutorial and worked example, to illustrate how one can use the brms and rethinking R packages to perform a Bayesian analysis of longitudinal data in a multilevel/hierarchical/mixed-effects setup.\nI wrote it mainly for my own benefit/learning (nothing forces learning a concept like trying to explain it.) Hopefully, others find it useful too.\nIt started out as a single post, then became too large and is now a multi-part series. The original series of posts, which I wrote in 2022 and continue to slightly update occasionally, consists of these parts:\nIn 2024, I wrote a few additional posts on that topic:\nI generally place further resources and acknowledgments sections at the end. However, since this series seems to be expanding and the order in which one goes through the posts is a bit flexible, I‚Äôll list some major resources right here at the beginning, before starting with the tutorial sequence. There are still links to additional resources spread throughout the posts."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#model-for-outcome-the-likelihood",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#model-for-outcome-the-likelihood",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Model for outcome (the likelihood)",
    "text": "Model for outcome (the likelihood)\nFor our scenario, the outcome of interest (the log of the virus load) is continuous, which we assume to be normally distributed. Note that this is technically never fully correct, since there is some lower limit of detection for the virus load, which would lead to a truncation at low values. (A similar upper limit of detection does at times also occur.) If you have such censored data, you have to decide what to do about them. Here, we assume for simplicity that all values are far away from any limits, such that a normal distribution is a good approximation.\nMaking this normal distribution assumption, the equation describing the outcome (the likelihood model) is\n\\[\nY_{i,t}  \\sim \\mathrm{Normal}\\left(\\mu_{i,t}, \\sigma\\right)\n\\]\nThe \\(Y_{i,t}\\) are the measured outcomes (log virus load here) for each individual \\(i\\) at time \\(t\\). This is shown as symbols in the (simulated) data you can see in the figure above. The deterministic time-series trajectory for each individual is given by \\(\\mu_{i,t}\\) (shown as lines in the figure above). \\(\\sigma\\) captures variation in the data that is not accounted for by the deterministic trajectories.\nNote that you could assume a different distribution, based on the specifics of your data. For instance, if you had a time-series of counts, you could use a Poisson distribution. Some of the details would then change, e.g., you wouldn‚Äôt have a mean and standard deviation in your model, but instead a rate. But the overall setup described here will still work the same way."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#model-for-deterministic-trajectories",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#model-for-deterministic-trajectories",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Model for deterministic trajectories",
    "text": "Model for deterministic trajectories\nNext, we need to describe the underlying deterministic time-series model for the outcome.\nThere are different ways for choosing this part of the model. If you have enough information about your system to propose a mechanistic/process model, it is generally the best idea to go with such a model. Unfortunately, this is rare. Further, process models for time-series data are often implemented as differential equations, and those can take a very long time to fit.\nA more common approach is to model the overall pattern in the data with some type of phenomenological/heuristic function, chosen to match the data reasonably well. Generalized linear models (GLM), such as linear or logistic models, fall into that category. Here, we use such a phenomenological function, but a GLM won‚Äôt describe the pattern in our data (rapid virus growth, followed by decline). Therefore, we use an equation that gets us the shape we are looking for. For our simple example here, I choose a function that grows polynomially and declines exponentially with time. To be clear, this function doesn‚Äôt try to represent any real processes or mechanisms, it is simply chosen as an easy way to capture the general pattern seen in the virus load time-series. This is very similar to the use of GLMs or other standard models, which often work well at describing the overall pattern, without assuming a mechanistic process leading to the relation between predictor and outcome assumed by the GLM.\nThe equation for our model is given by\n\\[\n\\mu_{i,t} = \\log\\left( t_i^{\\alpha_i} e^{-\\beta_i t_i} \\right)  \n\\] In the model, \\(t_i\\) are the times for each individual \\(i\\) at which their outcome \\(Y_{i,t}\\) was measured. Those could be the same for each individual, which we‚Äôll do here for simplicity, but they could also be all at different times and things won‚Äôt change. The model parameters are \\(\\alpha_i\\) and \\(\\beta_i\\), and their values describe the trajectory for each individual.\nYou can convince yourself with the following bit of code that this function, for the right values of \\(\\alpha\\) and \\(\\beta\\), gives you ‚Äúup, then down‚Äù curves as a function of time. Note that since we are modeling the log of the virus load, I already log-transformed the equation.\n\nt = seq(0.1,30,length=100) #simulating 30 days, don't start at 0 to avoid 0/inf in plot\nalpha = 20; beta = 2; #just some values to show shape\nmu = log(t^alpha*exp(-beta*t)) #log virus load\nplot(t,mu, type = \"l\",ylim=c(0,30)) #looks somewhat like virus load in acute infections\n\n\n\n\n(Log) virus load time series for the heuristic model we will use.\n\n\n\n\nThe simple function I use here is in fact not that great for most real data, and better functions exist. See part 4 of this tutorial, where I show a more complex 4-parameter function, the one we actually used for our research problem. But to keep things somewhat simple here, I‚Äôm sticking with the 2-parameter function. It is fully sufficient to illustrate all the conceptual ideas I want to discuss. It can give us time-series which look somewhat similar to real virus load data often seen in acute infections. Of course, you need to pick whatever function describes your data reasonably well."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#numerical-trickeries",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#numerical-trickeries",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Numerical trickeries",
    "text": "Numerical trickeries\nLet‚Äôs go on a brief detour and discuss an important topic that comes up often.\nIn the equation for \\(\\mu_{i,t}\\) I just introduced, only positive values of \\(\\alpha\\) and \\(\\beta\\) produce reasonable trajectories. It is common to have parameters that can only take on certain values (e.g., positive, between 0-1). The problem is that by default, most fitting routines assume that the parameters that need to be estimated can take on any value. It turns out that the fitting software we will use (Stan through rethinking and brms) can be told that some parameters are only positive. You‚Äôll see that in action later. But with different software, that might not be possible. Further, as you‚Äôll also see below, we don‚Äôt actually fit \\(\\alpha\\) and \\(\\beta\\) directly, and it is tricky to enforce them to be positive using the built-in parameter constraint functionality of Stan.\nA general trick is to redefine parameters and rewrite the model to ensure positivity. Here, we can do that by exponentiating the parameters \\(\\alpha\\) and \\(\\beta\\) like this\n\\[\n\\mu_{i,t}  = \\log\\left( t_i^{\\exp(\\alpha_{i})} e^{-\\exp(\\beta_{i}) t_i} \\right)\n\\] Now, \\(\\alpha_i\\) and \\(\\beta_i\\) themselves can take any value without the danger of getting a nonsensical shape for \\(\\mu_{i,t}\\). It is likely possible to fit the model without taking those exponents and hoping that during the fitting process, the fitting routine ‚Äúnotices‚Äù that only positive values make sense. However, it might make the numerical procedures less robust.\nAnother alternative would be to enforce positive \\(\\alpha_i\\) and \\(\\beta_i\\) by setting up the rest of the model such that they can only take positive values. I‚Äôll show a version for that in part 4.\nOne issue with that exponentiation approach is that it can sometimes produce very large or very small numbers and lead to numerical problems. For instance, if during the fitting the solver tries \\(\\beta_i = 10\\) and time is 10, then the exponent in the second term becomes \\(e^{-10 exp(10)}\\), and that number is so small that R sets it to 0. Similarly, if the solver happens to explore \\(\\alpha_i = 10\\) at time 10, we would end up with \\(10^{exp(10)}\\) in the first term, which R can‚Äôt handle and sets to Infinity. (Try by typing exp(-10 * exp(10)) or 10^exp(10) into the R console). In both cases, the result will not make sense and can lead to the numerical routine either completely failing and aborting with an error, or at a minimum wasting computational time by having to ignore those values. (Stan is good at usually not completely breaking and instead ignoring such nonsensical results, but one can waste a lot of computational time.)\nNumerical issues like this one are not uncommon and something to always be aware of. To minimize such problems with very large/small numbers, one can often use algebraic (logarithm, exponent, etc.) rules and rewrite the equations. In our case, we can rewrite as\n\\[\n\\mu_{i,t}  =  \\exp(\\alpha_{i}) \\log (t_{i}) -\\exp(\\beta_{i}) t_{i}\n\\] Using \\(\\mu_{i,t}\\) in this form in the code seems to work fine, as you‚Äôll see. Note that this is exactly the same equation as the one above, just rewritten for numerical convenience. Nothing else has changed."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#modeling-the-main-model-parameters",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#modeling-the-main-model-parameters",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Modeling the main model parameters",
    "text": "Modeling the main model parameters\nOk, now let‚Äôs get back to building the rest of our model. So far, we specified an equation for the virus load trajectory \\(\\mu_{i,t}\\). We assume that every individual has their own virus-load trajectory, specified by parameters \\(\\alpha_i\\) and \\(\\beta_i\\). We need to define those. We allow each individual to have their own individual-level contribution to the parameters, and also assume there is a potential population-level effect of dose.\nThe latter assumption is in fact our main scientific question. We want to know if the dose someone receives has a systematic impact on the virus load trajectories. At the same time, we want to allow for variation between individuals. We could also consider a model that allows the impact of dose to be different for every individual. With enough data, that might seem feasible. But here, we assume we have limited data. (Of course, this is just simulated data, so it is as large as we want it to be. But for the real research project which motivates this tutorial, we only have data on 20 individuals.) We also really want to focus on the overall, population-level, effect of dose, and are less interested to see if there is variation of dose effect among individuals.\nIt is not clear how to best model the potential impact of inoculum dose. We really don‚Äôt have much biological/scientific intuition. Without such additional insight, a linear assumption is generally a reasonable choice. We thus model the main parameters \\(\\alpha_i\\) and \\(\\beta_i\\) as being linearly related to the (log of) the dose. This assumption relating the parameter to the log of the dose is mostly heuristic. But it does make some biological sense as often in systems like this, outcomes change in response to the logarithm of some input.\nIn addition to the dose component, every individual can have their unique contribution to \\(\\alpha_i\\) and \\(\\beta_i\\).\nWriting this in equation form gives\n\\[\n\\begin{aligned}\n\\alpha_{i} &  =  a_{0,i} + a_1 \\log (D_i) \\\\\n\\beta_{i}  & =  b_{0,i} + b_1 \\log (D_i)\n\\end{aligned}\n\\] Here, \\(a_{0,i}\\) and \\(b_{0,i}\\) are the individual-level contributions of each person to the main parameters, and \\(a_1\\) and \\(b_1\\) quantify how the dose each individual receives, \\(D_i\\), impacts the overall time-series trajectories. \\(a_1\\) and \\(b_1\\) do not vary between individuals, thus we are only estimating the overall/mean/population-level impact of dose, and won‚Äôt try to see if different individuals respond differently to dose. If, after fitting the data, we find that the distributions for \\(a_1\\) and \\(b_1\\) are mostly around zero, we could conclude that dose does not have an impact. In contrast, if the distributions for those parameters are away from zero, we conclude that dose seems to impact the time-series trajectories.\nNote that if we were to fit this model in a frequentist framework, we would overfit (trying to estimate too many parameters). That is because if every individual has their own \\(a_{0,i}\\) and \\(b_{0,i}\\), the model can take any shape without needing the dose-related parameters to play a role. Thus we would have non-identifiability of parameters. As you‚Äôll see in the next post of this series, this feature of potential overfitting/non-identifiability can also be seen in the Bayesian approach, but we are still able to obtain reasonable fits and parameter estimates. We‚Äôll discuss that topic in more detail in the next post."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#some-rescaling",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#some-rescaling",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Some rescaling",
    "text": "Some rescaling\nAlright, time for another brief detour.\nThe model we have is ok. But as it is written right now, the parameters \\(a_{i,0}\\) and \\(b_{i,0}\\) describe the virus-load trajectory for an individual with a dose of 1 (log(1)=0). In our made-up example, individuals receive doses of strength 10/100/1000 but not 1. If we didn‚Äôt work with dose on a log scale, the \\(a_{i,0}\\) and \\(b_{i,0}\\) parameters would represent trajectories for individuals who received a dose of 0. That doesn‚Äôt make sense, since anyone not being challenged with virus will not have any virus trajectory.\nIt doesn‚Äôt mean the model is wrong, one can still fit it and get reasonable estimates. But interpretation of parameters, and thus choices for priors, might get trickier. In such cases, some transformation of the data/model can be useful.\nA common approach is to adjust predictor variables by standardizing (subtracting the mean and dividing by the standard deviation). Here we do something slightly different. We subtract the log of the middle dose. We call that dose \\(D_m\\). In our example it takes on the value of 100. In general, you can do any transformation that you think makes the setup and problem easier to interpret.\nThe equations then become\n\\[\n\\begin{aligned}\n\\alpha_{i} &  =  a_{0,i} + a_1 \\left(\\log (D_i) - \\log (D_m)\\right)  \\\\\n\\beta_{i}  & =  b_{0,i} + b_1 \\left(\\log (D_i) - \\log (D_m)\\right)\n\\end{aligned}\n\\] Now the intercept parameters \\(a_{i,0}\\) and \\(b_{i,0}\\) describe the main model parameters \\(\\alpha_i\\) and \\(\\beta_i\\), and thus the trajectory for the virus, if the dose is at the intermediate level. Thus, these parameters are now easy to interpret."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#quick-summary",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#quick-summary",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Quick summary",
    "text": "Quick summary\nAt this stage in the model building process, our model as the following parts\n\\[\n\\begin{aligned}\n\\textrm{Outcome} \\\\\nY_{i,t}   \\sim \\mathrm{Normal}\\left(\\mu_{i,t}, \\sigma\\right) \\\\\n\\\\\n\\textrm{Deterministic time-series trajectory} \\\\\n\\mu_{i,t}   =  \\exp(\\alpha_{i}) \\log (t_{i}) -\\exp(\\beta_{i}) t_{i} \\\\\n\\\\\n\\textrm{Deterministic models for main parameters} \\\\\n\\alpha_{i}   =  a_{0,i} + a_1 \\left(\\log (D_i) - \\log (D_m)\\right)  \\\\\n\\beta_{i}   =  b_{0,i} + b_1 \\left(\\log (D_i) - \\log (D_m)\\right) \\\\\n\\end{aligned}\n\\]\nThe model parameters are \\(\\sigma\\), \\(a_1\\), \\(b_1\\), \\(a_{i,0}\\) and \\(b_{i,0}\\). The latter two consist of as many parameters as there are individuals in the data. So if we have \\(N\\) individuals, we have a total of \\(3+2N\\) parameters.\nAt this point, we could fit the model in either a Bayesian or frequentist framework. For either approach, we need to determine what (if any) additional structure we want to impose on the model parameters.\nOne approach is to not impose any further structure. We could make the assumption that every individual is completely different from each other, such that their outcomes do not inform each other. That would mean we allow values of \\(a_{i,0}\\) and \\(b_{i,0}\\) to be different for each individual, and let them be completely independent from each other. Such a model is (in McElreath‚Äôs terminology) a no pooling model. Such a model is expected to fit the data for each individual well. But it would lead to overfitting, trying to estimate too many parameters given the data. That means the estimates for the parameters will be uncertain, and thus our question of interest, the possible impact of dose, will be hard to answer. Also, it won‚Äôt be very good at predicting future individuals.\nOn the other extreme, we could instead assume that all individuals share the same parameter values, i.e.¬†\\(a_{i,0} = a_0\\) and \\(b_{i,0} = b_0\\). This is called a full pooling model. You see this model often in data analyses, when investigators take the mean of some measurements and then just model the means. For our example, we would be modeling the mean virus load time-series trajectory for all individuals in a given dose group. This type of model can extract the population level (in our case dose) effect, but by ignoring the variation among individuals for the same dose, the model is likely overly confident in its estimates, and it leads to underfitting of the data. By not allowing differences between individuals, the model is likely too restrictive and thus is not that great at capturing the patterns seen in the data. We‚Äôll explore that when we fit the models.\nAn intermediate model - and usually the best approach - is one that neither allows the \\(a_{i,0}\\) and \\(b_{i,0}\\) to be completely independent or forces them to be exactly the same. Instead, it imposes some correlation between the parameters. This leads to the mixed/hierarchical/multilevel modeling approach. Such an approach can be implemented in both a frequentist or Bayesian framework. Here, we focus on the Bayesian approach, which I personally find more intuitive since everything is explicit and spelled out."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#specifying-priors",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#specifying-priors",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Specifying priors",
    "text": "Specifying priors\nSince we are working in a Bayesian framework, our parameters need priors. We assume that for all models we discuss below, the parameters \\(a_{0,i}\\), \\(b_{0,i}\\), \\(a_1\\) and \\(b_1\\) have priors described by Normal distributions. The standard deviation \\(\\sigma\\) is modeled by a Half-Cauchy distribution (a Cauchy distribution that‚Äôs only defined for positive values, since standard deviations need to be positive). Those choices are a mix of convention, numerical usefulness and first principles. See for instance the Statistical Rethinking book for more details. You can likely choose other prior distributions and results might not change much. If they do, it means you don‚Äôt have a lot of data to inform your results and need to be careful about drawing conclusions.\nThe equations for our priors are\n\\[\n\\begin{aligned}\n\\sigma  \\sim \\mathrm{HalfCauchy}(0, 1)  \\\\\na_1  \\sim \\mathrm{Normal}(0.1, 0.1) \\\\\nb_1  \\sim \\mathrm{Normal}(-0.1, 0.1) \\\\\na_{0,i}  \\sim \\mathrm{Normal}(\\mu_a, \\sigma_a) \\\\\nb_{0,i} \\sim \\mathrm{Normal}(\\mu_b, \\sigma_a) \\\\\n\\end{aligned}\n\\] I gave the prior distributions for \\(\\sigma\\), \\(a_1\\) and \\(b_1\\) fixed values. I chose those values to get reasonable simulation results (as you‚Äôll see below). We will use those same values for all models. The priors for \\(a_{i,0}\\) and \\(b_{i,0}\\) are more interesting. They depend on parameters themselves. In the next sections, we will explore different choices for those parameters \\(\\mu_a\\), \\(\\mu_b\\), \\(\\sigma_a\\) and \\(\\sigma_b\\), based on the different modeling approaches described in the previous section."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#model-1",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#model-1",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Model 1",
    "text": "Model 1\nOur first model is one that replicates the no pooling approach. In a Bayesian framework, such a no-pooling model can be implemented by making the priors for \\(a_{i,0}\\) and \\(b_{i,0}\\) very wide, which essentially assumes that any values are allowed, and there is (almost) no shared commonality/information among the parameters for each individual.\nIn our example, we can accomplish this by ensuring \\(\\sigma_a\\) and \\(\\sigma_b\\) are large, such that the normal distributions for \\(a_{i,0}\\) and \\(b_{i,0}\\) become very wide. In that case, the values for the mean, \\(\\mu_a\\) and \\(\\mu_b\\) don‚Äôt matter much since we allow the model to take on any values, even those far away from the mean. Therefore, we can just set \\(\\mu_a\\) and \\(\\mu_b\\) to some reasonable values, without paying too much attention.\nThis choice for the priors leads to a Bayesian model similar to a frequentist no-pooling model."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#model-2",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#model-2",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Model 2",
    "text": "Model 2\nNow we‚Äôll try to reproduce the full pooling model in a Bayesian framework. We could remove the individual-level variation by setting \\(a_{i,0} = a_0\\) and \\(b_{i,0} = b_0\\) (and we‚Äôll do that below). But if we want to keep the structure we have above, what we need to do is to ensure the priors for those parameters are very narrow, such that every individual is forced to have more or less the same value. We can accomplish that by setting values for \\(\\sigma_a\\) and \\(\\sigma_b\\) very close to zero.\nIf we set the \\(\\mu_a\\) and \\(\\mu_b\\) parameters to some fixed values, we would enforce \\(a_{i,0}\\) and \\(b_{i,0}\\) to take specific values too. We don‚Äôt want that, we want them to be estimated, we just want to make sure all individuals get pretty much the same estimate. To do so, we can give \\(\\mu_a\\) and \\(\\mu_b\\) their own distributions and make those wide/flat. A normal distribution for each parameter with a wide variance should work.\nWith these choices, we have a model that can find the mean for \\(a_{i,0}\\) and \\(b_{i,0}\\), but the spread in those parameters is minimal."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#model-3",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#model-3",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Model 3",
    "text": "Model 3\nOk, so we discussed that the no pooling model 1 is too flexible and thus likely overfits, the full pooling model 2 is too rigid and likely underfits. Why not build a model that has reasonable priors in between those two? That‚Äôs a good idea and it leads us to a partial pooling model.\nWe want priors for \\(a_{i,0}\\) and \\(b_{i,0}\\) that are not too flat/wide (model 1) or too narrow (model 2). They should allow some variation, but still ensure that there is shared information among the parameters. With that, we might be able to find a happy medium between underfitting and overfitting. Such priors are known as regularizing priors. They allow some variability for the parameters among individuals, while implementing the notion that the individuals share some commonality, and therefore their parameters should also share some common features, as indicated by belonging to the same prior distributions.\nThe question is, how to set the priors? One option is to pick some values for \\(\\mu_a\\), \\(\\mu_b\\), \\(\\sigma_a\\) and \\(\\sigma_b\\) (either by specifying their distributions or by directly setting values), then do prior predictive simulations, see if results look reasonable (no crazy outcomes, but still a good bit of variability) and then iterate until one found good priors. One can also explore the impact of the priors on the posterior. If they have a strong impact, it suggests there is not enough data to fully determine the posterior.\nThis approach of trial and error is reasonable, and we‚Äôll use it here for our model 3. But it also feels a bit like ad-hoc. One might want to ask the question if there is another way to pick the priors. The answer is yes, which brings us to our next model."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#model-4",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#model-4",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Model 4",
    "text": "Model 4\nInstead of trying to pick values for the priors manually (again, nothing wrong with that, but maybe not always optimal), one can let the data determine the priors. That approach involves estimating each of the parameters that specify the priors for \\(a_{i,0}\\) and \\(b_{i,0}\\).\nThe parameters \\(\\mu_a\\), \\(\\mu_b\\), \\(\\sigma_a\\) and \\(\\sigma_b\\) now get their own distributions, with their own priors (often called hyper-priors). The values for the hyper-priors are picked such that resulting simulations from the model produce (somewhat) reasonable trajectories, as you‚Äôll see below. In principle, one can further specify them as functions of other priors (turtles, I mean priors, all the way down!). But in most cases, including our example, not much is gained from that.\nWhat now happens is that as we fit the model, our priors for \\(a_{i,0}\\) and \\(b_{i,0}\\) share some information, and the amount of sharing is controlled by the hyper-prior parameters, which are determined by the data fitting process itself. It sounds a bit like magic, and I admit that on some deep level, I still don‚Äôt fully understand this magic, even though I can follow the steps. Maybe at some point in the future I will fully get it. For now I‚Äôm content with the level of understanding I have, and knowing that it works üòÑ.\nThis model is a partial pooling model like model 3, but now the pooling is determined adaptively by the data. This leads to a happy intermediate between the too-rigid full-pooling model and the too-flexible no-pooling model. This kind of model is often the best choice."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#recap",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#recap",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Recap",
    "text": "Recap\nOk, those were a lot of steps, so to have it all in one place, here are the models again, now shown with equations and with all components in one place.\nAll models have these parts:\n\\[\n\\begin{aligned}\n\\textrm{Outcome} \\\\\nY_{i,t}   \\sim \\mathrm{Normal}\\left(\\mu_{i,t}, \\sigma\\right) \\\\\n\\\\\n\\textrm{Deterministic time-series trajectory} \\\\\n\\mu_{i,t}   =  \\exp(\\alpha_{i}) \\log (t_{i}) -\\exp(\\beta_{i}) t_{i} \\\\\n\\\\\n\\textrm{Deterministic models for main parameters} \\\\\n\\alpha_{i}   =  a_{0,i} + a_1 \\left(\\log (D_i) - \\log (D_m)\\right)  \\\\\n\\beta_{i}   =  b_{0,i} + b_1 \\left(\\log (D_i) - \\log (D_m)\\right) \\\\\n\\\\\n\\textrm{population-level priors} \\\\\n\\sigma  \\sim \\mathrm{HalfCauchy}(0,1)  \\\\\na_1 \\sim \\mathrm{Normal}(0.1, 0.1) \\\\\nb_1 \\sim \\mathrm{Normal}(-0.1, 0.1) \\\\\na_{0,i} \\sim \\mathrm{Normal}(\\mu_a, \\sigma_a) \\\\\nb_{0,i}  \\sim \\mathrm{Normal}(\\mu_b, \\sigma_a) \\\\\n\\end{aligned}\n\\]\nFor model 1, we set the parameters describing the distribution for \\(a_{0,i}\\) and \\(b_{0,i}\\) to produce un-informative/flat priors. For our example, these values work:\n\\[\n\\begin{aligned}\n\\mu_a & = 3  \\\\\n\\mu_b & = 1  \\\\\n\\sigma_a & = 10  \\\\\n\\sigma_b & = 10   \n\\end{aligned}\n\\]\nFor model 2, we set the standard deviations to a very small value and give the mean parameters somewhat flexible distributions. These work:\n\\[\n\\begin{aligned}\n\\mu_a & \\sim \\mathrm{Normal}(3, 1) \\\\\n\\mu_b & \\sim \\mathrm{Normal}(1, 1) \\\\\n\\sigma_a & = 0.001  \\\\\n\\sigma_b & = 0.001  \n\\end{aligned}\n\\]\nAs mentioned, an alternative for model 2, which I‚Äôll call model 2a, is to reduce the parameters from 2N to 2 and specify these priors for what are now population-level only parameters, like this:\n\\[\n\\begin{aligned}\na_{0} &  \\sim \\mathrm{Normal}(3, 1) \\\\\nb_{0} & \\sim \\mathrm{Normal}(1, 1)\n\\end{aligned}\n\\]\nFor model 3, we set values that lead to priors that are reasonably intermediate between the model 1 too flat and model 2 too narrow priors. These work:\n\\[\n\\begin{aligned}\n\\mu_a & = 3  \\\\\n\\mu_b & = 1  \\\\\n\\sigma_a & = 1  \\\\\n\\sigma_b & = 1   \n\\end{aligned}\n\\]\nFinally, model 4 has distributions for all 4 parameters. These work for our example\n\\[\n\\begin{aligned}\n\\mu_a & \\sim \\mathrm{Normal}(3, 1) \\\\\n\\mu_b & \\sim \\mathrm{Normal}(1, 1) \\\\\n\\sigma_a & \\sim \\mathrm{HalfCauchy}(0,1)  \\\\\n\\sigma_b & \\sim \\mathrm{HalfCauchy}(0,1)  \n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#comment-on-terminology",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#comment-on-terminology",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Comment on terminology",
    "text": "Comment on terminology\nI have been talking about 4 different models (or 5 if you count model 2a). As I‚Äôm sure you realized, some models are structurally the same, just with different choices for the priors. In a Bayesian framework, the priors (which includes choices for both the distribution and values) are part of the model, thus in that sense, model 1 and 3 can be considered different models, even if we only change the values for the variance priors. For the purpose of this tutorial I‚Äôll take that perspective and consider them separate models. It also makes it easier to talk about them by giving each their own label/number."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#package-loading",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#package-loading",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Package loading",
    "text": "Package loading\nLoad required packages.\n\nlibrary(ggplot2)\nlibrary(readr)"
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#general-settings",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#general-settings",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "General settings",
    "text": "General settings\n\n## General settings\nset.seed(123) #for reproducibility\n# days at which we assume outcome is measured\ntimevec &lt;- c(0.1,1,3,5,7,10,14,21,28,35,42)\n\n#different number of individuals per dose to make it clearer which is which\n#also, that's the structure of the data which motivated the tutorial\nNlow = 7; Nmed = 8; Nhigh = 9; \nfilename = \"simdat.Rds\"\nfilenamecsv = \"simdat.csv\"\n#if you want to explore how model fitting changes if you increase sample size\n#turn on this line of code\n#this is used in part 4 of the tutorial\n#Nlow = 70; Nmed = 80; Nhigh = 90; filename = \"simdat_big.Rds\"\n\nNind = Nlow + Nmed + Nhigh; #total number of individuals\n\n# Set values for dose\n# since we only consider dose on a log scale\n# we'll log transform right here and then always use it in those log units\nhigh_dose = log(1000)\nmed_dose = log(100)\nlow_dose = log(10)\ndosevec = c(rep(low_dose,Nlow),rep(med_dose,Nmed),rep(high_dose,Nhigh))\n# we are also creating a version of the dose variable\n# that consists of ordered categories instead of numeric values\n# we'll use that mostly for plotting\ndosevec_cat = ordered(c(rep(\"low\", Nlow),\n                        rep(\"medium\",Nmed),\n                        rep(\"high\",Nhigh)),\n                      levels=c(\"low\",\"medium\",\"high\"))\n\n\n## Setting parameter values\n\nI chose fairly low sample sizes, with less than 10 individuals for each dose group. This is motivated by the real data I have in mind, which has similar sample sizes. Of course, more data is generally better. In part 4 of the tutorial I play around a bit with fitting larger samples."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#setting-parameter-values",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#setting-parameter-values",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Setting parameter values",
    "text": "Setting parameter values\nThe parameters \\(\\sigma\\), \\(a_1\\) and \\(b_1\\) show up in all models. For easy comparison between models, I‚Äôm setting them to the same value for all models.\nFor the estimation procedure (see part 2), we assume that those parameters follow the distributions shown above. We could sample a single value for each of them from such a distribution. But to reduce variability and to more easily compare estimated parameters to those used to simulate the data, I‚Äôm setting them to specific values, which you can conceptually think of as being a single sample from the distributions we discussed above. It makes sense to chose their means as the value to use.\n\nsigma = 1\na1 = 0.1\nb1 = -0.1\n\nNow well get values for the other parameters. For model 1, we have \\(N\\) parameters for \\(a_{i,0}\\) and \\(b_{i,0}\\), with priors that are very wide. We set them as follows\n\nm1_mua = 3\nm1_mub = 1\nm1_sigmaa = 1\nm1_sigmab = 1\nm1_a0 = rnorm(n=Nind, m1_mua, m1_sigmaa)\nm1_b0 = rnorm(n=Nind, m1_mub, m1_sigmab)\n# saving main parameters\nm1pars = c(sigma = sigma, a1 = a1, b1 = b1,\n           a0_mu = m1_mua, b0_mu = m1_mub)\n\nNote a few things here. First, the priors are narrower than I specified above. As you will see in the figures below, even with these less wide priors, results for model 1 still look way too variable. We can use the wider priors when we fit the model, to allow the data to dominate the fits. But for data generation, going too wild/wide seems pointless.\nSecond, you noticed that I did sample from distributions for the \\(a_{i,0}\\) and \\(b_{i,0}\\) parameters. That‚Äôs not necessary, I could have also specified values for each of the parameters, like I did for \\(\\sigma\\), \\(a_1\\) and \\(b_1\\), as long as the values can be thought of as coming from the underlying distribution. If I sample, I need to make sure to set a random seed (which I did above) to ensure reproducibility.\nLastly, I‚Äôm saving the parameters in a vector which will be added to the generated data so we can keep track of the parameters that were used to generate the data, and compare later with the estimates from the models.\nOk, now for model 2. We have 2 versions, model 2a collapses the individual-level parameters into a single population one. We‚Äôll explore that model when doing the fitting, for simulating the data I‚Äôm just going with model 2.\n\nm2_mua = 3\nm2_mub = 1\nm2_sigmaa = 0.0001\nm2_sigmab = 0.0001\nm2_a0 = rnorm(n=Nind, m2_mua, m2_sigmaa)\nm2_b0 = rnorm(n=Nind, m2_mub, m2_sigmab)\nm2pars = c(sigma = sigma, a1 = a1, b1 = b1,\n           a0_mu = m2_mua, b0_mu = m2_mub)\n\nFinally, model 3 with priors that have a width between those of model 1 and model 2.\n\nm3_mua = 3\nm3_mub = 1\nm3_sigmaa = 0.1\nm3_sigmab = 0.1\nm3_a0 = rnorm(n=Nind, m3_mua, m3_sigmaa)\nm3_b0 = rnorm(n=Nind, m3_mub, m3_sigmaa)\nm3pars = c(sigma = sigma, a1 = a1, b1 = b1,\n           a0_mu = m3_mua, b0_mu = m3_mub)\n\nNote that for the purpose of simulating data, model 4 is basically the same as model 3. We would need to sample (or pick) values for the parameters \\(\\mu_a\\), \\(\\sigma_a\\), \\(\\mu_b\\), and \\(\\sigma_b\\) and then use them to sample (or set) values for \\(a_{i,0}\\) and \\(b_{i,0}\\). This setup makes sense during fitting, but for generating data, it isn‚Äôt really different than what we already did. You can conceptually assume we did sample parameters and happen to get the values shown for model 3. Thus, model 4 collapses to the models we already specified.\nOverall, when generating data, we can go through all steps of sampling from each specified distribution to get values. Nothing wrong with that. But if we change the random seed, values change. And it is harder to compare the parameters used to generate the data with those that are estimated. Thus, it is generally easier during the data generation process to assume conceptually that values correspond to samples from distributions, but then set the values manually. Above, we used that approach for most parameters. We did sample the \\(a_{0,i}\\) and \\(b_{0,i}\\) to show explicitly the sampling steps involved in generating simulated data from our Bayesian models."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#creating-simulated-data",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#creating-simulated-data",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Creating simulated data",
    "text": "Creating simulated data\nNow we can combine the parameters as specified in the equations above to get simulated trajectories for each individual, for each of the models. We just need to add the parameters together in the way prescribed by the model to get to the outcome. This is a nice feature of Bayesian models, that you can run them both ‚Äúforward‚Äù to generate data given parameter values, and ‚Äúbackward‚Äù to estimate parameter values, given the data. Because of that feature, Bayesian models are generative models.\nHere is the code that computes the mean and the outcome for model 1, the wide model.\n\nm1_alpha = m1_a0 + a1*(dosevec - med_dose)\nm1_beta = m1_b0 + b1*(dosevec - med_dose)\n#doing matrix multiplication to get time-series for each individual\n#for that to work, the timevec vector needs to be transposed\nm1_mu =  exp(m1_alpha) %*% t(log(timevec)) - exp(m1_beta) %*% t(timevec)\n# apply variation following a normal distribution to each data point\nm1_y = rnorm(length(m1_mu),m1_mu, sigma)\n# in a final step, we reorganize the data into a long data frame with\n# columns id, time, dose, model,\n# the deterministic mean mu, and the normally distributed outcome.\n# We store dose in 3 versions, the original (log transformed one),\n# the one that has the middle value subtracted, and a categorical one.\n# Note that trick using sort to get time in the right order.\n# Not a robust way of doing things, but works here\nm1_dat &lt;- data.frame(id = rep(1:Nind,length(timevec)),\n                     dose = rep(dosevec,length(timevec)),\n                     dose_adj = rep(dosevec,length(timevec))-med_dose,\n                     dose_cat =  rep(dosevec_cat,length(timevec)),\n                     time = sort(rep(timevec,Nind)),\n                     mu = as.vector(m1_mu),\n                     outcome = as.vector(m1_y),\n                     model = \"m1\")\n\nNow we just repeat the same code again for the other models.\n\n#model 2\nm2_alpha = m2_a0 + a1*(dosevec - med_dose)\nm2_beta = m2_b0 + b1*(dosevec - med_dose)\nm2_mu =  exp(m2_alpha) %*% t(log(timevec)) - exp(m2_beta) %*% t(timevec)\nm2_y = rnorm(length(m2_mu),m2_mu, sigma)\nm2_dat &lt;- data.frame(id = rep(1:Nind,length(timevec)),\n                     dose = rep(dosevec,length(timevec)),\n                     dose_adj = rep(dosevec,length(timevec))-med_dose,\n                     dose_cat =  rep(dosevec_cat,length(timevec)),\n                     time = sort(rep(timevec,Nind)),\n                     mu = as.vector(m2_mu),\n                     outcome = as.vector(m2_y),\n                     model = \"m2\")\n\n#model 3\nm3_alpha = m3_a0 + a1*(dosevec - med_dose)\nm3_beta = m3_b0 + b1*(dosevec - med_dose)\nm3_mu =  exp(m3_alpha) %*% t(log(timevec)) - exp(m3_beta) %*% t(timevec)\nm3_y = rnorm(length(m3_mu),m3_mu, sigma)\nm3_dat &lt;- data.frame(id = rep(1:Nind,length(timevec)),\n                     dose = rep(dosevec,length(timevec)),\n                     dose_adj = rep(dosevec,length(timevec))-med_dose,\n                     dose_cat =  rep(dosevec_cat,length(timevec)),\n                     time = sort(rep(timevec,Nind)),\n                     mu = as.vector(m3_mu),\n                     outcome = as.vector(m3_y),\n                     model = \"m3\")"
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#plotting-the-simulated-data",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#plotting-the-simulated-data",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Plotting the simulated data",
    "text": "Plotting the simulated data\nTo ensure our simulated data makes sense, let‚Äôs plot what we produced. These lines of code create plots for each model/simulated dataset.\n\np1 &lt;- ggplot(m1_dat) +\n  geom_line(aes(x=time, y=mu, col = dose_cat, group = id)) +\n  geom_point(aes(x=time, y=outcome, col = dose_cat)) +\n  scale_y_continuous(limits = c(-30,200)) +\n  labs(y = \"Outcome (log virus load)\",  x = \"Time (days)\") +\n  theme_minimal()\n\n\np2 &lt;- ggplot(m2_dat) +\n  geom_line(aes(x=time, y=mu, col = dose_cat, group = id)) +\n  geom_point(aes(x=time, y=outcome, col = dose_cat)) +\n  scale_y_continuous(limits = c(-30,50)) +\n  labs(y = \"Outcome (log virus load)\",  x = \"Time (days)\") +\n  theme_minimal()\n\np3 &lt;- ggplot(m3_dat) +\n  geom_line(aes(x=time, y=mu, col = dose_cat, group = id)) +\n  geom_point(aes(x=time, y=outcome, col = dose_cat)) +\n  scale_y_continuous(limits = c(-30,50)) +\n  labs(y = \"Outcome (log virus load)\",  x = \"Time (days)\") +\n  theme_minimal()\n\nNow, let‚Äôs plot the simulated data. For each plot, the lines show the deterministic mean trajectory, and the symbols show the outcomes, which have some extra variation on top, determined by the value of \\(\\sigma\\).\n\nplot(p1)\n\n\n\n\n\n\n\nplot(p2)\n\n\n\n\n\n\n\nplot(p3)\n\n\n\n\n\n\n\n#save a plot so we can use it in the blog post\n#ggsave(file = paste0(\"featured.png\"), p3, dpi = 300, units = \"in\", width = 6, height = 6)\n\nAs you can see, the priors for model 1 are so wide that some of the resulting trajectories are not reasonable. The variation between individuals is so strong that the dose effect - which we programmed into the simulated data to exist - is swamped out. That could certainly be true for real data, sometimes there is just too much noise/variability to detect a pattern, even if it exists. But some of the trajectories produce virus load that‚Äôs just biologically unreasonable (note how high the y-values go.)\nOn the other extreme, the priors for model 2 allow so little variation that the individual-level variation is minimal and the only effect that is noticable is the dose dependence we assumed in our model (by setting \\(a_1\\) and \\(b_1\\) to non-zero values).\nModel 3 produces the most reasonable trajectories, with both the dose-effect showing, and some variation between individuals."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#saving-the-simulated-data",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#saving-the-simulated-data",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Saving the simulated data",
    "text": "Saving the simulated data\nFinally, let‚Äôs combine all the simulated data into a single list containing all data frames, and save it. I‚Äôm also saving the parameters for each model, and sample sizes, so we can quickly retrieve them when we compare with the model estimates.\nWe‚Äôll also save one of the plots (this is mainly so I can show it at the beginning of the tutorial).\n\n# write all simulations to an RDS file\nsimdat &lt;- list(m1 = m1_dat, m2 = m2_dat, m3 = m3_dat, m1pars = m1pars, m2pars = m2pars, m3pars = m3pars, Nlow = Nlow, Nmed = Nmed, Nhigh = Nhigh)\nsaveRDS(simdat, file = filename)\n# also write one to a CSV file so I can use it with Julia\nreadr::write_csv(m3_dat, file = filenamecsv)\n\nWe‚Äôll load and use the simdat file in the next parts of the tutorial."
  },
  {
    "objectID": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html",
    "href": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 3",
    "section": "",
    "text": "This is part 3 of a tutorial illustrating how one can use the brms and rethinking R packages to perform a Bayesian analysis of longitudinal data using a multilevel/hierarchical/mixed-effects setup.\nI assume you‚Äôve read both part 1, and part 2 otherwise this post won‚Äôt make much sense."
  },
  {
    "objectID": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#model-1",
    "href": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#model-1",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 3",
    "section": "Model 1",
    "text": "Model 1\nThis is one of the models with individual-level and dose-level effects, all priors fixed. This model has \\(2N+2+1\\) parameters. \\(N\\) each for the individual-level intercepts for \\(\\alpha\\) and \\(\\beta\\) (the \\(a_{0,i}\\) and \\(b_{0,i}\\) parameters), the two dose-level parameters \\(a_1\\) and \\(b_1\\), and 1 overall deviation, \\(\\sigma\\) for the outcome distribution.\n\n#no-pooling model\n#separate intercept for each individual/id\n#2x(N+1)+1 parameters\nm1eqs &lt;- bf(  #main equation for time-series trajectory\n          outcome ~  exp(alpha)*log(time) - exp(beta)*time,\n          #equations for alpha and beta\n          alpha ~ 0 + id + dose_adj,\n          beta  ~ 0 + id + dose_adj,\n          nl = TRUE)\n\nm1priors &lt;- c(#assign priors to all coefficients related to both id and dose_adj for alpha and beta\n              prior(normal(2, 10),  class = \"b\",  nlpar = \"alpha\"),\n              prior(normal(0.5, 10),  class = \"b\",  nlpar = \"beta\"),\n              #change the dose_adj priors to something different than the id priors\n              prior(normal(0.3, 1),   class = \"b\",  nlpar = \"alpha\", coef = \"dose_adj\"),\n              prior(normal(-0.3, 1),  class = \"b\",  nlpar = \"beta\", coef = \"dose_adj\"),\n              prior(cauchy(0,1), class = \"sigma\") )\n\nNotice how this notation in brms looks quite a bit different from the mathematical equations or the ulam implementation. That‚Äôs a part I don‚Äôt particularly like about brms, the very condensed formula notation. It takes time getting used to and it always requires extra checking to ensure the model implemented in code corresponds to the mathematical model. One can check by looking at the priors and make sure they look as expected. We‚Äôll do that below after we fit."
  },
  {
    "objectID": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#model-2a",
    "href": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#model-2a",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 3",
    "section": "Model 2a",
    "text": "Model 2a\nThis is the easiest model, with only population level effects for intercept and dose, so only 2+2+1 parameters.\n\n#full-pooling model\n#2+2+1 parameters\nm2aeqs &lt;- bf(  #main equation for time-series trajectory\n  outcome ~ exp(alpha)*log(time) - exp(beta)*time,\n  #equations for alpha and beta\n  alpha ~ 1 + dose_adj,\n  beta  ~  1 + dose_adj,\n  nl = TRUE)\n\nm2apriors &lt;- c(prior(normal(2, 2),  class = \"b\",  nlpar = \"alpha\", coef = \"Intercept\"),\n              prior(normal(0.5, 2),  class = \"b\",  nlpar = \"beta\", coef = \"Intercept\"),\n              prior(normal(0.3, 1),   class = \"b\",  nlpar = \"alpha\", coef = \"dose_adj\"),\n              prior(normal(-0.3, 1),  class = \"b\",  nlpar = \"beta\", coef = \"dose_adj\"),\n              prior(cauchy(0,1), class = \"sigma\")  )"
  },
  {
    "objectID": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#model-3",
    "href": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#model-3",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 3",
    "section": "Model 3",
    "text": "Model 3\nThis is the same as model 1 but with different values for the priors.\n\n#same as model 1 but regularizing priors\nm3eqs &lt;- m1eqs\n\nm3priors &lt;- c(#assign priors to all coefficients related to id and dose_adj for alpha and beta\n  prior(normal(2, 1),  class = \"b\",  nlpar = \"alpha\"),\n  prior(normal(0.5, 1),  class = \"b\",  nlpar = \"beta\"),\n  #change the dose_adj priors to something different than the id priors\n  prior(normal(0.3, 1),   class = \"b\",  nlpar = \"alpha\", coef = \"dose_adj\"),\n  prior(normal(-0.3, 1),  class = \"b\",  nlpar = \"beta\", coef = \"dose_adj\"),\n  prior(cauchy(0,1), class = \"sigma\") )"
  },
  {
    "objectID": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#model-4",
    "href": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#model-4",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 3",
    "section": "Model 4",
    "text": "Model 4\nThis is the adaptive-pooling multi-level model where priors are estimated. Here we have for each main parameter (\\(\\alpha\\) and \\(\\beta\\)) an overall mean and standard deviation, and N individual intercepts, so 2 times 1+1+N. And of course we still have the 2 dose-related parameters and the overall standard deviation, so a total of 2*(1+1+N)+2+1 parameters.\n\n#adaptive prior, partial-pooling model\nm4eqs &lt;- bf(  #main equation for time-series trajectory\n  outcome ~ exp(alpha)*log(time) - exp(beta)*time,\n  #equations for alpha and beta\n  alpha ~  (1|id) + dose_adj,\n  beta  ~  (1|id) + dose_adj,\n  nl = TRUE)\n\nm4priors &lt;- c(prior(normal(2, 1),  class = \"b\",  nlpar = \"alpha\", coef = \"Intercept\"),\n              prior(normal(0.5, 1),  class = \"b\",  nlpar = \"beta\", coef = \"Intercept\"),\n              prior(normal(0.3, 1),   class = \"b\",  nlpar = \"alpha\", coef = \"dose_adj\"),\n              prior(normal(-0.3, 1),  class = \"b\",  nlpar = \"beta\", coef = \"dose_adj\"),\n              prior(cauchy(0,1), class = \"sd\", nlpar = \"alpha\"),\n              prior(cauchy(0,1), class = \"sd\", nlpar = \"beta\"),\n              prior(cauchy(0,1), class = \"sigma\")  )"
  },
  {
    "objectID": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#combine-models",
    "href": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#combine-models",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 3",
    "section": "Combine models",
    "text": "Combine models\nTo make our lives easier below, we combine all models and priors into lists.\n\n#stick all models into a list\nmodellist = list(m1=m1eqs,m2a=m2aeqs,m3=m3eqs,m4=m4eqs)\n#also make list for priors\npriorlist = list(m1priors=m1priors,m2apriors=m2apriors,m3priors=m3priors,m4priors=m4priors)\n# set up a list in which we'll store our results\nfl = vector(mode = \"list\", length = length(modellist))"
  },
  {
    "objectID": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#fitting-setup",
    "href": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#fitting-setup",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 3",
    "section": "Fitting setup",
    "text": "Fitting setup\nWe define some general values for the fitting. Since the starting values depend on number of chains, we need to do this setup first.\n\n#general settings for fitting\n#you might want to adjust based on your computer\nwarmup = 6000\niter = warmup + floor(warmup/2)\nmax_td = 18 #tree depth\nadapt_delta = 0.9999\nchains = 5\ncores  = chains\nseed = 1234"
  },
  {
    "objectID": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#setting-starting-values",
    "href": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#setting-starting-values",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 3",
    "section": "Setting starting values",
    "text": "Setting starting values\nWe‚Äôll again set starting values, as we did for ulam/rethinking. Note that brms needs them in a somewhat different form, namely as list of lists for each model, one list for each chain.\nI set different values for each chain, so I can check that each chain ends up at the same posterior. This is inspired by this post by Solomon Kurz, though I keep it simpler and just use the jitter function.\nNote that this approach not only jitters (adds noise/variation) between chains, but also between the individual-level parameters for each chain. That‚Äôs fine for our purpose, it might even be beneficial.\n\n## Setting starting values\n#starting values for model 1\nstartm1 = list(a0 = rep(2,Ntot), b0 = rep(0.5,Ntot), a1 = 0.5 , b1 = -0.5, sigma = 1)\n#starting values for model 2a\nstartm2a = list(a0 = 2, b0 = 0.5, a1 = 0.5 , b1 = 0.5, sigma = 1)\n#starting values for model 3\nstartm3 = startm1\n#starting values for models 4\nstartm4 = list(mu_a = 2, sigma_a = 1, mu_b = 0, sigma_b = 1, a1 = 0.5 , b1 = -0.5, sigma = 1)\n#put different starting values in list\n#need to be in same order as models below\n#one list for each chain, thus a 3-leveled list structure\n#for each chain, we add jitter so they start at different values\nstartlist = list( rep(list(lapply(startm1,jitter,10)),chains),\n                  rep(list(lapply(startm2a,jitter,10)),chains),\n                  rep(list(lapply(startm3,jitter,10)),chains),\n                  rep(list(lapply(startm4,jitter,10)),chains)\n                  )"
  },
  {
    "objectID": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#model-fitting",
    "href": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#model-fitting",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 3",
    "section": "Model fitting",
    "text": "Model fitting\nWe‚Äôll use the same strategy to loop though all models and fit them. The fitting code looks very similar to the previous one for rethinking/ulam, only now the fitting is done calling the brm function.\n\n# fitting models\n#loop over all models and fit them using ulam\nfor (n in 1:length(modellist))\n{\n\n  cat('************** \\n')\n  cat('starting model', names(modellist[n]), '\\n')\n\n  tstart=proc.time(); #capture current time\n\n  fl[[n]]$fit &lt;- brm(formula = modellist[[n]],\n                   data = fitdat,\n                   family = gaussian(),\n                   prior = priorlist[[n]],\n                   init = startlist[[n]],\n                   control=list(adapt_delta=adapt_delta, max_treedepth = max_td),\n                   sample_prior = TRUE,\n                   chains=chains, cores = cores,\n                   warmup = warmup, iter = iter,\n                   seed = seed,\n                   backend = \"cmdstanr\"\n  )# end brm statement\n\n  tend=proc.time(); #capture current time\n  tdiff=tend-tstart;\n  runtime_minutes=tdiff[[3]]/60;\n\n  cat('model fit took this many minutes:', runtime_minutes, '\\n')\n  cat('************** \\n')\n\n  #add some more things to the fit object\n  fl[[n]]$runtime = runtime_minutes\n  fl[[n]]$model = names(modellist)[n]\n}\n# saving the results so we can use them later\nfilepath = fs::path(\"C:\",\"Dropbox\",\"datafiles\",\"longitudinalbayes\",\"brmsfits\", ext=\"Rds\")\n#filepath = fs::path(\"D:\",\"Dropbox\",\"datafiles\",\"longitudinalbayes\",\"brmsfits\", ext=\"Rds\")\nsaveRDS(fl,filepath)\n\nYou‚Äôll likely find that model 1 takes the longest, the other ones run faster. You can check the runtime for each model by looking at fl[[n]]$runtime. It‚Äôs useful to first run with few iterations (100s instead of 1000s), make sure everything works in principle, then do a ‚Äúfinal‚Äù long run with longer chains."
  },
  {
    "objectID": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#models-1-and-3",
    "href": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#models-1-and-3",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 3",
    "section": "Models 1 and 3",
    "text": "Models 1 and 3\nLet‚Äôs explore those two models first. Recall that they are the same, apart from the prior definitions. As previously, the wider priors for model 1 make it less efficient. With the settings I used, run times were 417 minutes for model 1 versus 61 minutes for model 3.\nLet‚Äôs see if the priors impact the results, i.e.¬†the posterior distributions. We can actually do that by looking briefly at the summaries for both fits.\n\n#save some typing\nfit1 &lt;- fl[[1]]$fit\nfit3 &lt;- fl[[3]]$fit\nsummary(fit1)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: outcome ~ exp(alpha) * log(time) - exp(beta) * time \n         alpha ~ 0 + id + dose_adj\n         beta ~ 0 + id + dose_adj\n   Data: fitdat (Number of observations: 264) \n  Draws: 5 chains, each with iter = 9000; warmup = 6000; thin = 1;\n         total post-warmup draws = 15000\n\nRegression Coefficients:\n               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nalpha_id1          3.50      1.67     0.26     6.80 1.00     2840     4734\nalpha_id2          3.46      1.67     0.21     6.75 1.00     2839     4794\nalpha_id3          3.26      1.67     0.02     6.56 1.00     2839     4760\nalpha_id4          3.19      1.67    -0.05     6.49 1.00     2841     4760\nalpha_id5          3.24      1.67    -0.01     6.53 1.00     2839     4760\nalpha_id6          3.33      1.67     0.08     6.62 1.00     2839     4760\nalpha_id7          3.28      1.67     0.03     6.58 1.00     2841     4796\nalpha_id8          2.98      0.02     2.95     3.01 1.00    17885    10342\nalpha_id9          2.91      0.02     2.88     2.94 1.00    17315    10942\nalpha_id10         2.98      0.02     2.95     3.01 1.00    17656    10966\nalpha_id11         2.94      0.02     2.91     2.97 1.00    18085    10133\nalpha_id12         2.84      0.02     2.81     2.88 1.00    17692    11631\nalpha_id13         2.97      0.02     2.94     3.00 1.00    18451    10345\nalpha_id14         3.09      0.01     3.06     3.12 1.00    18387    10003\nalpha_id15         2.95      0.02     2.91     2.98 1.00    17682    10963\nalpha_id16         2.77      1.67    -0.52     6.01 1.00     2839     4781\nalpha_id17         2.54      1.67    -0.76     5.79 1.00     2840     4750\nalpha_id18         2.73      1.67    -0.57     5.97 1.00     2839     4798\nalpha_id19         2.76      1.67    -0.53     6.01 1.00     2839     4820\nalpha_id20         2.73      1.67    -0.56     5.98 1.00     2840     4771\nalpha_id21         2.71      1.67    -0.59     5.96 1.00     2840     4751\nalpha_id22         2.66      1.67    -0.64     5.91 1.00     2839     4807\nalpha_id23         2.65      1.67    -0.64     5.90 1.00     2840     4764\nalpha_id24         2.59      1.67    -0.70     5.84 1.00     2838     4762\nalpha_dose_adj     0.22      0.73    -1.19     1.65 1.00     2839     4785\nbeta_id1           0.75      1.71    -2.66     4.10 1.00     2420     4179\nbeta_id2           0.65      1.71    -2.76     4.01 1.00     2420     4181\nbeta_id3           0.70      1.71    -2.72     4.04 1.00     2419     4158\nbeta_id4           0.71      1.71    -2.70     4.06 1.00     2419     4155\nbeta_id5           0.93      1.71    -2.48     4.28 1.00     2418     4167\nbeta_id6           0.68      1.71    -2.73     4.03 1.00     2419     4175\nbeta_id7           0.77      1.71    -2.64     4.13 1.00     2419     4155\nbeta_id8           1.01      0.01     0.99     1.04 1.00    16977    10323\nbeta_id9           0.91      0.02     0.88     0.94 1.00    17374    11382\nbeta_id10          0.98      0.01     0.96     1.01 1.00    18009    10155\nbeta_id11          1.15      0.01     1.13     1.18 1.00    18260    10293\nbeta_id12          1.05      0.01     1.02     1.07 1.00    17891    11580\nbeta_id13          1.01      0.01     0.98     1.04 1.00    18998    10824\nbeta_id14          0.95      0.01     0.92     0.98 1.00    18321    10396\nbeta_id15          0.79      0.02     0.75     0.82 1.00    17550    11046\nbeta_id16          1.36      1.71    -1.99     4.77 1.00     2418     4208\nbeta_id17          1.08      1.71    -2.27     4.49 1.00     2419     4159\nbeta_id18          1.36      1.71    -2.00     4.77 1.00     2421     4150\nbeta_id19          1.44      1.71    -1.92     4.85 1.00     2417     4173\nbeta_id20          1.09      1.71    -2.25     4.50 1.00     2420     4083\nbeta_id21          1.31      1.71    -2.04     4.73 1.00     2420     4118\nbeta_id22          1.24      1.71    -2.10     4.65 1.00     2421     4122\nbeta_id23          1.12      1.71    -2.23     4.53 1.00     2419     4157\nbeta_id24          1.09      1.71    -2.26     4.51 1.00     2419     4209\nbeta_dose_adj     -0.21      0.74    -1.69     1.24 1.00     2419     4163\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.06      0.05     0.97     1.17 1.00    16342    11307\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nsummary(fit3)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: outcome ~ exp(alpha) * log(time) - exp(beta) * time \n         alpha ~ 0 + id + dose_adj\n         beta ~ 0 + id + dose_adj\n   Data: fitdat (Number of observations: 264) \n  Draws: 5 chains, each with iter = 9000; warmup = 6000; thin = 1;\n         total post-warmup draws = 15000\n\nRegression Coefficients:\n               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nalpha_id1          3.31      0.25     2.83     3.80 1.00     2443     4189\nalpha_id2          3.27      0.25     2.79     3.75 1.00     2418     4115\nalpha_id3          3.07      0.25     2.59     3.55 1.00     2447     4120\nalpha_id4          3.00      0.25     2.52     3.48 1.00     2433     4196\nalpha_id5          3.05      0.25     2.56     3.53 1.00     2437     4206\nalpha_id6          3.14      0.25     2.66     3.62 1.00     2465     4179\nalpha_id7          3.09      0.25     2.61     3.57 1.00     2438     4311\nalpha_id8          2.98      0.02     2.95     3.01 1.00    18493    10858\nalpha_id9          2.91      0.02     2.87     2.94 1.00    18973    11242\nalpha_id10         2.98      0.02     2.95     3.01 1.00    18479    10529\nalpha_id11         2.94      0.02     2.91     2.97 1.00    18804    10454\nalpha_id12         2.84      0.02     2.81     2.87 1.00    18873    11236\nalpha_id13         2.97      0.02     2.94     3.00 1.00    19043    11438\nalpha_id14         3.09      0.01     3.06     3.12 1.00    19247    10690\nalpha_id15         2.95      0.02     2.91     2.98 1.00    19114    12099\nalpha_id16         2.96      0.25     2.48     3.44 1.00     2432     4256\nalpha_id17         2.73      0.25     2.25     3.21 1.00     2418     4214\nalpha_id18         2.92      0.25     2.43     3.39 1.00     2433     4335\nalpha_id19         2.95      0.25     2.47     3.43 1.00     2438     4308\nalpha_id20         2.92      0.25     2.43     3.40 1.00     2427     4161\nalpha_id21         2.90      0.25     2.41     3.37 1.00     2418     4311\nalpha_id22         2.85      0.25     2.36     3.33 1.00     2439     4182\nalpha_id23         2.84      0.25     2.36     3.32 1.00     2431     4213\nalpha_id24         2.78      0.25     2.30     3.26 1.00     2438     4170\nalpha_dose_adj     0.14      0.11    -0.07     0.35 1.00     2426     4307\nbeta_id1           1.05      0.24     0.58     1.53 1.00     2953     5165\nbeta_id2           0.96      0.24     0.49     1.43 1.00     2937     5242\nbeta_id3           1.00      0.24     0.53     1.47 1.00     2944     5168\nbeta_id4           1.01      0.24     0.54     1.49 1.00     2937     5142\nbeta_id5           1.24      0.24     0.76     1.71 1.00     2939     5218\nbeta_id6           0.99      0.24     0.52     1.46 1.00     2946     5117\nbeta_id7           1.08      0.24     0.60     1.55 1.00     2941     5223\nbeta_id8           1.01      0.01     0.99     1.04 1.00    18029    10844\nbeta_id9           0.91      0.02     0.88     0.93 1.00    18953    11005\nbeta_id10          0.98      0.01     0.96     1.01 1.00    18500    10509\nbeta_id11          1.15      0.01     1.13     1.17 1.00    18599    10418\nbeta_id12          1.05      0.01     1.02     1.07 1.00    19002    10853\nbeta_id13          1.01      0.01     0.98     1.04 1.00    18714    11040\nbeta_id14          0.95      0.01     0.92     0.98 1.00    19168    10286\nbeta_id15          0.79      0.02     0.75     0.82 1.00    18771    11344\nbeta_id16          1.06      0.24     0.58     1.53 1.00     2943     5165\nbeta_id17          0.78      0.25     0.30     1.25 1.00     2941     5155\nbeta_id18          1.05      0.24     0.58     1.53 1.00     2939     5207\nbeta_id19          1.14      0.24     0.66     1.61 1.00     2951     5251\nbeta_id20          0.79      0.25     0.31     1.26 1.00     2962     5253\nbeta_id21          1.00      0.24     0.52     1.47 1.00     2944     5222\nbeta_id22          0.94      0.24     0.46     1.41 1.00     2951     5207\nbeta_id23          0.82      0.25     0.34     1.29 1.00     2943     5263\nbeta_id24          0.79      0.25     0.31     1.26 1.00     2957     5311\nbeta_dose_adj     -0.08      0.11    -0.29     0.13 1.00     2939     5258\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.06      0.05     0.97     1.17 1.00    15961    10880\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNote the different naming of the parameters in brms. It‚Äôs unfortunately not possible (as far as I know) to get the names match the mathematical model. The parameters that have dose in their names are the ones we called \\(a_1\\) and \\(b_1\\) in our models. The many _id parameters are our previous \\(a_0\\) and \\(b_0\\) parameters. Conceptually, the latter are on the individual level. But we don‚Äôt have a nested/multi-level structure here, which seems to lead brms to consider every parameter on the same level, and thus labeling them all population level.\nNow, let‚Äôs look at priors and posteriors somewhat more. First, we extract priors and posteriors.\n\n#get priors and posteriors for models 1 and 3\nm1prior &lt;- prior_draws(fit1)\nm1post &lt;- as_draws_df(fit1)\nm3prior &lt;- prior_draws(fit3)\nm3post &lt;- as_draws_df(fit3)\n\nNow we can plot the distributions. I‚Äôm focusing on the \\(a_1\\) and \\(b_1\\) parameters since those are of more interest, and because I couldn‚Äôt figure out quickly how to get out and process all the individual level \\(a_0\\) and \\(b_0\\) parameters from brms üòÅ.\n\n#showing density plots for a1\n\n#make a data frame and get it in shape for ggplot\na1df &lt;- data.frame(m1_prior = m1prior$b_alpha_dose_adj,\n                   m1_post = m1post$b_alpha_dose_adj,\n                   m3_prior = m3prior$b_alpha_dose_adj,\n                   m3_post =  m3post$b_alpha_dose_adj) %&gt;%\n        pivot_longer(cols = everything(), names_to = c(\"model\",\"type\"), names_pattern = \"(.*)_(.*)\", values_to = \"value\")\n# make plot\np1 &lt;- a1df %&gt;%\n  ggplot() +\n  geom_density(aes(x = value, color = model, linetype = type), size = 1) +\n  theme_minimal()\nplot(p1)\n\n\n\n\n\n\n\n#save for display on post\nggsave(file = paste0(\"featured.png\"), p1, dpi = 300, units = \"in\", width = 6, height = 6)\n\n\n#showing density plots for b1\nb1df &lt;- data.frame(m1_prior = m1prior$b_beta_dose_adj,\n                   m1_post = m1post$b_beta_dose_adj,\n                   m3_prior = m3prior$b_beta_dose_adj,\n                   m3_post =  m3post$b_beta_dose_adj) %&gt;%\n  pivot_longer(cols = everything(), names_to = c(\"model\",\"type\"), names_pattern = \"(.*)_(.*)\", values_to = \"value\")\n\np2 &lt;- b1df %&gt;%\n  ggplot() +\n  geom_density(aes(x = value, color = model, linetype = type), size = 1) +\n  theme_minimal()\nplot(p2)\n\n\n\n\n\n\n\n\nAs before, the priors for the \\(a_1\\) and \\(b_1\\) parameters are the same. We only changed the \\(a_0\\) and \\(b_0\\) priors, but that change leads to different posteriors for \\(a_1\\) and \\(b_1\\). It‚Äôs basically the same result we found with ulam/rethinking.\nIt would be surprising if we did NOT find the same correlation structure again in the parameters, let‚Äôs check it.\n\n# a few parameters for each dose\n#low dose\npairs(fit1, variable = variables(fit1)[c(1:4,25)])\n\n\n\n\n\n\n\n#medium dose\npairs(fit1, variable = variables(fit1)[c(8:11,25)])\n\n\n\n\n\n\n\n#high dose\npairs(fit1, variable = variables(fit1)[c(16:19,25)])\n\n\n\n\n\n\n\n\nApart from the unfortunate naming of parameters in brms, these are the same plots as we made for the ulam fits and show the same patterns.\nLet‚Äôs look at the posteriors in numerical form.\n\n# model 1 first\nfit1pars = posterior::summarize_draws(m1post, \"mean\", \"sd\", \"quantile2\", default_convergence_measures())\n\n#only entries for the a0 parameters\na0post &lt;- m1post %&gt;% dplyr::select(starts_with('b_alpha_id'))\nfit1a0mean &lt;- mean(colMeans(a0post))\n#only entries for the b0 parameters\nb0post &lt;- m1post %&gt;% dplyr::select(starts_with('b_beta_id'))\nfit1b0mean &lt;- mean(colMeans(b0post))\nfit1otherpars &lt;- fit1pars %&gt;% dplyr::filter(!grepl('_id',variable)) %&gt;%\n  dplyr::filter(!grepl('prior',variable))\nprint(fit1otherpars)\n\n# A tibble: 4 √ó 8\n  variable             mean     sd       q5     q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;               &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 b_alpha_dose_adj    0.224 0.726    -0.972    1.44  1.00    2839.    4785.\n2 b_beta_dose_adj    -0.212 0.744    -1.44     1.01  1.00    2419.    4163.\n3 sigma               1.06  0.0514    0.981    1.15  1.00   16342.   11307.\n4 lp__             -549.    5.58   -559.    -540.    1.00    4974.    8286.\n\nprint(c(fit1a0mean,fit1b0mean))\n\n[1] 2.960140 1.006334\n\n# repeat for model 3\nfit3pars = posterior::summarize_draws(m3post, \"mean\", \"sd\", \"quantile2\", default_convergence_measures())\n#only entries for the a0 parameters\na0post &lt;- m3post %&gt;% dplyr::select(starts_with('b_alpha_id'))\nfit3a0mean &lt;- mean(colMeans(a0post))\n#only entries for the b0 parameters\nb0post &lt;- m3post %&gt;% dplyr::select(starts_with('b_beta_id'))\nfit3b0mean &lt;- mean(colMeans(b0post))\nfit3otherpars &lt;- fit3pars %&gt;% dplyr::filter(!grepl('_id',variable)) %&gt;%\n  dplyr::filter(!grepl('prior',variable))\nprint(fit3otherpars)\n\n# A tibble: 4 √ó 8\n  variable              mean     sd        q5       q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;                &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 b_alpha_dose_adj    0.142  0.107    -0.0334    0.316   1.00    2426.    4307.\n2 b_beta_dose_adj    -0.0811 0.106    -0.254     0.0921  1.00    2939.    5258.\n3 sigma               1.06   0.0515    0.982     1.15    1.00   15960.   10880.\n4 lp__             -453.     5.60   -463.     -444.      1.00    4605.    7829.\n\nprint(c(fit3a0mean,fit3b0mean))\n\n[1] 2.9756367 0.9808696\n\n\nAgain, model 1 seems worse, with higher uncertainty intervals for the \\(a_1\\) and \\(b_1\\) parameters and the mean further away from the true value.\nWe can also compare the models as we did for rethinking using these lines of code:\n\nfit13comp &lt;- loo_compare(add_criterion(fit1,\"waic\"),\n            add_criterion(fit3,\"waic\"),\n            criterion = \"waic\")\nprint(fit13comp, simplify = FALSE)\n\n                            elpd_diff se_diff elpd_waic se_elpd_waic p_waic\nadd_criterion(fit1, \"waic\")    0.0       0.0  -416.3      11.7         43.5\nadd_criterion(fit3, \"waic\")    0.0       0.2  -416.3      11.7         43.5\n                            se_p_waic waic   se_waic\nadd_criterion(fit1, \"waic\")    4.3     832.5   23.4 \nadd_criterion(fit3, \"waic\")    4.3     832.6   23.4 \n\n\nModel performance is similar between models. The WAIC values are also close to those reported by rethinking."
  },
  {
    "objectID": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#comparison-with-the-truth-and-ulam",
    "href": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#comparison-with-the-truth-and-ulam",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 3",
    "section": "Comparison with the truth and ulam",
    "text": "Comparison with the truth and ulam\nThe values used to generate the data are: \\(\\sigma =\\) 1, \\(\\mu_a =\\) 3, \\(\\mu_b =\\) 1, \\(a_1 =\\) 0.1, \\(b_1 =\\) -0.1.\nSince the models are the same as those we previously fit with ulam, only a different R package is used to run them, we should expect very similar results. This is the case. We find that as for the ulam fits, the estimates for \\(a_0\\), \\(b_0\\) and \\(\\sigma\\) are similar to the values used the generate the data, but estimates for \\(a_1\\) and \\(b_1\\) are not that great. The agreement with ulam is good, because we should expect that if we fit the same models, results should - up to numerical/sampling differences - be the same, no matter what software implementation we use. It also suggests that we did things right - or made the same mistake in both implementations! üòÅ.\nWhy the WAIC estimates are different is currently not clear to me. It could be that the 2 packages use different definitions/ways to compute it. Or something more fundamental is still different. I‚Äôm not sure."
  },
  {
    "objectID": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#model-2a-1",
    "href": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#model-2a-1",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 3",
    "section": "Model 2a",
    "text": "Model 2a\nThis is the model with only population-level estimates. We already explored it somewhat above when we looked at traceplots and trankplots and the like. Here is just another quick table for the posteriors.\n\nm2post &lt;- as_draws_df(fit2)\nfit2pars = posterior::summarize_draws(m2post, \"mean\", \"sd\", \"quantile2\", default_convergence_measures())\nfit2otherpars &lt;- fit2pars %&gt;% dplyr::filter(!grepl('prior',variable))\nprint(fit2otherpars)\n\n# A tibble: 6 √ó 8\n  variable               mean      sd        q5      q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;                 &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 b_alpha_Intercept    2.98   0.0211     2.95    3.02e+0  1.00    6244.    6691.\n2 b_alpha_dose_adj     0.0960 0.00967    0.0802  1.12e-1  1.00    6569.    7301.\n3 b_beta_Intercept     0.992  0.0188     0.961   1.02e+0  1.00    6386.    6724.\n4 b_beta_dose_adj     -0.0971 0.00862   -0.111  -8.29e-2  1.00    6947.    7786.\n5 sigma                6.88   0.302      6.39    7.39e+0  1.00    8390.    7850.\n6 lp__              -892.     1.59    -895.     -8.90e+2  1.00    4964.    7039.\n\n\nThe parameters that have _Intercept in their name are what we called \\(\\mu_a\\) and \\(\\mu_b\\), the ones containing _dose are our \\(a_1\\) and \\(b_1\\). We find pretty much the same results we found using ulam. Specifically, the main parameters are estimated well, but because the model is not very flexible, the estimate for \\(\\sigma\\) is much larger, since it needs to account for all the individual-level variation we ommitted from the model itself."
  },
  {
    "objectID": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#model-4-1",
    "href": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#model-4-1",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 3",
    "section": "Model 4",
    "text": "Model 4\nThis is what I consider the most interesting and conceptually best model. It performed best in the ulam fits. Let‚Äôs see how it looks here. It is worth pointing out that this model ran much faster compared to models 1 and 3, it only took 10.5518333 minutes.\nWe‚Äôll start with the summary for the model.\n\nfit4 &lt;- fl[[4]]$fit\nm4prior &lt;- prior_draws(fit4)\nm4post &lt;- as_draws_df(fit4)\nsummary(fit4)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: outcome ~ exp(alpha) * log(time) - exp(beta) * time \n         alpha ~ (1 | id) + dose_adj\n         beta ~ (1 | id) + dose_adj\n   Data: fitdat (Number of observations: 264) \n  Draws: 5 chains, each with iter = 9000; warmup = 6000; thin = 1;\n         total post-warmup draws = 15000\n\nMultilevel Hyperparameters:\n~id (Number of levels: 24) \n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(alpha_Intercept)     0.09      0.02     0.07     0.13 1.00     3685     6514\nsd(beta_Intercept)      0.12      0.02     0.09     0.16 1.00     4048     5853\n\nRegression Coefficients:\n                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nalpha_Intercept     2.99      0.02     2.95     3.03 1.00     3771     5404\nalpha_dose_adj      0.09      0.01     0.07     0.11 1.00     3979     5040\nbeta_Intercept      0.99      0.02     0.94     1.03 1.00     3486     5134\nbeta_dose_adj      -0.11      0.01    -0.13    -0.08 1.00     3855     5732\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.06      0.05     0.97     1.17 1.00    10136    10314\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNext, the prior/posterior plots. To ensure one can see the priors, I‚Äôm cutting off the y-axis at 10, that‚Äôs why the posteriors look a bit weird. They do infected extend and peak like the distributions shown for models 1 and 3.\n\n#showing density plots for a1 and b1\n#make a data frame and get it in shape for ggplot\nm4df &lt;- data.frame(a1_prior = m4prior$b_alpha_dose_adj,\n                   a1_post = m4post$b_alpha_dose_adj,\n                   b1_prior = m4prior$b_beta_dose_adj,\n                   b1_post = m4post$b_beta_dose_adj) %&gt;%\n  pivot_longer(cols = everything(), names_to = c(\"parameter\",\"type\"), names_pattern = \"(.*)_(.*)\", values_to = \"value\")\n# make plot\np1 &lt;- m4df %&gt;%\n  ggplot() +\n  ylim(0, 10) + xlim(-2, 2) +\n  geom_density(aes(x = value, color = parameter, linetype = type), adjust = 10, size = 1) +\n  ggtitle('model 4, parameters a1 and b1') +\n  theme_minimal()\nplot(p1)\n\n\n\n\n\n\n\n\nNumerical output for the posterior:\n\nfit4pars = posterior::summarize_draws(m4post, \"mean\", \"sd\", \"quantile2\", default_convergence_measures())\nfit4otherpars &lt;- fit4pars %&gt;% dplyr::filter(!grepl('_id',variable)) %&gt;%\n  dplyr::filter(!grepl('prior',variable)) %&gt;%\n  dplyr::filter(!grepl('z_',variable))\n\nprint(fit4otherpars)\n\n# A tibble: 6 √ó 8\n  variable               mean     sd        q5       q95  rhat ess_bulk ess_tail\n  &lt;chr&gt;                 &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 b_alpha_Intercept    2.99   0.0197    2.95      3.02    1.00    3771.    5404.\n2 b_alpha_dose_adj     0.0861 0.0106    0.0688    0.104   1.00    3979.    5040.\n3 b_beta_Intercept     0.987  0.0247    0.946     1.03    1.00    3486.    5134.\n4 b_beta_dose_adj     -0.106  0.0131   -0.127    -0.0844  1.00    3855.    5732.\n5 sigma                1.06   0.0517    0.981     1.15    1.00   10135.   10314.\n6 lp__              -468.     7.47   -481.     -457.      1.00    2720.    4987.\n\n\nThese estimates look good, close to the truth.\nFinishing with the pairs lots:\n\n# a few parameters for each dose\n#low dose\npairs(fit4, variable = variables(fit4)[c(1:4,25)])\n\n\n\n\n\n\n\n#medium dose\npairs(fit4, variable = variables(fit4)[c(8:11,25)])\n\n\n\n\n\n\n\n#high dose\npairs(fit4, variable = variables(fit4)[c(16:19,25)])\n\n\n\n\n\n\n\n\nThe strong correlations between parameters are reduced, the same we say with the ulam models.\nAs was the case for the ulam fits, model 4 seems to perform overall best."
  },
  {
    "objectID": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#comparing-all-models",
    "href": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#comparing-all-models",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 3",
    "section": "Comparing all models",
    "text": "Comparing all models\nWe can repeat the model comparison we did above, now including all 4 models. I‚Äôm looking now at both WAIC and LOO (leave one out). Note the various warning messages. We got that as well when we computed PSIS (which is similar to LOO) with rethinking.\n\nfit1a &lt;- add_criterion(fit1,c(\"waic\",\"loo\"))\nfit2a &lt;- add_criterion(fit2,c(\"waic\",\"loo\"))\nfit3a &lt;- add_criterion(fit3,c(\"waic\",\"loo\"))\nfit4a &lt;- add_criterion(fit4,c(\"waic\",\"loo\"))\ncompall1 &lt;- loo_compare(fit1a,fit2a,fit3a,fit4a, criterion = \"waic\")\ncompall2 &lt;- loo_compare(fit1a,fit2a,fit3a,fit4a, criterion = \"loo\")\nprint(compall1, simplify = FALSE)\n\n      elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic   se_waic\nfit4a    0.0       0.0  -415.7      11.7         42.7    4.3     831.4   23.4 \nfit1a   -0.6       1.2  -416.3      11.7         43.5    4.3     832.5   23.4 \nfit3a   -0.6       1.3  -416.3      11.7         43.5    4.3     832.6   23.4 \nfit2a -473.6      23.0  -889.4      22.4         10.3    3.0    1778.7   44.8 \n\nprint(compall2, simplify = FALSE)\n\n      elpd_diff se_diff elpd_loo se_elpd_loo p_loo  se_p_loo looic  se_looic\nfit4a    0.0       0.0  -419.8     12.1        46.8    5.0    839.6   24.2  \nfit3a   -0.6       1.4  -420.4     12.1        47.6    5.0    840.7   24.2  \nfit1a   -1.1       1.5  -421.0     12.2        48.2    5.1    841.9   24.4  \nfit2a -469.6      23.0  -889.4     22.4        10.3    3.1   1778.9   44.8  \n\n\nModel 4 is considered best, though not by much. The above results, namely faster runtime and better estimates, speak more convincingly to the fact that model 4 is the best of these. The LOO is close to the PSIS metric reported by rethinking, even though I don‚Äôt think it‚Äôs defined and computed exactly the same."
  },
  {
    "objectID": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#prior-exploration",
    "href": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#prior-exploration",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 3",
    "section": "Prior exploration",
    "text": "Prior exploration\nSince brms has a way of specifying the model and priors that makes direct mapping to the mathematical model a bit more opaque, it is useful to explore if the models we run are what we think we run. brms has two helpful functions for looking at priors. One can help set priors before fitting, the other shows priors after fitting. To make the output manageable, we look at the simplest model, model 2. This looks as follows\n\n#defining model again\nm2aeqs &lt;- bf(outcome ~ exp(alpha)*log(time) - exp(beta)*time,\n  alpha ~ 1 + dose_adj,\n  beta  ~  1 + dose_adj,\n  nl = TRUE)\npreprior2 &lt;- get_prior(m2aeqs,data=fitdat,family=gaussian())\npostprior2 &lt;- prior_summary(fit2)\nprint(preprior2)\n\n               prior class      coef group resp dpar nlpar lb ub       source\n student_t(3, 0, 23) sigma                                  0         default\n              (flat)     b                           alpha            default\n              (flat)     b  dose_adj                 alpha       (vectorized)\n              (flat)     b Intercept                 alpha       (vectorized)\n              (flat)     b                            beta            default\n              (flat)     b  dose_adj                  beta       (vectorized)\n              (flat)     b Intercept                  beta       (vectorized)\n\nprint(postprior2)\n\n           prior class      coef group resp dpar nlpar lb ub  source\n          (flat)     b                           alpha       default\n  normal(0.3, 1)     b  dose_adj                 alpha          user\n    normal(2, 2)     b Intercept                 alpha          user\n          (flat)     b                            beta       default\n normal(-0.3, 1)     b  dose_adj                  beta          user\n  normal(0.5, 2)     b Intercept                  beta          user\n    cauchy(0, 1) sigma                                  0       user\n\n\nThe first output shows the priors as the model sees them, before we apply any settings. It uses defaults. The second output shows the actual priors used when fitting the model, which are the ones we set. I find these functions and the information useful, but overall it‚Äôs still a bit confusing to me. For instance why are there those flat entries in there? I don‚Äôt know what they mean.\nIt gets worse for bigger models, and here things get confusing to me. This is looking at the priors for models 1,3 and 4. Recall that we expect \\(2(N+1)+1\\) priors for models 1 and 3, and \\(2(N+1+1)+1\\) for model 4. Since our data has 24 samples, we should find 51 and 53 priors. Here is what we get:\n\npostprior1 &lt;- prior_summary(fit1)\npostprior3 &lt;- prior_summary(fit3)\npostprior4 &lt;- prior_summary(fit4)\nprint(paste(nrow(postprior1),nrow(postprior3),nrow(postprior4)))\n\n[1] \"53 53 13\"\n\n\nCloser inspection shows that for models 1 and 3, the priors include those strange flat ones that only have a class but no coefficient. My guess is those are not ‚Äúreal‚Äù, and thus we actually have the right number of priors/parameters. This can be checked by looking at the names of all the parameters for say model 1. Here they are:\n\nnames(m1post)\n\n  [1] \"b_alpha_id1\"            \"b_alpha_id2\"            \"b_alpha_id3\"           \n  [4] \"b_alpha_id4\"            \"b_alpha_id5\"            \"b_alpha_id6\"           \n  [7] \"b_alpha_id7\"            \"b_alpha_id8\"            \"b_alpha_id9\"           \n [10] \"b_alpha_id10\"           \"b_alpha_id11\"           \"b_alpha_id12\"          \n [13] \"b_alpha_id13\"           \"b_alpha_id14\"           \"b_alpha_id15\"          \n [16] \"b_alpha_id16\"           \"b_alpha_id17\"           \"b_alpha_id18\"          \n [19] \"b_alpha_id19\"           \"b_alpha_id20\"           \"b_alpha_id21\"          \n [22] \"b_alpha_id22\"           \"b_alpha_id23\"           \"b_alpha_id24\"          \n [25] \"b_alpha_dose_adj\"       \"b_beta_id1\"             \"b_beta_id2\"            \n [28] \"b_beta_id3\"             \"b_beta_id4\"             \"b_beta_id5\"            \n [31] \"b_beta_id6\"             \"b_beta_id7\"             \"b_beta_id8\"            \n [34] \"b_beta_id9\"             \"b_beta_id10\"            \"b_beta_id11\"           \n [37] \"b_beta_id12\"            \"b_beta_id13\"            \"b_beta_id14\"           \n [40] \"b_beta_id15\"            \"b_beta_id16\"            \"b_beta_id17\"           \n [43] \"b_beta_id18\"            \"b_beta_id19\"            \"b_beta_id20\"           \n [46] \"b_beta_id21\"            \"b_beta_id22\"            \"b_beta_id23\"           \n [49] \"b_beta_id24\"            \"b_beta_dose_adj\"        \"sigma\"                 \n [52] \"prior_b_alpha_id1\"      \"prior_b_alpha_id2\"      \"prior_b_alpha_id3\"     \n [55] \"prior_b_alpha_id4\"      \"prior_b_alpha_id5\"      \"prior_b_alpha_id6\"     \n [58] \"prior_b_alpha_id7\"      \"prior_b_alpha_id8\"      \"prior_b_alpha_id9\"     \n [61] \"prior_b_alpha_id10\"     \"prior_b_alpha_id11\"     \"prior_b_alpha_id12\"    \n [64] \"prior_b_alpha_id13\"     \"prior_b_alpha_id14\"     \"prior_b_alpha_id15\"    \n [67] \"prior_b_alpha_id16\"     \"prior_b_alpha_id17\"     \"prior_b_alpha_id18\"    \n [70] \"prior_b_alpha_id19\"     \"prior_b_alpha_id20\"     \"prior_b_alpha_id21\"    \n [73] \"prior_b_alpha_id22\"     \"prior_b_alpha_id23\"     \"prior_b_alpha_id24\"    \n [76] \"prior_b_alpha_dose_adj\" \"prior_b_beta_id1\"       \"prior_b_beta_id2\"      \n [79] \"prior_b_beta_id3\"       \"prior_b_beta_id4\"       \"prior_b_beta_id5\"      \n [82] \"prior_b_beta_id6\"       \"prior_b_beta_id7\"       \"prior_b_beta_id8\"      \n [85] \"prior_b_beta_id9\"       \"prior_b_beta_id10\"      \"prior_b_beta_id11\"     \n [88] \"prior_b_beta_id12\"      \"prior_b_beta_id13\"      \"prior_b_beta_id14\"     \n [91] \"prior_b_beta_id15\"      \"prior_b_beta_id16\"      \"prior_b_beta_id17\"     \n [94] \"prior_b_beta_id18\"      \"prior_b_beta_id19\"      \"prior_b_beta_id20\"     \n [97] \"prior_b_beta_id21\"      \"prior_b_beta_id22\"      \"prior_b_beta_id23\"     \n[100] \"prior_b_beta_id24\"      \"prior_b_beta_dose_adj\"  \"prior_sigma\"           \n[103] \"lprior\"                 \"lp__\"                   \".chain\"                \n[106] \".iteration\"             \".draw\"                 \n\n\nWe can see that there are the right number of both priors and posterior parameters, namely 2 times 24 for the individual level parameters, plus 2 dose parameters and \\(\\sigma\\).\nI find model 4 more confusing. Here is the full list of priors:\n\nprint(postprior4)\n\n           prior class      coef group resp dpar nlpar lb ub       source\n          (flat)     b                           alpha            default\n  normal(0.3, 1)     b  dose_adj                 alpha               user\n    normal(2, 1)     b Intercept                 alpha               user\n          (flat)     b                            beta            default\n normal(-0.3, 1)     b  dose_adj                  beta               user\n  normal(0.5, 1)     b Intercept                  beta               user\n    cauchy(0, 1)    sd                           alpha  0            user\n    cauchy(0, 1)    sd                            beta  0            user\n    cauchy(0, 1)    sd              id           alpha  0    (vectorized)\n    cauchy(0, 1)    sd Intercept    id           alpha  0    (vectorized)\n    cauchy(0, 1)    sd              id            beta  0    (vectorized)\n    cauchy(0, 1)    sd Intercept    id            beta  0    (vectorized)\n    cauchy(0, 1) sigma                                  0            user\n\n\nAnd this shows the names of all parameters\n\nnames(m4post)\n\n  [1] \"b_alpha_Intercept\"         \"b_alpha_dose_adj\"         \n  [3] \"b_beta_Intercept\"          \"b_beta_dose_adj\"          \n  [5] \"sd_id__alpha_Intercept\"    \"sd_id__beta_Intercept\"    \n  [7] \"sigma\"                     \"r_id__alpha[1,Intercept]\" \n  [9] \"r_id__alpha[2,Intercept]\"  \"r_id__alpha[3,Intercept]\" \n [11] \"r_id__alpha[4,Intercept]\"  \"r_id__alpha[5,Intercept]\" \n [13] \"r_id__alpha[6,Intercept]\"  \"r_id__alpha[7,Intercept]\" \n [15] \"r_id__alpha[8,Intercept]\"  \"r_id__alpha[9,Intercept]\" \n [17] \"r_id__alpha[10,Intercept]\" \"r_id__alpha[11,Intercept]\"\n [19] \"r_id__alpha[12,Intercept]\" \"r_id__alpha[13,Intercept]\"\n [21] \"r_id__alpha[14,Intercept]\" \"r_id__alpha[15,Intercept]\"\n [23] \"r_id__alpha[16,Intercept]\" \"r_id__alpha[17,Intercept]\"\n [25] \"r_id__alpha[18,Intercept]\" \"r_id__alpha[19,Intercept]\"\n [27] \"r_id__alpha[20,Intercept]\" \"r_id__alpha[21,Intercept]\"\n [29] \"r_id__alpha[22,Intercept]\" \"r_id__alpha[23,Intercept]\"\n [31] \"r_id__alpha[24,Intercept]\" \"r_id__beta[1,Intercept]\"  \n [33] \"r_id__beta[2,Intercept]\"   \"r_id__beta[3,Intercept]\"  \n [35] \"r_id__beta[4,Intercept]\"   \"r_id__beta[5,Intercept]\"  \n [37] \"r_id__beta[6,Intercept]\"   \"r_id__beta[7,Intercept]\"  \n [39] \"r_id__beta[8,Intercept]\"   \"r_id__beta[9,Intercept]\"  \n [41] \"r_id__beta[10,Intercept]\"  \"r_id__beta[11,Intercept]\" \n [43] \"r_id__beta[12,Intercept]\"  \"r_id__beta[13,Intercept]\" \n [45] \"r_id__beta[14,Intercept]\"  \"r_id__beta[15,Intercept]\" \n [47] \"r_id__beta[16,Intercept]\"  \"r_id__beta[17,Intercept]\" \n [49] \"r_id__beta[18,Intercept]\"  \"r_id__beta[19,Intercept]\" \n [51] \"r_id__beta[20,Intercept]\"  \"r_id__beta[21,Intercept]\" \n [53] \"r_id__beta[22,Intercept]\"  \"r_id__beta[23,Intercept]\" \n [55] \"r_id__beta[24,Intercept]\"  \"prior_b_alpha_Intercept\"  \n [57] \"prior_b_alpha_dose_adj\"    \"prior_b_beta_Intercept\"   \n [59] \"prior_b_beta_dose_adj\"     \"prior_sigma\"              \n [61] \"prior_sd_id\"               \"prior_sd_id__1\"           \n [63] \"lprior\"                    \"lp__\"                     \n [65] \"z_1[1,1]\"                  \"z_1[1,2]\"                 \n [67] \"z_1[1,3]\"                  \"z_1[1,4]\"                 \n [69] \"z_1[1,5]\"                  \"z_1[1,6]\"                 \n [71] \"z_1[1,7]\"                  \"z_1[1,8]\"                 \n [73] \"z_1[1,9]\"                  \"z_1[1,10]\"                \n [75] \"z_1[1,11]\"                 \"z_1[1,12]\"                \n [77] \"z_1[1,13]\"                 \"z_1[1,14]\"                \n [79] \"z_1[1,15]\"                 \"z_1[1,16]\"                \n [81] \"z_1[1,17]\"                 \"z_1[1,18]\"                \n [83] \"z_1[1,19]\"                 \"z_1[1,20]\"                \n [85] \"z_1[1,21]\"                 \"z_1[1,22]\"                \n [87] \"z_1[1,23]\"                 \"z_1[1,24]\"                \n [89] \"z_2[1,1]\"                  \"z_2[1,2]\"                 \n [91] \"z_2[1,3]\"                  \"z_2[1,4]\"                 \n [93] \"z_2[1,5]\"                  \"z_2[1,6]\"                 \n [95] \"z_2[1,7]\"                  \"z_2[1,8]\"                 \n [97] \"z_2[1,9]\"                  \"z_2[1,10]\"                \n [99] \"z_2[1,11]\"                 \"z_2[1,12]\"                \n[101] \"z_2[1,13]\"                 \"z_2[1,14]\"                \n[103] \"z_2[1,15]\"                 \"z_2[1,16]\"                \n[105] \"z_2[1,17]\"                 \"z_2[1,18]\"                \n[107] \"z_2[1,19]\"                 \"z_2[1,20]\"                \n[109] \"z_2[1,21]\"                 \"z_2[1,22]\"                \n[111] \"z_2[1,23]\"                 \"z_2[1,24]\"                \n[113] \".chain\"                    \".iteration\"               \n[115] \".draw\"                    \n\n\nTo compare directly, this is the model we want:\n\\[\n\\begin{aligned}\nY_{i,t}  & \\sim \\mathrm{Normal}\\left(\\mu_{i,t}, \\sigma\\right) \\\\\n\\mu_{i,t} &  =  \\exp(\\alpha_{i}) \\log (t_{i}) -\\exp(\\beta_{i}) t_{i} \\\\\n\\alpha_{i} &  =  a_{0,i} + a_1 \\left(\\log (D_i) - \\log (D_m)\\right)  \\\\\n\\beta_{i} &  =  b_{0,i} + b_1 \\left(\\log (D_i) - \\log (D_m)\\right) \\\\\na_{0,i} & \\sim \\mathrm{Normal}(\\mu_a, \\sigma_a) \\\\\nb_{0,i} & \\sim \\mathrm{Normal}(\\mu_b, \\sigma_a) \\\\\na_1 & \\sim \\mathrm{Normal}(0.3, 1) \\\\\nb_1 & \\sim \\mathrm{Normal}(-0.3, 1) \\\\\n\\mu_a & \\sim \\mathrm{Normal}(2, 1) \\\\\n\\mu_b & \\sim \\mathrm{Normal}(0.5, 1) \\\\\n\\sigma &  \\sim \\mathrm{HalfCauchy}(0,1)  \\\\\n\\sigma_a & \\sim \\mathrm{HalfCauchy}(0,1)  \\\\\n\\sigma_b & \\sim \\mathrm{HalfCauchy}(0,1)  \n\\end{aligned}\n\\]\nIf understand brms correctly, those z_ parameters are internal adjustments to make things more efficient and can otherwise be ignored. That means we have 2 times 24 parameters for the individual levels that all start with r_id. Those correspond to the \\(a_{0,i}\\) and \\(b_{0,1}\\), and they don‚Äôt have pre-defined priors, since they are computed based on other parameters. Then we have 2 dose parameters, which map to \\(a_1\\) and \\(b_1\\), both come with priors. We have 2 _Intercept parameters, which correspond to \\(\\mu_a\\) and \\(\\mu_b\\), again with priors. We have \\(\\sigma\\) with prior, and the two sd_id parameters seem to be those we call \\(\\sigma_a\\) and \\(\\sigma_b\\) in our equations.\nSo it looks like there is a match between our mathematical model we want, and the way we implemented it in brms. Still, I find the brms notation confusing and not that easy to follow. In that respect I much prefer ulam/rethinking.\nIn any case, I somewhat convinced myself that I‚Äôm fitting the same models here with brms that I‚Äôm fitting with ulam."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sam Passmore",
    "section": "",
    "text": "Hi! I‚Äôm Dr Sam Passmore, and you have found my website!\nI am currently a Post-doctoral Research Fellow with the Evolution of Cultural Diversity Initiative, within the College of Asia and the Pacific, at the Australian National Univeristy (ANU).\nI use quantitative approaches to research anthropological and linguistic questions surrounding kinship, usually through an evolutionary lens.\nThis website contains a short bio, blog posts, and descriptions of my various research interests."
  },
  {
    "objectID": "posts/deep-ancestry-news.html",
    "href": "posts/deep-ancestry-news.html",
    "title": "Sookias, Passmore, and Atkinson (2018) - News coverage",
    "section": "",
    "text": "A paper I published with Roland Sookias and Quentin Atkinson was covered in the news! Perhaps not the papers I would have chosen, but exciting to see reports on your work in any case. There was coverage by the Times and the New York Post.\nThey didn‚Äôt really focus on the right aspects of the paper, which was unfortunate. We wrote this letter in response to the news coverage, but I do not think it was published anywhere except here.\nDear editor,\nWe write to voice our concern regarding your newspaper‚Äôs coverage of our recently published research article in the piece ‚ÄûHow communism can leave health and wealth in the red for decades‚Äù by Oliver Moody (April 11th 2018). We appreciate interest in our work, and the writer was well-informed. However, we would like to make clear that the main finding of our work was not ‚Äì contra presentation in the article ‚Äì a negative effect of communism on human development.\nOur dataset was not sufficient to assess this, and communism was used as a control variable to represent recent, horizontally-transferred culture as opposed to ancestrally inherited culture. We make this very clear in the main text of our article, which is freely accessible online (‚ÄúDeep cultural ancestry and human development indicators across nation states‚Äù Royal Society Open Science, doi:10.1098/rsos.171411). Geography, not ‚Äúcommunist‚Äù history, was the single best predictor of human development levels we found, and whether due to climate, access to trade, or spread of ideas, geography has far more impact than recent or historical culture on development.\nAlthough former nominally ‚Äúcommunist‚Äù Eurasian countries may have lower human development today, the reasons for this are complex, and in several points which must be born in mind in the context of our study:\n1. The dictatorial, not socialist, nature of these governments may be responsible, and was not separately tested in our study: state economic involvement, for example, is and was high in Scandinavia, but these countries have outstanding human development indices.\n2. Economic collapse following the end of communism, rather than communism per se was an important factor in poor human development in much of the former USSR.\n3. Central Asia and Eastern Europe was less wealthy even prior to communism, and the geography of some of this region (aridity, low access to the sea etc., distance from technological innovation) has almost certainly continued to impede development.\n4. Some nominally communist governments not included/separately included in our study ‚Äì e.g.¬†Cuba, the Indian state of Kerala ‚Äì have exceptionally high human development for their region.\nOverall, our study was not sufficient, and did not aim to investigate the socioeconomic impact of communism. Our principal finding is that geography and modern culture are more important than inherited ‚Äúdeep‚Äù culture in dictating modern human development index values.\nYours faithfully,\nDr.¬†Roland B. Sookias; Mr.¬†Samuel Passmore; Prof.¬†Quentin D. Atkinson"
  },
  {
    "objectID": "posts/bayestraitr.html",
    "href": "posts/bayestraitr.html",
    "title": "bayestraitr",
    "section": "",
    "text": "I‚Äôve been using BayesTraits for the last year or so, and had developed a package to make it easy to create data for BayesTraits and to read the output into R - called bayestraitr.\nThe main difference between this package and the existing R-BayesTraits package btw is that my package doesn‚Äôt let you interact with BayesTraits in R, only create BayesTraits input, and read BayesTraits output. I found this approach useful because BayesTraits is a constantly evolving software. By only dealing with inputs and outputs, users can interact with the most up-to-date version of BayesTraits, without relying on R packages to update alongside.\nYou can check out the code and some more examples on the github page.\nHere I will walk through how to use it!\n\nInstalling\nTo install this package run this line:\ndevtools::install_github('SamPassmore/bayestraitr')\nand load with\nlibrary(bayestraitr)\n\n\nReading output files\nThere are three functions in this package at the moment for reading in BayesTrait *.Log files, *.Schedule files, & *.Stones\nlog = bt_read.log('./bayestrait_output/Artiodactyl.txt.Log.txt')\nWill return a dataframe of the log matrix. The file header are contained as an attribute which can be accessed by using attributes(log)$settings\nschedule = bt_read.schedule('./bayestrait_output/Artiodactyl.txt.Schedule.txt')\nThis returns one data.frame with the contents of the Schedule file.\nstones = bt_read.stones('./bayestrait_output/Artiodactyl.txt.Stones.txt')\nThis returns a list of 2 items. \\$stones_sampling holds the stones sampling, and marginal_likelihood holds the marginal log-likelihood. The stones file header can be found in the attributes by using the code attributes(stones)$settings\n\n\nWriting input files\nThe function bt_write takes various tree types (e.g.¬†phylo & multiphylo) and data.frame input and writes them as input files for BayesTraits. This means replacing NAs with ‚Äú-‚Äù, removing column headers, and ensuring data files are tab-delimited.\nFirst, have you data and phylogeny loaded into R\nlibrary(ape)\ntree = read.nexus('bayestraits_output/Artiodactyl.trees')\ndata = read.csv('bayestraits_output/Artiodactyl.tsv', sep = \"\\t\")\nAs with most phylogenetic software in R, you need to make sure the rows of your dataframe indicate the taxa of the phylogeny they connect to.\nrownames(data) = data$taxa\nFinally, use bt_write to write your BayesTraits input files by indicating the names of your data, tree(s), as well as your variable(s) of interest, and the location and filename for the outputs. This function will save two files, one which holds your data (with the suffix *.btdata) and one which holds your tree(s) (*.bttrees).\nbt_write(tree = tree, data = data, variables = 'trait1', filename = 'test')\nThis package is in development, so please log any issues you find while using it! And keep an eye out for new functions!"
  },
  {
    "objectID": "posts/2024-05-20-musical-history/index.html",
    "href": "posts/2024-05-20-musical-history/index.html",
    "title": "Global musical diversity is largely independent of linguistic and genetic histories",
    "section": "",
    "text": "I lead a new paper published at Nature Communications titled ‚ÄúGlobal musical diversity is largely independent of linguistic and genetic histories‚Äù. This paper utilizes data from an earlier project, The Global Jukebox, to explore the spatial and temporal structure of music, and compare it to linguistic and genetic similarities. In general, we found that music contains spatial and temporal patterns needed to infer historical patterns, but that those patterns seem to diverge from linguistic and genetic histories. How musical, linguistic, and genetic histories can tell us an intergrated story of human history an interesting project for future studies\nThis paper is culmulation of many years work with my co-authors Patrick Savage, Anna Lomax-Wood, Dor Shilton, Chiara Barbieri, Hideo Daikoku, and Quentin Atkinson.\nRelated media\nThe paper was covered by a few online media websites: - Forbes - Forbes (Japan) - MyScience - ANU Reporter\nAs a result of the paper coming out I did a number of Radio interviews in Australia at: - ABC Radio Canberra (at 41 minutes) - ABC NewsRadio - ABC Western Australia - 2CC Radio\nTwitter / X coverage of the paper is recorderd with Altmetrics. At the time of writing, the paper had been seen 172 Twitter / X posts from 143 users, with an upper bound of 466,489 followers."
  },
  {
    "objectID": "posts/2023-07-07-kinbank-a-database-of-kinship-terminology/index.en.html",
    "href": "posts/2023-07-07-kinbank-a-database-of-kinship-terminology/index.en.html",
    "title": "Kinbank: A database of Kinship terminology media watch",
    "section": "",
    "text": "I recently published the main output from my PhD - Kinbank. Kinbank is a database of more than 1,200 kinship terminology from languages around the world. This post is a place to record the media reporting on the paper.\nYou can interact with the dataset here: www.kinbank.net\nYou can read the paper here: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0283218\nRelated media\nHere is my twitter thread about the paper: https://twitter.com/SamPassmore_/status/1661561173811294208?s=20\nPhys.org: Database stores names for family members in 1,200+ languages Swifttelecast: Database stores names for family members in 1,200+ languages\nANU Reporter: What‚Äôs in a name? Why the words we use for family members tell a much bigger story\nMuseu Em√≠lio Goeldi: Twitter thread\nReddit discussion: Linguistics\nPodcast ‚ÄúBeacause Language‚Äù: around 30 minute mark"
  },
  {
    "objectID": "posts/2023-05-16-cultural-evolution-society-ace-ecr-awards/index.en.html",
    "href": "posts/2023-05-16-cultural-evolution-society-ace-ecr-awards/index.en.html",
    "title": "Cultural Evolution Society ACE ECR Awards",
    "section": "",
    "text": "I recently applied to the Cultural Evolution Society‚Äôs Advancing Cultural Evolution (ACE) grant scheme. I narrowly missed out on the funding, but was listed as an honourable mention.\nFor wider interest, I reproduced the project description I submitted below. If you are interested in helping progress this project - please get in touch.\nResearch Question\nWhat causes kinship terminology to change? Kinship terminology are the system of words we use for family members, such as aunt or cousin in English. These words divide kin into categories that reflect the communicative needs of a society, which are in turn thought to reflect behavioural norms. The assumed connection between language and behaviour has been leveraged by researchers to make inferences on the social organisations of past societies, but is this assumption supported? And can we make specific predictions about this relationship?\nHypotheses\nThis project takes on two levels. First, we test the general claim that kinship terminology changes track significant changes in society, through the proximate measure of population size. Larger populations usually contain more levels of social hierarchy, more social institutions, and less dependence on kin for support, resulting in a change in kinship system. There is direct evidence that decreases in population size causes changes in kinship organisation, resulting in kinship terminology change.1 Kinship terminology change will be measured through kinship terminology complexity, calculated using the number of kinterms and types of categories.\nSecond, we identify whether specific changes in parts of kinship organisation (e.g.¬†changes in cousin marriage patterns) result in specific changes in kinship terminology (e.g.¬†changes in kinterms for cousins). We test three specific hypotheses: 1) Changes in cross-cousin marriage practices result in changes in cross-cousin distinctions, 2) Changes to extended family systems result in the extension of kinterms from siblings to cousins 3) Changes in intergenerational kinterms indicate changes in descent patterns. Each of these hypotheses identifies kinship system changes that influence the movement of people.\nPredictions: Kinship terminology change will show significant correlations to population size. However, the specific hypotheses only show relationships within some language families.\nData & Methods\nThis project will use phylogenetic sister-pair analyses to identify changes in kinship terminology.2 Sister-pair languages are the closest relatives to each other on a model of linguistic history. Within a sister-pair we know there have been no branching events since their most recent split, and this implies that any differences in kinship terminology must have occurred since that split. We can correlate terminology differences with changes to our hypothesised drivers, population size and kinship system change.\nSister-pairs will be identified from the taxonomic models of language evolution held in Glottolog. Data on kinship terminology will be drawn from Kinbank, a database of kinship terminology. Population sizes are drawn from Ethnologue but will be bolstered with ethnographic sources where available. Information on kinship system organisation will be drawn from D-PLACE. Languages will be sampled from the Austronesian (209 languages in Kinbank), Bantu (113), Indo-European (73), and Uto-Aztecan (37) language families. The sister-pair sampling approach will require these datasets to be enriched with further data collection to maximise the total pairs.\nProject Impact\nKinship systems are central systems to models of cultural evolution. For individuals, kinship norms guide our daily-lives, e.g.¬†in the construction of social-networks,3 and informing where we live.4 Individual-level processes accumulate to create macro-channels of human and cultural movement, directing the paths of language5 and technology.6 However, kinship systems are not stable through time. The macroevolutionary impact of kinship systems means a change in kinship systems produces a change in the channels of cultural evolution. Models of human and cultural history must understand the varying impact of kinship system change.\nInferring the kinship systems of the past is challenging. Behaviour does not fossilize, and although genetics may reveal the movements of some people, they ultimately only reveal part of the story. For example, genetics might reveal the presence of cross-cousin marriage practices, it cannot reveal anything about the relationships between men and their sibling‚Äôs children or the level of interdependence between cousins and siblings.\nAnthropologists have often relied on kinship terminology as a proximate tool for understanding societal kinship organisation. Notably, linguistic reconstruction of kinship terminology has been used to reconstruct the social organisation of societies in the past.7\nThe assumption used in kinship terminology reconstruction is that kinship terminology structure is caused by other parts of society. Simply, language structure reflects behaviour. Evidence for this assumption comes from the Kuikuru (Brazil), who saw a rapid population decline, meaning communities merged to consolidate resources. This also caused a shift from a kinship system of exogamy to endogamy, and a change in kinship terminology.8 The language-behaviour assumption is often extended to make inferences in the deep past. Reconstructions of cross-cousin kinterms in Nilo-Saharic languages inferred that cross-cousin marriage occurred in the past.9 In Proto-Oceanic societies, kinterms for mother‚Äôs brother and wife‚Äôs father were reconstructed to be the same, inferring the historic presence of the generalized exchange of women.10\nLarge-scale comparative models of kinship terminology across three language families have questioned that the assumption of a universal relationship between kinship terminology and behaviour is not supported.11 However, it was unclear whether the absence of correlation was due to data granularity or a genuine separation of language and behaviour. The release of Kinbank a database of over 1,000 kinship terminology, means we can assess the relationship between language and behaviour in a more granular and systematic fashion.\nBy identifying which changes in kinship behaviour are most likely to be reflected in changes to kinship terminology, we can more confidently paint pictures of our ancestors‚Äô social lives and how their social norms influenced cultural change."
  },
  {
    "objectID": "posts/2023-05-16-cultural-evolution-society-ace-ecr-awards/index.en.html#footnotes",
    "href": "posts/2023-05-16-cultural-evolution-society-ace-ecr-awards/index.en.html#footnotes",
    "title": "Cultural Evolution Society ACE ECR Awards",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDole, G. E. (1969). Generation Kinship Nomenclature as an Adaptation to Endogamy. Southwestern Journal of Anthropology, 25(2), 105‚Äì123. https://doi.org/10.1086/soutjanth.25.2.3629197‚Ü©Ô∏é\nBromham, L., Hua, X., Fitzpatrick, T. G., & Greenhill, S. J. (2015). Rate of language evolution is affected by population size. Proceedings of the National Academy of Sciences, 112(7), 2097‚Äì2102. https://doi.org/10.1073/pnas.1419704112‚Ü©Ô∏é\nMattison, S. M. et al.¬†Market integration, income inequality, and kinship system among the Mosuo of China. Evolutionary Human Sciences 1‚Äì34 (2022) doi:10.1017/ehs.2022.52.‚Ü©Ô∏é\nFortunato, L. & Jordan, F. Your place or mine? A phylogenetic comparative analysis of marital residence in Indo-European and Austronesian societies. Philosophical Transactions of the Royal Society B: Biological Sciences 365, 3913‚Äì3922 (2010).‚Ü©Ô∏é\nLansing, J. S. et al.¬†Kinship structures create persistent channels for language transmission. PNAS 114, 1291‚Äì12915 (2017).‚Ü©Ô∏é\nBuckley, C. D. & Boudot, E. The evolution of an ancient technology. Open Science 4, 170208 (2017).‚Ü©Ô∏é\nEhret, C. Reconstructing ancient kinship. Practice and theory in an African case study. Kinship, Language, and Prehistory: Per Hage and the Renaissance in Kinship Studies, University of Utah Press, Salt Lake City 46‚Äì74 (2011).‚Ü©Ô∏é\nDole, G. E. Generation Kinship Nomenclature as an Adaptation to Endogamy. Southwestern Journal of Anthropology 25, 105‚Äì123 (1969).‚Ü©Ô∏é\nEhret, C. Reconstructing ancient kinship. Practice and theory in an African case study. Kinship, Language, and Prehistory: Per Hage and the Renaissance in Kinship Studies, University of Utah Press, Salt Lake City 46‚Äì74 (2011).‚Ü©Ô∏é\nHage, P. Marking Universals and the Structure and Evolution of Kinship Terminologies: Evidence from Salish. The Journal of the Royal Anthropological Institute 5, 423‚Äì441 (1999).‚Ü©Ô∏é\nPassmore, S. & Jordan, F. M. No universals in the cultural evolution of kinship terminology. Evolutionary Human Sciences 2, (2020).‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2023-04-10-synapse-seminar/index.en.html",
    "href": "posts/2023-04-10-synapse-seminar/index.en.html",
    "title": "SYNAPSE seminar",
    "section": "",
    "text": "I gave a talk in the Evolution of Cultural Diversity seminar series ‚ÄúSYNAPSE‚Äù. The talk is about my research on kinship terminology. I talk about the relationship of kinship terminology to social organisation, and the importance of controlling for interdependence between langauges when examining this relationship. I then introduce Kinbank, a database of over 1,200 kinship terminology, and show how we can use this database to examine the extent of global diversity through statistical pipelines. Finally, I breifly discuss the relationship between kinship terminology and behaviour that I examined through small experimental pardigm comparing between English and Hindi.\nYou can watch the lecture at the location below:\n\n\n\nOpening slide for the SYNAPSE seminar talk by Sam Passmore"
  },
  {
    "objectID": "posts/2022-10-11-funding-in-australia-and-new-zealand/index.html",
    "href": "posts/2022-10-11-funding-in-australia-and-new-zealand/index.html",
    "title": "Funding in Australia and New Zealand",
    "section": "",
    "text": "To keep track of the various funding opportunities available to me and that I am interested in, I have created this blog page. I am primarily using it for my own purposes, since I keep forgetting when and where things are due, but it might also be useful to other Antipodean early career researchers.\nMy PhD was conferred in January 2020."
  },
  {
    "objectID": "posts/2022-10-11-funding-in-australia-and-new-zealand/index.html#marsden-fund-fast-start-grant",
    "href": "posts/2022-10-11-funding-in-australia-and-new-zealand/index.html#marsden-fund-fast-start-grant",
    "title": "Funding in Australia and New Zealand",
    "section": "Marsden Fund Fast-Start Grant",
    "text": "Marsden Fund Fast-Start Grant\n\nUp to seven years after your is PhD conferred\nI can apply up to 2027"
  },
  {
    "objectID": "posts/2022-10-11-funding-in-australia-and-new-zealand/index.html#decra-discovery-early-career-researcher-award",
    "href": "posts/2022-10-11-funding-in-australia-and-new-zealand/index.html#decra-discovery-early-career-researcher-award",
    "title": "Funding in Australia and New Zealand",
    "section": "DECRA (Discovery Early Career Researcher Award)",
    "text": "DECRA (Discovery Early Career Researcher Award)\n\nUp to four years after your is PhD conferred : ‚Äúhave an award of PhD date on, or after 1 March 2017‚Äù\n\nI can apply during the 2024 round & 2025 round"
  },
  {
    "objectID": "posts/2022-10-11-funding-in-australia-and-new-zealand/index.html#future-fellowships",
    "href": "posts/2022-10-11-funding-in-australia-and-new-zealand/index.html#future-fellowships",
    "title": "Funding in Australia and New Zealand",
    "section": "Future Fellowships",
    "text": "Future Fellowships\n\nThis is aimed at mid-career researchers. I cannot apply until after 2024."
  },
  {
    "objectID": "posts/2022-10-11-funding-in-australia-and-new-zealand/index.html#westpac-research-fellowship",
    "href": "posts/2022-10-11-funding-in-australia-and-new-zealand/index.html#westpac-research-fellowship",
    "title": "Funding in Australia and New Zealand",
    "section": "Westpac Research Fellowship",
    "text": "Westpac Research Fellowship\n\nThis is aimed at mid career researchers. I cannot apply until after 2023."
  },
  {
    "objectID": "posts/2022-10-11-funding-in-australia-and-new-zealand/index.html#anu-chl-specific-funding",
    "href": "posts/2022-10-11-funding-in-australia-and-new-zealand/index.html#anu-chl-specific-funding",
    "title": "Funding in Australia and New Zealand",
    "section": "ANU & CHL specific funding",
    "text": "ANU & CHL specific funding\n\nAsia Pacific Innovation Program (APIP)\nANU ECR Travel Grant\n\nAllowable once every two years"
  },
  {
    "objectID": "posts/2023-09-27-journal-impact-factor-google-scholar/index.en.html",
    "href": "posts/2023-09-27-journal-impact-factor-google-scholar/index.en.html",
    "title": "Journal Impact Factor & Google Scholar",
    "section": "",
    "text": "I am currently writing a grant, and grant agencies are very interested in Journal Impact Factors (IF). Sometimes this might be explicit, but in this case I just want to flag the journals I have published in that are the most well known and IF seems to be a good indicator of this. I am not very interested in IF, and so I am not really sure which of my publications are in good places, and which of them are not. So, here is an attempt to automate figuring out which of my publications are in high impact factor places using a combination of Google Scholar and Journal Citation Reports (JCR).\nFirst, I can use the scholar package in R to download all my published articles. This will create a dataframe where each row is a paper, and there are some columns describing that paper, like the title, authors, and most importantly, the journal.\n\nlibrary(knitr)\nlibrary(dplyr)\nlibrary(scholar)\n\n# My scholar ID\nid = 'zrjq62QAAAAJ'\n\n# Get my publications (a data frame)\npublications = get_publications(id)\n\npublications[,c(\"title\", \"author\", \"journal\", \"year\", \"cites\")] %&gt;% \n  head(.) %&gt;% \n  kable(.)\n\n\n\n\n\n\n\n\n\n\n\ntitle\nauthor\njournal\nyear\ncites\n\n\n\n\nGrambank reveals the importance of genealogical constraints on linguistic diversity and highlights the impact of language loss\nH Skirg√•rd, HJ Haynie, DE Blasi, H Hammarstr√∂m, J Collins, JJ Latarche, ‚Ä¶\nScience Advances\n2023\n101\n\n\nSequence alignment of folk song melodies reveals cross-cultural regularities of musical evolution\nPE Savage, S Passmore, G Chiba, TE Currie, H Suzuki, QD Atkinson\nCurrent Biology\n2022\n67\n\n\nCHIELD: the causal hypotheses in evolutionary linguistics database\nSG Roberts, A Killin, A Deb, C Sheard, SJ Greenhill, K Sinnem√§ki, ‚Ä¶\nJournal of language evolution\n2020\n43\n\n\nThe Global Jukebox: A public database of performing arts and culture.\nALC Wood, KR Kirby, CR Ember, S Silbert, H Daikoku, J McBride, ‚Ä¶\nPLOS ONE\n2022\n39\n\n\nNo universals in the cultural evolution of kinship terminology\nS Passmore, FM Jordan\nEvolutionary Human Sciences\n2020\n35\n\n\nShared cultural history as a predictor of political and economic changes among nation states\nLJ Matthews, S Passmore, PM Richard, RD Gray, QD Atkinson\nPloS one\n2016\n35\n\n\n\n\n\nThen, I can also use a lesser known package called JCRImpactFactor, to cross-reference the journals I have published in with their Impact Factor as recorded in this package. This package only contains the Impact Factor values up to 2019, but that will do for my purposes.\n\nlibrary(JCRImpactFactor)\n\n# Find impact factors\njournals = find.IF.JCR(publications$journal, year = 2019) \n\n[1] \"The following 13 journal(s) are not in JCR\"\n [1] \"Journal of language evolution\" \"Evolutionary Human Sciences\"  \n [3] \"Biological theory\"             \"Journal of Cognition\"         \n [5] \"Religion, Brain & Behavior\"    \"Open Mind\"                    \n [7] \"\"                              \"\"                             \n [9] \"OSF\"                           \"OSF\"                          \n[11] \"OSF\"                           \"\"                             \n[13] \"OSF\"                          \n\n# I have published in some journals more than once, so remove duplicates\njournals = journals[!duplicated(journals$Full.Journal.Title),]\n\njournals %&gt;% \n  arrange(desc(IF2019)) %&gt;%\n  kable(., row.names = FALSE)\n\n\n\n\nFull.Journal.Title\nIF2019\n\n\n\n\nScience Advances\n13.116\n\n\nNature Communications\n12.121\n\n\nCURRENT BIOLOGY\n9.601\n\n\nCOGNITION\n3.294\n\n\nPLoS One\n2.740\n\n\nRoyal Society Open Science\n2.647\n\n\nTopics in Cognitive Science\n2.511\n\n\n\n\n\nIt is worth noting that not all Journals have Impact factors recorded by JCR. Notably, this database contains no information on: The Journal of Language Evolution (IF = 2.25), Evolutionary Human Sciences (2.53), Biological Theory (1.14), or Religion, Brain, and Behavior (1.25).\nSo, in the end it was relatively straightforward to automate, although it doesn‚Äôt give perfect results. If anyone knows of a more up-to-date location for Journal Impact Factors, let me know!"
  },
  {
    "objectID": "posts/2023-02-10-ancestral-state-reconstruction-with-inla/index.html",
    "href": "posts/2023-02-10-ancestral-state-reconstruction-with-inla/index.html",
    "title": "Ancestral state reconstruction with INLA",
    "section": "",
    "text": "A common task in phylogenetics is ancestral state reconstruction. There are a number of tools to do this inside and outside R. Some noteable R examples are: using ape (which I will use for comparison today), and phytools, which are both frequentist implementations. Bayesian implementations are available using BayesTraits. This post shows how to implement a Bayesian ancestral state reconstruction approach using the R modelling package INLA. As with my previous INLA post, the process is a result of discussions with Russell Dinnage, and any mistakes are surely introduced by me.\nFor starters. I first simulate a simple ancestral state result using ape, where the ancestral state of the phylogeny is 5:\n\nlibrary(ape)\n\n# Simulate a tree for n taxa. \nn = 100 \nsim.tree = rcoal(n = n)\n\n# Simulate a continuous trait along that tree, where the ancestral state is 5\n# With a standard deviation of 1.5 (sigma)\ny = rTraitCont(phy = sim.tree, root.value = 5, sigma = 1.5)\n\n# Calculate the ancestral state\nanc = ace(y, sim.tree)\ncat(\"The ancestral state is estimated to be:\", round(anc$ace[1], 2), \"\\n\",\n    \"With a 95% CI of:\", round(anc$CI95[1,], 3))\n\nThe ancestral state is estimated to be: 4.38 \n With a 95% CI of: 2.466 6.297\n\n\nThe default ancestral state model is Brownian motion, and it does a pretty good job at estimating our known ancestral state of 5. This coding procedure is pretty simple, and to code the same results in INLA is quite complicated, so you may ask why we need something more complicated, which I will discuss at the end.\nNext, we code the same model up using INLA, which requires a small amount of data wrangling:\n\nlibrary(INLA)\nlibrary(stringr)\n\n# INLA requires the data as a dataframe. \nmodel_df = data.frame(label = names(y), y = y)\n\nAs discussed in my previous INLA post, INLA requires a precision matrix. In this instance, I use nodes = \"ALL\" rather than nodes = \"TIPS\". This means we include the relationships between all taxa and all nodes. We will use the information between nodes in the phylogeny to reconstruct our ancestral state.\n\n# INLA also requires the relationships between data to be \n# precision matrix\nprec_mat = MCMCglmm::inverseA(sim.tree,\n                   nodes = \"ALL\",\n                   scale = FALSE)$Ainv\n\nAs with many regression based functions, we will introduce the data we want to reconstruct using a dataframe. In the case of reconstruction, we will not only include rows in the dataframe for each observed datapoint (in this case extant taxa), but also for the unobserved points we want to reconstruct (the internal nodes and root of the phylogeny). The unknown data will be respresented with missing / NA values. In the next block of code I also create an index to match the rows of the dataframe to the rows of the precision matrix.\n\n# add in node data\nnode_idx = str_detect(rownames(prec_mat), \"Node\")\nnode_df = data.frame(label = rownames(prec_mat)[node_idx], \n                     y = NA)\n\nmodel_df = rbind(model_df, node_df)\n\n# Create matching index\nmodel_df$phylo_id = match(model_df$label, rownames(prec_mat))\n\nNow it is time to put it all together to calculate the ancestral states. To learn more about the construction of INLA models see my earlier post, or go to the INLA webpage. A new argument in this this model is control.predictor=list(compute=TRUE). This tells the model to predict the values which are missing in the response. In our case, there are the internal nodes.\n\nfit = inla(y ~ f(phylo_id,\n                 model = \"generic0\",\n                 Cmatrix = prec_mat), \n           data = model_df, \n           control.predictor=list(compute = TRUE),\n           control.compute=list(config = TRUE)\n           )\n\n\ncat(\"The ancestral state is estimated to be:\", round(fit$summary.fitted.values[101,1], 2), \"\\n\",\n    \"With a 95% CI of:\", unlist(round(fit$summary.fitted.values[101,c(3,5)], 3)))\n\nThe ancestral state is estimated to be: 5.39 \n With a 95% CI of: 4.224 6.55\n\n\nRecall that the ape::ace function estimated the mean to be 4.38 (95% CI: 2.47 - 6.3). Both approaches are close to the ancestral state (5), but INLA is closer and providing a narrower confidence interval. Comparatively, the INLA approach is more fiddly, but is providing us with less error, so there is some reward for the time, however there are two more benefits worth looking at. The first benefit is that INLA is a Bayesian approach, and being a Bayesian approach means we can also plot the posterior predictive distribution of the results, providing a clearer view of the error distribution:\n\npost = INLA::inla.posterior.sample(1000, fit)\nanc.post = lapply(post, function(p) p$latent[101]) %&gt;% \n  unlist()\n\nplot(density(anc.post), main = \"Posterior distribution of Ancestral State estimates\",\n     xlab = \"Ancestral state Estimate\", col = \"blue\", \n     xlim = range(y))\nabline(v = 5, lty = \"longdash\", lwd = 2, col = \"black\")\ntext(x = 4.9, y = 0.2, label = \"True Ancestral State\", srt = 90)\nabline(v = unlist(fit$summary.fitted.values[101,1]), lty = \"dotdash\", lwd = 2, col = \"blue\")\ntext(x = unlist(fit$summary.fitted.values[101,1]) - 0.1, y = 0.2, label = \"INLA Estimate\", srt = 90)\n# abline(v = unlist(fit$summary.fitted.values[101,c(3,5)]), lty = \"dashed\", col = \"blue\")\nabline(v = anc$ace[1], lty = \"dotdash\", lwd = 2, col = \"red\")\ntext(x = anc$ace[1] - 0.1, y = 0.2, label = \"ape::ace Estimate\", srt = 90)\nabline(v = anc$CI95[1,], lty = \"dashed\", col = \"red\")\ntext(x = anc$CI95[1,] - 0.1, y = 0.2, label = \"ape::ace 95% CI\", srt = 90)\n\n\n\n\n\n\n\n\nThe graph above exemplifies the difference in error and provides a clear visualization of the INLA model‚Äôs prediction. From here, we could tweak priors to try and improve the estimate and errors of the model.\nA second advantage of the INLA approach is that we can accomodate other factors into the model that we think might influence the ancestral state other than the phylogenetic history. For example, environmental factors may influence the current state of traits. I may extend this example in the future to accommodate such a possibility."
  },
  {
    "objectID": "posts/2023-02-15-to-make-a-reference-group-or-to-not-make-a-reference-group-that-is-the-question-this-blog-deals-with/index.en.html",
    "href": "posts/2023-02-15-to-make-a-reference-group-or-to-not-make-a-reference-group-that-is-the-question-this-blog-deals-with/index.en.html",
    "title": "To make a reference group, or to not make a reference group, that is the question this blog deals with.",
    "section": "",
    "text": "When modelling a single categorical variable in linear regression, there are two possibilities. Most commonly, and the default in R, is to estimate the difference between each level of the categorical variable relative to one reference (or baseline) level. This is called reference level coding. The less common alternative is to model each category separately, without a baseline, called level means coding. The choice between these two depends on what hypothesis you want to test, and changes the interpretation of coefficients and p-values. But first, lets understand the difference in what the model coefficients estimate.\nTo illustrate this example, imagine we have two societies of people who are known for their musical performance. One society, which we will call society A, is known for their preference for singing together, or group singing. As an example of group singing, we might think of the grand and unified choruses found in Samoa.\nIn the second society, society B, most songs only involve one singer, or solo singing. For an example of solo singing, consider the vivid tales told by solo singers throughout the British Isles.\nBetween these two fake societies, both groups perform group singing and solo singing, but 8 out of 10 songs from A are group singing, but only 2 from 10 within B.\ndf = data.frame(group = rep(c(\"A\", \"B\"), each = 10),\n                singing = c(rep(0:1, times = c(2, 8)),\n                            rep(0:1, times = c(8, 2))))\nIf we run the model with an intercept we will be using reference level coding. Group A will become the reference level to which other groups are compared. The intercept will be the ratio of group and solo songs, and the coefficient will tell us how different society B is from society A. If the model is run without an intercept, then we are performing level means coding, and the coefficients will estimate the ratio of group singing songs to solo singing within each society.\nLet‚Äôs look at the actual output. fit.1 contains the formula singing ~ group. By default, the modelling function glm includes an intercept, and makes the first level of the factor the baseline. So the formula really looks like this: singing ~ 1 + group. By default, the baseline factor is the first in the alphabet, society A.\nfit.1 = glm(singing ~ group, \n             data = df, \n           family = \"binomial\")\nIn the second model the formula is singing ~ 0 + group. The part of the formula 0 + specifically tells the modelling function to have no intercept, and model each category as a seperate level.\nfit.2 = glm(singing ~ 0 + group,\n            data = df, \n            family = \"binomial\")\nLet‚Äôs check the coefficients do what we think.\nSince we use a logistic regression, the raw coefficients are in log odds. To get the odds ratio, we exponentiate the results. Remember, society A has 2 songs which are sung by soloists, and 8 songs which are sung by a group, so society A‚Äôs ratio is 8/2 = 4. Society B has the reverse, 2 songs sung in a group and 8 songs sung by soloists, 2/8 = 0.25.\n### The True ratios \n# Within each society\nratios = tapply(df$singing, df$group, function(x) sum(x==1)/sum(x==0))\nratios\n\n   A    B \n4.00 0.25 \n\n# The ratio between ratios \nratios[2] / ratios[1]\n\n     B \n0.0625\nTo see if the model is estimating correctly, we compare the true ratios above, to the model coefficients. The first model should show the ratio of the baseline society as the intercept, society A = 4, and difference in ratios between groups as the coefficient. The second should return the ratios within each society, 4 for society A, and 0.25 for society B. That is, 4 / 0.25 = 0.0625.\n### Model Estimates of the ratios \n# Ratio of the baseline group and the difference between groups\nexp(coef(fit.1))\n\n(Intercept)      groupB \n     4.0000      0.0625 \n\n# Group A is the baseline and intercept\nexp(coef(fit.1)[1]) \n\n(Intercept) \n          4 \n\n# Group B is the baseline + the differences between groups\nexp(coef(fit.1)[1] + coef(fit.1)[2]) \n\n(Intercept) \n       0.25 \n\n# Difference between Group A and Group B \nexp(coef(fit.1)[2]) \n\ngroupB \n0.0625\nThe model with no intercept returns the estimated ratios for each society.\n# Ratios within each group\nexp(coef(fit.2))\n\ngroupA groupB \n  4.00   0.25"
  },
  {
    "objectID": "posts/2023-02-15-to-make-a-reference-group-or-to-not-make-a-reference-group-that-is-the-question-this-blog-deals-with/index.en.html#should-i-use-a-reference-group-or-not",
    "href": "posts/2023-02-15-to-make-a-reference-group-or-to-not-make-a-reference-group-that-is-the-question-this-blog-deals-with/index.en.html#should-i-use-a-reference-group-or-not",
    "title": "To make a reference group, or to not make a reference group, that is the question this blog deals with.",
    "section": "Should I use a reference group or not?",
    "text": "Should I use a reference group or not?\nThe choice between having a reference group or not depends on the hypothesis you want to test, and importantly changes the interpretation of the coefficent p-values. Let‚Äôs consider the two hypotheses based on our fictional group singing vs solo singing data.\nIf we use a reference group, we have one coefficient estimating the difference in ratios between society A and society B. The p-value for the intercept in this model is testing whether the ratio of group singing to solo singing in the reference group is different from 0. The p-value for the coefficient is testing the hypotheses ‚ÄúIs this society B the same, or different from society A‚Äù.\nThe p-values in a model without a reference group (that is, with no intercept), then the all the p-values are testing the hypotheses that the ratio of group singing to solo singing is not zero."
  },
  {
    "objectID": "posts/2023-11-09-understanding-ordinal-models/index.en.html",
    "href": "posts/2023-11-09-understanding-ordinal-models/index.en.html",
    "title": "Understanding Ordinal models",
    "section": "",
    "text": "In my role at ANU and within ECDI I have started a weekly drop-in session for quantitative and computer programming questions. If you are a student or staff member at ANU, you can check it out here.\nI received an enquiry from Caroline Hendy, for a project she is running with Emma Breslow, about running ordinal models to detect differences between two intersecting groups of individuals. I don‚Äôt have that much experience in running ordinal models so here I am working through the problem!"
  },
  {
    "objectID": "posts/2023-11-09-understanding-ordinal-models/index.en.html#data",
    "href": "posts/2023-11-09-understanding-ordinal-models/index.en.html#data",
    "title": "Understanding Ordinal models",
    "section": "Data",
    "text": "Data\nFirst off I will generate some fake data mimicking the data structure in the original problem. The project is looking to understand the patterns of language change across intersecting social identities: Jewish vs not Jewish and Australian vs American. Jewish communities across the world are generally connected through cultural and language, and most will be familiar, and use a number of Yiddish words: Chutzpah, Schlep, and Speil to name a few. Jewish communities exist across different countries and cultural environments, such as in Australia and America, so does familiarity with Yiddish words vary between these communities? And how does familiarity with these words vary amongst non-Jewish people in each of these countries?\nSpecifically, the hypotheses are:\n\nJewish people will be more familiar with all these words that non-Jewish people (regardless of Country)\nNon-Jewish Americans will be more familiar with the words than Non-Jewish Australians.\n\nThe response variable is a four-point scale in which participants are asked about their familiarity with various Yiddish words: 1 = Never heard it; 2 = Heard but don‚Äôt understand; 3 = Heard, understood, but don‚Äôt use; 4 = Heard, understood, and use. We also have two categories which each individual can fit into: Australian or American, and Jewish or not Jewish. Let‚Äôs create some fake data.\nA package designed for creating simulated data is faux, maintained by Lisa DeBruine. I‚Äôve not used this package before, but I have always heard good things. You can install it directly from CRAN, or from github for the development version (see the faux website for details).\nI am going to set up the design so that there is only a small difference between countries, and a large difference between communities. That is, a person who is Australian should be slightly less familiar than an American with the set of words, but Jewish people should generally be more familiar with these words than non-Jewish people, regardless of country.\nAs far as I can tell, faux can‚Äôt create ordinal variables, but we can mimic the effect by forcing the randomly creating variables to the nearest integer. I also load all the packages used throughout this post here.\n\n# packages for dealing with data and plotting\nlibrary(tidyr)\nlibrary(dplyr)\nlibrary(rlang)\nlibrary(stringr)\nlibrary(ggplot2)\nlibrary(bayesplot)\n\n# packages for simulating data\nlibrary(faux)\n\n# modelling packages\nlibrary(brms)\nlibrary(emmeans)\n\n# Build the model design\n## Subjects can be from these countries, and these communities (varies \"between\" subjects\")\nbetween = list(country = c(\"American\", \"Australian\"), jewish = c(\"Jewish\", \"NotJewish\"))\n\n# All subject will see these words ('varies within subjects')\nwords = c(\"Mensch\", \"Maven\", \"Kvetch\", \"Kippah\", \"Menorah\", \"Schmutz\")\nwithin = list(words = words)\n\n# This sets the average value for each category and word. I assume all words have the same impact\n# to make things simpler to interpret later\nmu = data.frame(\n  American_Jewish = rep(3.5, length(words)), # Jewish people score around 3.5 (of 4) on average\n  Australian_Jewish = rep(3.5, length(words)),\n  American_NotJewish = rep(1.5, length(words)), # American non-Jewish people score 1.5 (of 4) on average\n  Australian_NotJewish = rep(1.0, length(words)), # Australian non-Jewish people score 1.5 (of 4) on average\n  row.names = words\n)\n\n# Simulate some data\ndf = sim_design(between = between, \n                within = within, mu = mu,\n                n = 100, sd = 0.75, \n                dv = \"familiarity\", \n                plot = FALSE)\n\n# plot the altered design\nfaux::plot_design(df)\n\n\n\n\n\n\n\n# Make the dataset long\ndf = pivot_longer(df, cols = c(\"Mensch\", \"Maven\", \"Kvetch\", \"Kippah\", \"Menorah\", \"Schmutz\"), \n                  values_to = \"familiarity\")\n\n# Force the data to be integer using bankers rounding (i.e. above 0.5 goes up, below goes down)\ndf$familiarity = round(df$familiarity)\n\n# Since the simulation is continuous, it doesn't always work (sometimes we get 5s and 0s). \n# I fix the data to be between 1 and 4\ndf$familiarity[df$familiarity &lt; 1] = 1\ndf$familiarity[df$familiarity &gt; 4] = 4\n\n# ensure levels are ordered correctly \ndf$jewish = factor(df$jewish, levels = c(\"NotJewish\", \"Jewish\"))"
  },
  {
    "objectID": "posts/2023-11-09-understanding-ordinal-models/index.en.html#hypothesis-1-jewish-people-will-be-more-familiar-with-all-these-words",
    "href": "posts/2023-11-09-understanding-ordinal-models/index.en.html#hypothesis-1-jewish-people-will-be-more-familiar-with-all-these-words",
    "title": "Understanding Ordinal models",
    "section": "Hypothesis 1: Jewish people will be more familiar with all these words",
    "text": "Hypothesis 1: Jewish people will be more familiar with all these words\nThe benefit of generating the data is that we know what the result should be. After changing the data to make it fit ordinal assumptions, we know that the cumulative probability for a Jewish person knowing these words (i.e.¬†a Likert score of 3 or better) is approximately 99%, and a non-Jewish person is 51%. These cumulative probability scores should be approximately returnable by the model, if the fit is any good.\nIn the next code section, we run the same model as above, but now with a single predictor indicating whether the participant was Jewish or not.\n\n# Data summary\ntapply(as.numeric(df$familiarity), df$jewish, summary)\n\n$NotJewish\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   1.000   1.000   1.413   2.000   4.000 \n\n$Jewish\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   1.00    3.00    3.00    3.38    4.00    4.00 \n\ntapply(df$familiarity, df$jewish, table)\n\n$NotJewish\n\n  1   2   3   4 \n771 364  63   2 \n\n$Jewish\n\n  1   2   3   4 \n  7 123 477 593 \n\n# Single predictor model\nb2 = \n  brm(data = df, \n      family = cumulative,\n      familiarity ~ jewish,\n      prior = c(prior(normal(0, 1.5), class = Intercept),\n                prior(normal(0, 1.5), class = b)), # Add a normal prior for the beta coefs\n      iter = 2000, warmup = 1000, cores = 4, chains = 1,\n      init = list(inits), # here we add our start values that we used for the complete dataset\n      file = \"jewish_only\")  \n\nTo show that the model is representing the data, we can calculate the cuts from the raw data. The intercepts in this model represent someone who is not Jewish, so the intercept values should be pretty similar to the cuts for non-Jewish people.\n\n# function to calculate cuts following the code from above\ncut_func = function(f){\n  pr_k = table(f) / length(f)\n  \n  cum_pr_k = round(cumsum(pr_k), 5)\n  \n  # logit function from above\n  logit = function(x) log(x / (1 - x))\n  logit_cumprk = logit(cum_pr_k)\n  \n  list(cum_pr_k = cum_pr_k, logit_cumprk = logit_cumprk)\n}\n\nraw_data = tapply(df$familiarity, df$jewish, cut_func)\n\n# as log odds\nround(raw_data$NotJewish$logit_cumprk, 2)\n\n   1    2    3    4 \n0.59 2.86 6.39  Inf \n\nround(fixef(b2)[1:3,1], 2) # only looking at the intercepts\n\nIntercept[1] Intercept[2] Intercept[3] \n        0.49         2.77         5.08 \n\n# as probabilities \nround(raw_data$NotJewish$cum_pr_k, 2)\n\n   1    2    3    4 \n0.64 0.95 1.00 1.00 \n\nround(inv_logit_scaled(fixef(b2)[1:3,1]), 2)\n\nIntercept[1] Intercept[2] Intercept[3] \n        0.62         0.94         0.99 \n\n# as log odds\nround(raw_data$Jewish$logit_cumprk, 2)\n\n    1     2     3     4 \n-5.14 -2.11  0.02   Inf \n\nround(fixef(b2)[1:3,1] - fixef(b2)[4,1], 2)\n\nIntercept[1] Intercept[2] Intercept[3] \n       -4.55        -2.28         0.04 \n\n# as probabilities\nround(raw_data$Jewish$cum_pr_k, 2)\n\n   1    2    3    4 \n0.01 0.11 0.51 1.00 \n\nround(inv_logit_scaled(fixef(b2)[1:3,1] - fixef(b2)[4,1]), 2)\n\nIntercept[1] Intercept[2] Intercept[3] \n        0.01         0.09         0.51 \n\n\nThis model is doing a pretty good job at estimating the cumulative probability between Jewish and non-Jewish participants.\n\nWhy are you subtracting the coefficient for Jewish participants? Wouldn‚Äôt you normally add the coefficients?\nAn excellent question. For this we need to look at the maths underneath the model. I take the model from page 386 of McElreath‚Äôs Statistical Rethinking:\n\\(log { Pr(y_i \\leq k) \\over 1 - Pr(y_i \\leq k) = \\alpha_k - \\phi_i}\\) \\(\\phi_i = \\beta x_i\\)\nMcElreath writes:\n\nWhy is the linear model \\(\\phi\\) subtracted from each intercept? Because if we decrease the log-cumulative-odds of every outcome value k below the maximum, this necessarily shifts the probability mass upwards towards the higher outcome values.\n\nI understand this as a decision to improve interpretability. If \\(\\phi_i\\) was added to \\(\\alpha_k\\), we could get the same model predictions, but \\(\\beta &gt; 0\\) would mean increasing \\(x\\) would result in decrease in the response. Using \\(\\alpha_k - \\phi_i\\) means that increasing \\(x\\) means an increase in the response. So, subtracting the coefficient is done to make the results more intuitive.\nLet‚Äôs try plotting these results, following code from Solomon Kurz tidyr re-write of SR.\n\nnd = data.frame(jewish = c(\"Jewish\", \"NotJewish\"))\nfitted_df = fitted(b2, newdata = nd, summary = FALSE)\n\nfitted_df2 &lt;-\n  rbind(fitted_df[, , 1],\n        fitted_df[, , 2],\n        fitted_df[, , 3],\n        fitted_df[, , 4]) %&gt;% \n  data.frame() %&gt;% \n  set_names(pull(nd, jewish)) %&gt;% \n  mutate(response = rep(1:4, each = n() / 4),\n         draw     = rep(1:1000, times = 4)) %&gt;% \n  pivot_longer(-c(draw, response),\n               names_to = c(\"jewish\"),\n               values_to = \"pk\") %&gt;% \n  mutate(jewish = jewish %&gt;% factor(., levels = c(\"NotJewish\", \"Jewish\")))\n\nplot_df =\n  fitted_df2 %&gt;% \n  # this will help us define the three panels of the triptych\n  mutate(facet = factor(str_c(\"jewish=\", jewish))) %&gt;% \n  # these next three lines allow us to compute the cumulative probabilities\n  group_by(draw, facet, jewish) %&gt;% \n  arrange(draw, facet, jewish, response) %&gt;% \n  mutate(probability = cumsum(pk)) %&gt;% \n  ungroup() %&gt;% \n  # these next three lines are how we randomly selected 50 posterior draws\n  nest(data = -draw) %&gt;% \n  slice_sample(n = 100) %&gt;%\n  unnest(data) %&gt;% \n  mutate(group = paste0(draw, response))\n  \n# plot!\nlibrary(ggthemes)\nggplot(data = plot_df, aes(x = jewish, y = probability)) +\ngeom_line(aes(group = as.character(interaction(draw, response)), color = probability),\n          alpha = 1/10) +\ngeom_point(data = df %&gt;%  # wrangle the original data to make the dots\n             mutate(jewish = jewish %&gt;% factor(., levels = c(\"NotJewish\", \"Jewish\"))) %&gt;%\n             group_by(jewish) %&gt;%\n             count(familiarity) %&gt;%\n             mutate(probability = cumsum(n / sum(n)),\n                    facet = factor(str_c(\"jewish=\", jewish))),\n           color = canva_pal(\"Green fields\")(4)[2]) +\nscale_color_gradient(low = canva_pal(\"Green fields\")(4)[4],\n                     high = canva_pal(\"Green fields\")(4)[1]) +\nxlab(\"\") + ylab(\"Probability\") + \ntheme(legend.position = \"none\") \n\n\n\n\n\n\n\n\nThis graph looks a little complicated. The black dots in the graph are the cumulative probability values calculated directly form the data and the lines represent the models expectations, with the variability of the lines indicating error. Focusing on the Not Jewish (left) column of the graph, the lowest dot and line is at around 0.62, meaning the data and model expect around 62% of non-Jewish people to have scored a 1 on the likert scale (Never heard it), for Jewish people, the equivalent proportion is 1%.\nSo how can we conclude that Jewish participants are more familiar with these words that non-Jewish participants, with confidence."
  },
  {
    "objectID": "posts/2023-11-09-understanding-ordinal-models/index.en.html#hypothesis-2-are-non-jewish-australians-less-familiar-with-these-words-that-non-jewish-americans",
    "href": "posts/2023-11-09-understanding-ordinal-models/index.en.html#hypothesis-2-are-non-jewish-australians-less-familiar-with-these-words-that-non-jewish-americans",
    "title": "Understanding Ordinal models",
    "section": "Hypothesis 2: Are non-Jewish Australians less familiar with these words that Non-Jewish Americans?",
    "text": "Hypothesis 2: Are non-Jewish Australians less familiar with these words that Non-Jewish Americans?\nWe have previously modeled a main effect of being Jewish and it‚Äôs relationship to Yiddish word familiarity. Now we want to see if there is a national effect - being in Australia, or in America. Specifically, the interest is in whether non-Jewish people from America, where Jewish culture is widely prominent, are more familiar with Yiddish words than non-Jewish Australians. We also want to do this, while keeping the Jewish effect in the model.\nSince the baseline level of the previous model is being Jewish, adding a predictor for country will tell us whether non-Jewish Australians or non-Jewish Americans are different or not, and we don‚Äôt need to worry about fancy interactions.\n\nb3 = \n  brm(data = df, \n      family = cumulative,\n      familiarity ~ jewish + country,\n      prior = c(prior(normal(0, 1.5), class = Intercept),\n                prior(normal(0, 0.5), class = b)),\n      iter = 2000, warmup = 1000, cores = 4, chains = 1,\n      init = list(inits), # here we add our start values \n      file = \"jewish_country\")  \n\nfixef(b3)\n\n                    Estimate  Est.Error        Q2.5      Q97.5\nIntercept[1]       0.2066124 0.07152018  0.07907362  0.3500715\nIntercept[2]       2.5282643 0.11378562  2.31421286  2.7484561\nIntercept[3]       4.6200212 0.13962022  4.36691231  4.9073730\njewishJewish       4.9376494 0.13818377  4.66458764  5.2260303\ncountryAustralian -0.5445326 0.08023831 -0.69960194 -0.3896266\n\n# as probabilities \nround(inv_logit_scaled(fixef(b3)[4:5,1]), 4)\n\n     jewishJewish countryAustralian \n           0.9929            0.3671 \n\n\nThis tells us that being in Australia decreases the probability of knowing a Jewish word, by approximately 63%.\nHowever, by adding a second categorical variable, we are changing the interpretation of the Jewishness effect. Now, the coefficient for Jewishness will indicate the expected value of a Jewish Australian, because the baseline level of the Country predictor is Australian. Below we see that the coefficient for Jewishness in the model has changed as a result.\n\n# Without Country as a predictor\nfixef(b2)[\"jewishJewish\",]\n\n Estimate Est.Error      Q2.5     Q97.5 \n5.0444483 0.1514008 4.7625205 5.3290368 \n\n# With Country as a predictor\nfixef(b3)[\"jewishJewish\",]\n\n Estimate Est.Error      Q2.5     Q97.5 \n4.9376494 0.1381838 4.6645876 5.2260303 \n\n\nSo, how can we know the main effect of Jewishness, and maintain the non-Jewish country comparison in the same model?\nDirectly testing this is impossible. Adding the second binary variable means that the reference level for the model will always mean an individual will be one of two categories, meaning we cannot retreieve a main effect of Jewishness. We could maintain the main effect of Jewishness by using contrast coding. This would mean the effect of Jewishness is the average effect across both countries, but it would now mean the coefficient for country is showing us the deviation from the overall mean, and not a comparison between country categories.\nInstead, to have both a main effect of Jewishness, and a comparison between the non-Jewish country categories, we will need to invoke post-hoc comparisons by estimating the marginal means of our effects.\nThis is easily achieved in R through the package emmeans, however, its application to Bayesian ordinal models is currently experimental. So, we must calculate the effects manually. They are, however, not experiemental in frequentist ordinal models, so we can compare the results to the frequentist results to make sure we are working things out correctly.\nFirst I quickly work out the results using the frequentist approach.\n\nlibrary(ordinal)\n\n# Run the ordinal model\ndf$familiarity = factor(df$familiarity)\nb3.1 = clm(familiarity ~ jewish + country,\n            data = df)\n\nclm_marginal = emmeans(b3.1, ~ familiarity | jewish, mode = \"prob\")\nclm_marginal\n\njewish = NotJewish:\n familiarity    prob       SE  df asymp.LCL asymp.UCL\n 1           0.63984 0.013800 Inf   0.61279    0.6669\n 2           0.31136 0.012900 Inf   0.28600    0.3367\n 3           0.04308 0.005030 Inf   0.03322    0.0529\n 4           0.00571 0.000859 Inf   0.00403    0.0074\n\njewish = Jewish:\n familiarity    prob       SE  df asymp.LCL asymp.UCL\n 1           0.01043 0.001520 Inf   0.00745    0.0134\n 2           0.09401 0.007800 Inf   0.07872    0.1093\n 3           0.40236 0.013900 Inf   0.37521    0.4295\n 4           0.49320 0.014400 Inf   0.46506    0.5213\n\nResults are averaged over the levels of: country \nConfidence level used: 0.95 \n\n\nThe code above tests the marginal mean of Jewishness, averaging over the levels of country, for each rating value. Because the model doesn‚Äôt account for variations in words, it also averages over the effect of these, but because the model doesn‚Äôt account for this difference, not because of the marginal mean calculation.\nHow is this calculated? Marginal means are built off averaging the results from a reference grid. We can look at the reference grid for our frequentist marginal means in the object we just created. I just show the first line for convenience.\n\nclm_marginal@linfct[1,]\n\n  NotJewish.American.1|2      Jewish.American.1|2 NotJewish.Australian.1|2 \n                     0.5                      0.0                      0.5 \n   Jewish.Australian.1|2   NotJewish.American.2|3      Jewish.American.2|3 \n                     0.0                      0.0                      0.0 \nNotJewish.Australian.2|3    Jewish.Australian.2|3   NotJewish.American.3|4 \n                     0.0                      0.0                      0.0 \n     Jewish.American.3|4 NotJewish.Australian.3|4    Jewish.Australian.3|4 \n                     0.0                      0.0                      0.0 \n\n\nWhat we see is every possible combination of factors in our model: Jewish or Not Jewish, Australian or American, and the three threshold parameters. This first line is going to give us the probability of a Not Jewish person providing a value less than 2 (i.e.¬†1 Never heard), average across the effect of country. We do this by putting a value into the cells we want to average over (all 1|2 cells that are Not Jewish) that is weighted equally across all categories. Since we have two categories (Australia and America), the value is 0.5, but if there were four categories, the value would be 0.25.\nWe can calculate the probability of each state that is coded greater than 1 by multiplying this vector with the appropriate coefficients, which are also held in the marginal means object. All states coded as 0, will obviously equate multiple to zero, but our two conditions coded as 0.5, will give us the weighted probability of that state. We can then simply sum these traits to get the average probability for Not Jewish participants choosing a value of less than 2.\n\nclm_marginal@bhat * clm_marginal@linfct[1,]\n\n  NotJewish.American.1|2      Jewish.American.1|2 NotJewish.Australian.1|2 \n               0.2979237                0.0000000                0.3419199 \n   Jewish.Australian.1|2   NotJewish.American.2|3      Jewish.American.2|3 \n               0.0000000                0.0000000                0.0000000 \nNotJewish.Australian.2|3    Jewish.Australian.2|3   NotJewish.American.3|4 \n               0.0000000                0.0000000                0.0000000 \n     Jewish.American.3|4 NotJewish.Australian.3|4    Jewish.Australian.3|4 \n               0.0000000                0.0000000                0.0000000 \n\nsum(clm_marginal@bhat * clm_marginal@linfct[1,])\n\n[1] 0.6398436\n\n\nThe 0.5 might be confusing, but it is just a faster way of getting the average. We could code all states we are interested in as 1, then divide the summed result by two to get the same result.\n\nas_one = ifelse(clm_marginal@linfct[1,] &gt; 0.1, 1, 0)\n\nsum(clm_marginal@bhat * as_one) / 2\n\n[1] 0.6398436\n\n\nA final point, is that because we only estimate three thresholds, but there are four categories, the fourth category is just the remainder of the first three. See below:\n\n# if we calculate the probability for level 4 as we do everything else we get a negative value\nsum(clm_marginal@bhat * clm_marginal@linfct[4,])\n\n[1] -0.9942876\n\n# But the remainder is what we a really after:\n1 + sum(clm_marginal@bhat * clm_marginal@linfct[4,])\n\n[1] 0.005712429\n\n\nTo get an understanding of the effect of being Jewish, averaged over country, we can take the ratios between the probabilities to understand how more or less likely an individual from each group is to know. word. For example, in response 2 (have heard, don‚Äôt understand) the probability for a Not Jewish person is 0.31, and the probability for a Jewish person is 0.07.\n\n1 - (0.07 / 0.31)\n\n[1] 0.7741935\n\n\nThe ratio of these values tells us that a Jewish Person is 77% less likely to have heard, but not understood a Yiddish word compared to a Non-Jewish person, averaged over their country of origin.\nLet‚Äôs do the same estimation, but with our Bayesian model.\n\nBayesian Marginal Means\nTo save, space I will use the reference grid created by emmeans. The associated coefficients are the sum of all relevant coeffcients to the column in the reference grid, transformed onto the inverse logit scale. There might be a clever way to calculate this, but I do it manually so we can see what is happening. The output is not exactly the same as the frequentist approach because the estimated coefficients are slightly different.\n\nreference_grid = clm_marginal@linfct\n\nbhat = c()\nbhat[1] = inv_logit_scaled(fixef(b3)[1])\nbhat[2] = inv_logit_scaled(fixef(b3)[1] - fixef(b3)[4])\nbhat[3] = inv_logit_scaled(fixef(b3)[1] - fixef(b3)[5])\nbhat[4] = inv_logit_scaled(fixef(b3)[1] - fixef(b3)[4] - fixef(b3)[5])\nbhat[5] = inv_logit_scaled(fixef(b3)[2])\nbhat[6] = inv_logit_scaled(fixef(b3)[2] - fixef(b3)[4])\nbhat[7] = inv_logit_scaled(fixef(b3)[2] - fixef(b3)[5])\nbhat[8] = inv_logit_scaled(fixef(b3)[2] - fixef(b3)[4] - fixef(b3)[5])\nbhat[9] = inv_logit_scaled(fixef(b3)[3])\nbhat[10] = inv_logit_scaled(fixef(b3)[3] - fixef(b3)[4])\nbhat[11] = inv_logit_scaled(fixef(b3)[3] - fixef(b3)[5]) \nbhat[12] = inv_logit_scaled(fixef(b3)[3] - fixef(b3)[4] - fixef(b3)[5]) \n\njewish_probabilities = apply(reference_grid, 1, function(x) sum(x * bhat))\njewish_probabilities[4] = 1 + jewish_probabilities[4]\njewish_probabilities[8] = 1 + jewish_probabilities[8]\n\njewish_probabilities\n\n[1] 0.615449133 0.325478986 0.051352086 0.007719795 0.011856009 0.096443127\n[7] 0.380569790 0.511131074\n\n\nThe benefit of an MCMC approach is that rather than just estimating probabilities, we can estimated their posterior distribution, providing a more intuitive understanding of the model estimate and error.\n\nposterior = as_draws_df(b3)\n\nbhat_posterior = matrix(NA, nrow = nrow(posterior), ncol = 12)\nbhat_posterior[,1] = inv_logit_scaled(posterior$`b_Intercept[1]`)\nbhat_posterior[,2] = inv_logit_scaled(posterior$`b_Intercept[1]` - posterior$b_jewishJewish)\nbhat_posterior[,3] = inv_logit_scaled(posterior$`b_Intercept[1]` - posterior$b_countryAustralian)\nbhat_posterior[,4] = inv_logit_scaled(posterior$`b_Intercept[1]` - posterior$b_jewishJewish - posterior$b_countryAustralian)\nbhat_posterior[,5] = inv_logit_scaled(posterior$`b_Intercept[2]`)\nbhat_posterior[,6] = inv_logit_scaled(posterior$`b_Intercept[2]` - posterior$b_jewishJewish)\nbhat_posterior[,7] = inv_logit_scaled(posterior$`b_Intercept[2]` - posterior$b_countryAustralian)\nbhat_posterior[,8] = inv_logit_scaled(posterior$`b_Intercept[2]` - posterior$b_jewishJewish - posterior$b_countryAustralian)\nbhat_posterior[,9] = inv_logit_scaled(posterior$`b_Intercept[3]`)\nbhat_posterior[,10] = inv_logit_scaled(posterior$`b_Intercept[3]` - posterior$b_jewishJewish)\nbhat_posterior[,11] = inv_logit_scaled(posterior$`b_Intercept[3]` - posterior$b_countryAustralian) \nbhat_posterior[,12] = inv_logit_scaled(posterior$`b_Intercept[3]` - posterior$b_jewishJewish - posterior$b_countryAustralian) \n\nposterior_probabilities = apply(reference_grid, 1, \n                                function(x) apply(bhat_posterior, 1, \n                                                  function(y) sum(x * y)))\nposterior_probabilities[,4] = 1 + posterior_probabilities[,4]\nposterior_probabilities[,8] = 1 + posterior_probabilities[,8]\nposterior_probabilities = data.frame(posterior_probabilities)\ndimnames(posterior_probabilities) = list(1:nrow(posterior), \n                                         c(paste0(1:4, \"N-J\"), paste0(1:4, \"J\")))\n\nposterior_long = pivot_longer(posterior_probabilities, cols = everything())\n\nmcmc_areas(posterior_probabilities)"
  },
  {
    "objectID": "posts/2022-11-29-the-global-jukebox/index.html",
    "href": "posts/2022-11-29-the-global-jukebox/index.html",
    "title": "The Global Jukebox",
    "section": "",
    "text": "Recently, I was a co-author on the release paper for the Global Jukebox dataset (GJB). The GJB is a collection of 5,000+ songs from over 1,000 societies - all available to listen to only at theglobaljukebox.org. The published article is available at PLOS One.\nAlongside the audio recordings, the GJB contains a number of expertly coded datasets containing information on the musical style, coordination of musicians (between vocalists, between instrumentalists and the relationship between the vocalists and instrumentalists), the breathing patterns in songs, the vowel and consonant patterns in singing, as well as some bibliographic datasets on instruments and musical ensembles. My role was to organise and clean these datasets into useful formats for future users - which I did utilizing the common linguistic dataset format CLDF. All the datasets can be found saved on Github at the address www.github.com/theglobaljukebox/. Each dataset is also versioned on Zenodo which is linked within each repository.\nSince music is so prevalent in everyday life, there was great interest in the global variance of music that the GJB openly provides. This page documents some of the media interest for this papers release.\nPress releases:\n\n‰ºùÁµ±Ëä∏ËÉΩ„Éá„Éº„Çø„Éô„Éº„Çπ„ÄåGlobal Jukebox„Äç„ÅÆÂÖ¨ÈñãÔºç1,026Ê∞ëÊóè„Çí‰ª£Ë°®„Åô„Çã„ÄÅ5,776‰ª∂„ÅÆÈü≥Â£∞Ë®òÈå≤„Åå„Ç§„É≥„Çø„É©„ÇØ„ÉÜ„Ç£„Éñ„Å´Âà©Áî®ÂèØËÉΩÔºç\n‚ÄúGlobal Jukebox‚Äù Performing Arts Database Now Publicly Available: Data Made Available for Interactive Collection of 5,776 Recordings, Representing 1,026 Societies\n\n\n\nThe Global Jukebox was well received in Japan receiving 354.8k views, 27.9k likes and 12.4k retweets on twitter after a review of the site was posted by Japanese musician Ry≈´ichir≈ç Sakan."
  },
  {
    "objectID": "posts/2022-11-29-the-global-jukebox/index.html#famous-in-japan",
    "href": "posts/2022-11-29-the-global-jukebox/index.html#famous-in-japan",
    "title": "The Global Jukebox",
    "section": "",
    "text": "The Global Jukebox was well received in Japan receiving 354.8k views, 27.9k likes and 12.4k retweets on twitter after a review of the site was posted by Japanese musician Ry≈´ichir≈ç Sakan."
  },
  {
    "objectID": "posts/2023-04-26-abc-canberra-language-diversity-and-endangerment/index.en.html",
    "href": "posts/2023-04-26-abc-canberra-language-diversity-and-endangerment/index.en.html",
    "title": "ABC Canberra: Language Diversity and Endangerment",
    "section": "",
    "text": "On Monday, I gave a short interview on the ABC Canberra Drive show, with Anna Vidot, to talk about language diversity and endangerment on the back of the release of Grambank.\nYou can listen to the interview here, starting around 50 minutes.\nThis was my first radio interview, and because I was nervous, I didn‚Äôt answer all the questions as well I would have liked. In particular, I blanked on the question ‚ÄúWhy should we save languages‚Äù? So, here is my short second attempt at an answer:\n\nLanguage is the vehicle for a much broader concept of culture. By revitalizing and encouraging linguistic diversity in the world, we are also creating space for diversity in cultural practice. Creating space for cultural diversity builds an environment where individuals are free to express themselves and begins to repeal the oppressive impacts of colonialism.\n\n\nTaking a purely scientific point of view, language diversity allows us to explore the impact of social and cultural contexts on cognitive diversity. Our current knowledge of linguistic diversity, and its impact on psychology, is incredibly limited, and the rate of which languages are no longer being spoken is decreasing faster than we are learning about them. Shifting our efforts towards revitalizing languages and creating space for languages to support themselves creates more time for the scientific enquiry of diversity, but might also encourage those who come from non-western psychological background to help us further our understanding of human cognition.\n\nSome other notable media coverage of the Grambank release\n\nThe Conversation: Research on 2,400 languages shows nearly half the world‚Äôs language diversity is at risk.\nPhys.org: Database shows the diversity of the world‚Äôs languages\nScienmag: Grambank shows the diversity of the world‚Äôs languages\nEurekAlert!: World‚Äôs largest grammar database reveals accelerating loss of language diversity\n\nA complete collection is available from Altmetrics."
  },
  {
    "objectID": "posts/2023-05-16-dap-workshop/index.en.html",
    "href": "posts/2023-05-16-dap-workshop/index.en.html",
    "title": "Data Analysis Plan (DAP) Workshop 2023",
    "section": "",
    "text": "Recently I assisted running the Statistics Bootcamp at the Australian National University. The workshop was organised by the Researcher Development Team and led by A/Prof Alice Richardson of the Statistical Support Network.\nThe bootcamp was an intense 3-day period of learning and doing for the HDR participants, from 10 ‚Äì 12 May 2023. This was the only the second time for a statistically-focused bootcamp, and it attracted over fifty expressions of interest amongst the PhD cohort across ANU.\nThe bootcamp consists of a combination of short lectures and extended writing periods. The goal of these combinations is to encourage graduate students to think about the statistical implications of their proposed research projects before they begin analysis. For example, what is the research question? What data will you use? What is the main exposure or outcome? What are the confounders? How will you deal with missing data? By encouraging students to write a data analysis plan early, we can establish the amount of work needed early on in the research program, and hopeful avoid any design problems early in the process (rather than explaining that statitics aren‚Äôt magic near the hand in date).\nThe data analysis plan template can be found at this address: https://docs.google.com/document/d/1Lmchg6JHhSOCMgq3ulcyHIox80LPMWFLbRgNtgEjXi4/edit\nThe schedule and materials for the 2023 Bootcamp can be found at this address: https://docs.google.com/document/d/1Rz6aYXV7eXfAdfa01GXuRpTJT4peBLB2/edit"
  },
  {
    "objectID": "posts/2023-08-21-talks-in-shandong-and-korea/index.en.html",
    "href": "posts/2023-08-21-talks-in-shandong-and-korea/index.en.html",
    "title": "Talks in Shandong and Korea",
    "section": "",
    "text": "I recently gave two talks in Shandong, China and in Seoul, Korea (via Zoom).\nIn these talks I discussed research I had done with Patrick Savage on the Global Diversity of music. Both of these projects utilized data from the Global Jukebox, which you can interact with here, or read about here.\nAt these events I discussed the outcomes of two projects. First, can we understand the extent of global musical diversity by examining songs and musical styles that appear to be outliers. This project tries to combine a qualitative understanding of outliers, with a systematic understanding of why musical styles might become very different from their neighbors. This project has just been published in the Journal of Cognition, and I will write more about it shortly.\nThe second project similarities in musical style can tell us anything about the cultural histories of human society. This project finds that there is some autocorrelative features in music, but they do not seem to bare any resemblance to patterns of linguistic or genetic history. This raises the question of how does music evolve over time, and what can it tell us about the complexities of human history. This project is still under review, but you can see a pre-print here.\nMusic, Math, and Language Workshop jointly hosted by Chosun University and Korea Institute for Advanced Study (Seoul, Korea)\nBelow is a poster for my talk for the International Academic Communication Month on Computational Analysis of World Traditional Music, hosted by the Shandong College of Arts (Shandong, China).."
  },
  {
    "objectID": "posts/2024-08-13-review-tracking/index.en.html",
    "href": "posts/2024-08-13-review-tracking/index.en.html",
    "title": "Review Tracking",
    "section": "",
    "text": "In this post I keep track of Journals I have reviewed for, and how often. I‚Äôve started this a bit late, so there are probably some reviews that I have forgotten about, but I‚Äôll try to keep it up to date!\n\n\n\nJournal\nPapers\n\n\n\n\nNature Human Behaviour\n1\n\n\nNature Communications\n1\n\n\nPNAS\n1 (as guest editor)\n\n\nAmerican Anthropologist\n1\n\n\nLanguage and Cognition\n1\n\n\nEvolutionary Human Science\n2\n\n\nReligion, Brain, and Behaviour\n1\n\n\nFrontiers Psychology\n2\n\n\nHumanities and Social Sciences Communications\n1\n\n\nOpen Mind\n1\n\n\nCogSci (Conference)\n1\n\n\nLingua\n1\n\n\nOxford Handbooks\n1\n\n\nRoutledge (Book Proposal)\n1\n\n\nAnthropos (Book Reviews)\n1\n\n\nJournal of the Royal Society Interface\n1\n\n\nBiology and Philosophy\n1"
  },
  {
    "objectID": "posts/cultured-scene.html",
    "href": "posts/cultured-scene.html",
    "title": "Cultured Scene",
    "section": "",
    "text": "I wrote a piece for the December issue of Cultured Scene! It is a short(ish) piece on why kinship is interesting, why we need to take a quantitative approach to studying it, and what I have done so far in my PhD.\nCheck it out here"
  },
  {
    "objectID": "posts/galtons-problem.html",
    "href": "posts/galtons-problem.html",
    "title": "The problem with Galton‚Äôs problem",
    "section": "",
    "text": "The Problem with Galton‚Äôs Problem\nWe need to talk about ‚ÄúGalton‚Äôs Problem‚Äù.\nThat is, the statistical non-independence of cultural data points due to a common source - famously identified by Francis Galton during his critique of Edward Tylor‚Äôs analysis of marriage practices in 1888. This post is not about the importance of this idea when analysing cross-cultural data. The consideration and explicit analysis on non-independence has advanced the quantitative analysis of cultural data considerably, revealing many interesting discoveries about our past. This post is a call to reconsider what we name this phenomenon.\nThe last few years have highlighted how often society places people with disreputable ideas in positions of admiration (although this problem has a much longer history). The naming of scientific observations after notable scientists who higlight an idea is no exception to this. Francis Galton is widely considered the father of eugenics and actively petitioned for the selective breeding of people within the UK. There are few better examples of disreputable ideas than this. Throughout the academic world there is a reconsideration of how we should relate to historical figures - UCL commendably renamed lecture theaters after reassessing their relationship with their previously esteemed alumni.\nThe argument often made for maintaining the names of people like Galton in prominent positions is that we are admiring their scientific discoveries and not their politics. This is a complicated argument to take up and a bigger debate than I hope to address here. But if our goal is championing the importance of science and communicating great ideas of the past, naming them after leading figures is an awkward and opaque route to take. To take ‚ÄúGalton‚Äôs problem‚Äù at face value gives no indication to what the problem is, and given his eugenic beliefs, has quite sinister undertones.\nStatistics is notorious for attributing ideas to the discoverer: Gaussian-distribution, Pearson‚Äôs correlation, Student‚Äôs T-tests (which have their own interesting history). However more recently, statistical discoveries are descriptively named: Linear regression, Neural networks, Classification Trees. Descriptive names offer transparency and are historically unproblematic, and this is an approach we should use retrospectively.\nI am not writing this to admonish people who have used this term. In part, this is a letter to myself, having spent much of my academic career thinking about the implications of interdependencies in cultural data and writing using the given terminology. There is a need in both my home fields of statistics and anthropology to reassess the use of nomenclature and to be more considerate towards those who might be ostracized by the honorifics used towards people like Galton.\nMoving forward, I will describe this phenomenon as forms of autocorrelation. Autocorrelation is the statistical term to describe the relationship between data points within a variable. While typically this refers more specifically to series autocorrelation (the correlation of a signal with a delayed copy of itself which is caused by the delay), prefixing autocorrelation with the acting process is a descriptive and transparent way to describe the type of autocorrelation at hand. For patterns of shared ancestry: phylogenetic autocorrelation; when describing geographical relationships: spatial autocorrelation.\nAs an added bonus, the use of this terminology improves interdisciplinarity by using terminology in line with other fields concerned with autocorrelation, such as biology or epidemiology. Some more technically-minded people might argue that autocovariance is a more appropriate term, and this is a debate I am happy to have.\nStatistics and Cross-cultural research has a sordid history with racism. There is much more history to address, many more ideas to rethink, and much more work to do. But this change is a small and achievable step in the right direction."
  },
  {
    "objectID": "posts/2022-07-15-phylogenetic-signal-in-inla/index.html",
    "href": "posts/2022-07-15-phylogenetic-signal-in-inla/index.html",
    "title": "Phylogenetic Signal in INLA",
    "section": "",
    "text": "Recently, my research has been focused on quantifying the patterns of similarity we observe within a particular linguistic or cultural trait, also known as autocorrelation. In anthropology some people call this ‚ÄòGalton‚Äôs Problem‚Äô but I have outlined why I prefer the term autocorrelation here. A helpful way to remember what autocorrelation means is that the auto is used to mean self (as it is in autobiography). So autocorrelation really just means self-correlation.\nFor the most part, I have been using Gaussian Process models to quantify autocorrelational effects of phylogenies of linguistic and cultural data, and have been building upwards from the BRMS phylogenetic multilevel model example provided by Paul B√ºrkner. This example shows us how to estimate the amount of phylogenetic signal, specifically Pagel‚Äôs Lambda, within a Bayesian multilevel model. Paul offers a number of different situations, but I am particularly interested in the simple and repeated-measures model.\nBRMS is an excellent package for building these models, but an alternative I have started using is INLA, under the guidance of Russell Dinnage. To ensure I understand the dynamics of this new modelling environment, Russell helped me replicated Paul B√ºrkner‚Äôs example in INLA.\nThe goal here is to replicate Paul‚Äôs analyses within the INLA framework. Here is the introduction from Paul‚Äôs vignette:\n\nIn the present vignette, we want to discuss how to specify phylogenetic multilevel models using brms. These models are relevant in evolutionary biology when data of many species are analyzed at the same time. The usual approach would be to model species as a grouping factor in a multilevel model and estimate varying intercepts (and possibly also varying slopes) over species. However, species are not independent as they come from the same phylogenetic tree and we thus have to adjust our model to incorporate this dependency. The examples discussed here are from chapter 11 of the book Modern Phylogenetic Comparative Methods and the application in Evolutionary Biology (de Villemeruil & Nakagawa, 2014). The necessary data can be downloaded from the corresponding website (https://www.mpcm-evolution.com/). Some of these models may take a few minutes to fit.\n\n\n\n\n\nAssume we have measurements of a phenotype, phen (say the body size), and a cofactor variable (say the temperature of the environment). We prepare the data using the following code.\n\nWe are going to use the same data as Paul, so we can compare outcomes:\n\nlibrary(INLA)\nlibrary(brms)\nlibrary(ape)\nlibrary(ggplot2)\nlibrary(tidyr)\n\nphylo &lt;- ape::read.nexus(\"https://paul-buerkner.github.io/data/phylo.nex\")\ndata_simple &lt;- read.table(\n  \"https://paul-buerkner.github.io/data/data_simple.txt\",\n  header = TRUE\n)\n\nTo model the relationships between societies BRMS requires a covariance matrix, but INLA uses precision matrices. So, where BRMS uses ape::vcv.phylo to build a covariance matrix, we instead used MCMCglmm::inverseA to get a precision matrix of the phylogeny.\nINLA also requires that we identify the relationship between the precision matrix and data frame using numerical indices, rather than character names, as in BRMS. To get the indices you can use the match function.\n\n# INLA Precision matrix\nphylo_prec_mat = MCMCglmm::inverseA(phylo,\n                                   nodes = \"TIPS\",\n                                   scale = TRUE)$Ainv\n\n# matching with numerical indicies\ndata_simple$phylo_id = match(data_simple$phylo, rownames(phylo_prec_mat))\n\n# BRMS Covariance matrix\nA &lt;- ape::vcv.phylo(phylo)\n\nWith the precision matrix built and data lined up we can build the model using the default prior settings for INLA. Beneath the INLA model, I show the BRMS code for comparison.\n\n# INLA model\nmodel_simple_inla = inla(phen ~ cofactor +\n                    f(phylo_id,\n                      model = \"generic0\",\n                      Cmatrix = phylo_prec_mat),\n                  family = \"gaussian\",\n                  data = data_simple)\n\n# BRMS Model\nmodel_simple_brms &lt;- brm(\n  phen ~ cofactor + (1|gr(phylo, cov = A)),\n  data = data_simple,\n  family = gaussian(),\n  data2 = list(A = A),\n  prior = c(\n    prior(normal(0, 10), \"b\"),\n    prior(normal(0, 50), \"Intercept\"),\n    prior(student_t(3, 0, 20), \"sd\"),\n    prior(student_t(3, 0, 20), \"sigma\")\n  )\n)\n\nINLA and BRMS have similar model grammars, but there are important differences. Both approaches use the standard model formula for response and fixed effects: y ~ x. The primary difference in model grammar between BRMS and INLA comes with the specification of random effects, so it worth spending some time comparing the two. INLA specifies random effects using f, which is short for function. f has three inputs in the above model. First is the numerical indices variable indicating which taxa is assigned to which row of the input data. By offering a single categorical variable to f, we are building a model with random intercepts, mimicing the BRMS model. Next is the model argument. There are 50 model possibilities which define how the precision estimates are calculated (see names(inla.models()$latent) for all models). For the replication of the BRMS example, we use the generic0 model. generic0 which applies one parameter $\\tau$ to the precision matrix and is the model I have used most for autocorrelational models. Although it is worthwhile to look through the list of possibilities to see if any other model might better suit your purposes. The final argument in f is Cmatrix, which is where the precision matrix goes.\nAfter looking at f we can look at the remaining arguments to the inla function: family, and data. family works like many other R statistical modelling packages, and specifies the distribution you want to use for your model. Here, we are using gaussian, but INLA can handle most common distributions, like binomial or poisson. data should again be familiar, this takes the data to be used in the model. data works much like many other modelling inputs, and is where the fixed effect data comes from for your model.\n\nsummary(model_simple_inla)\n\nTime used:\n    Pre = 2.07, Running = 0.63, Post = 0.0492, Total = 2.75 \nFixed effects:\n              mean    sd 0.025quant 0.5quant 0.975quant   mode kld\n(Intercept) 39.766 6.860     26.273   39.741     53.381 39.739   0\ncofactor     5.178 0.137      4.911    5.178      5.446  5.178   0\n\nRandom effects:\n  Name    Model\n    phylo_id Generic0 model\n\nModel hyperparameters:\n                                         mean    sd 0.025quant 0.5quant\nPrecision for the Gaussian observations 0.012 0.002      0.009    0.012\nPrecision for phylo_id                  0.006 0.002      0.003    0.005\n                                        0.975quant  mode\nPrecision for the Gaussian observations      0.016 0.012\nPrecision for phylo_id                       0.010 0.005\n\nMarginal log-Likelihood:  -1008.86 \n is computed \nPosterior summaries for the linear predictor and the fitted values are computed\n(Posterior marginals needs also 'control.compute=list(return.marginals.predictor=TRUE)')\n\n\nINLA prints two main areas of output we are interested in: Fixed effects (titled Populations-Level Effects in BRMS output) and Model hyperparameters (Group-Level Effects). The hyperparameters for random effects in INLA are precision estimates, as opposed to variance estimates in BRMS. So, they are not directly comparable. Fortunately, precision estimates are easily converted to the more intuitive variance estimates using the formula \\(\\sqrt(1 / \\tau)\\). Pagel‚Äôs Lambda is simply calcualted by B√ºrkner as (variance explained by phylogeny / total variation). Let‚Äôs compare the Pagel‚Äôs Lambda estimates for the INLA and BRMS models.\nAfter this conversion, we see that INLA estimates a slightly smaller phylogenetic effect than BRMS does.\n\n# INLA\nposterior_sample = inla.hyperpar.sample(4000, model_simple_inla) # 4000 samples to match BRMS default\nlambda = (1 / posterior_sample[,2]) / (rowSums(1 / posterior_sample))\nsummary(lambda)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.3432  0.6203  0.6882  0.6806  0.7485  0.8977 \n\n# BRMS output\nhyp &lt;- \"sd_phylo__Intercept^2 / (sd_phylo__Intercept^2 + sigma^2) = 0\"\nhyp &lt;- hypothesis(model_simple_brms, hyp, class = NULL)\nsummary(hyp$samples[,1])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.3493  0.6496  0.7121  0.7028  0.7629  0.9008 \n\nplot_df = data.frame(inla = lambda,\n                     brms = c(hyp$samples[,1]),\n                     iter = 1:4000)\n\nggplot() + \n  geom_density(data = plot_df, aes(inla), fill = \"red\", alpha = 0.3) + \n  geom_density(data = plot_df, aes(brms), fill = \"blue\", alpha = 0.3) +\n  xlab(\"Phylogenetic signal\")\n\n\n\n\n\n\n\n\nThe INLA model performs quite well using default priors, with similar estimates, but slightly higher error. One of the more attractive aspects of INLA is the introduction of penalizing complexity priors. The basic idea behind penalizing complexity priors is that we should prefer a simpler model, until there is enough support for a more complex model. A change in prior results in a very small difference between the INLA and BRMS estimates for phylogenetic signal.\n\npcprior = list(prec = list(\n  prior=\"pc.prec\",\n  param = c(20, 0.1)) \n)\n\nmodel_pcprior_inla = inla(phen ~ cofactor +\n                    f(phylo_id,\n                      model = \"generic0\",\n                      Cmatrix = phylo_prec_mat,\n                      hyper = pcprior),\n                  data = data_simple,\n                  control.compute=list(config = TRUE))\n\n# INLA\nposterior_sample = inla.hyperpar.sample(4000, model_pcprior_inla) # 4000 samples to match BRMS default\nlambda.pc = (1 / posterior_sample[,2]) / (rowSums(1 / posterior_sample))\n\nplot_df$inla.pc = lambda.pc\n\nsummary(plot_df[,c(1:2,4)])\n\n      inla             brms           inla.pc      \n Min.   :0.3432   Min.   :0.3493   Min.   :0.3431  \n 1st Qu.:0.6203   1st Qu.:0.6496   1st Qu.:0.6427  \n Median :0.6882   Median :0.7121   Median :0.7068  \n Mean   :0.6806   Mean   :0.7028   Mean   :0.6977  \n 3rd Qu.:0.7485   3rd Qu.:0.7629   3rd Qu.:0.7623  \n Max.   :0.8977   Max.   :0.9008   Max.   :0.9031  \n\nggplot() + \n  geom_density(data = plot_df, aes(inla), fill = \"red\", alpha = 0.3) + \n  geom_density(data = plot_df, aes(inla.pc), fill = \"yellow\", alpha = 0.3) +\n  geom_density(data = plot_df, aes(brms), fill = \"blue\", alpha = 0.3) +\n  xlab(\"Phylogenetic signal\")\n\n\n\n\n\n\n\n\n\n\nB√ºrkner also presents an example for modelling phylogenetic signal when there are multiple samples per taxa in the phylogeny. Again I quote the original article, and show the INLA formulation alongside the BRMS formulation. The organising of the precision matrix for this model is much the same as above, so I will brush over this part here.\n\nOften, we have multiple observations per species and this allows to fit more complicated phylogenetic models.\n\n\ndata_repeat &lt;- read.table(\n  \"https://paul-buerkner.github.io/data/data_repeat.txt\",\n  header = TRUE\n)\n\ndata_repeat$spec_mean_cf &lt;-\n  with(data_repeat, sapply(split(cofactor, phylo), mean)[phylo])\nhead(data_repeat)\n\n       phen  cofactor species phylo spec_mean_cf\n1 107.41919 11.223724    sp_1  sp_1    10.309588\n2 109.16403  9.805934    sp_1  sp_1    10.309588\n3  91.88672 10.308423    sp_1  sp_1    10.309588\n4 121.54341  8.355349    sp_1  sp_1    10.309588\n5 105.31638 11.854510    sp_1  sp_1    10.309588\n6  64.99859  4.314015    sp_2  sp_2     3.673914\n\n# recreate phylo id variable\ndata_repeat$phylo_id_rep &lt;- match(data_repeat$phylo, rownames(phylo_prec_mat))\ndata_repeat$species_id = data_repeat$phylo_id_rep\n\n\nThe variable spec_mean_cf just contains the mean of the cofactor for each species. The code for the repeated measurement phylogenetic model looks as follows:\n\n\nThe variables phylo and species are identical as they are both identifiers of the species. However, we model the phylogenetic covariance only for phylo and thus the species variable accounts for any specific effect that would be independent of the phylogenetic relationship between species (e.g., environmental or niche effects). Again we can obtain model summaries as well as estimates of the phylogenetic signal.\n\n\n# INLA\nmodel_inla_repeat1 = inla(\n  phen ~ spec_mean_cf + \n    f(phylo_id_rep,\n      model = \"generic0\",\n      Cmatrix = phylo_prec_mat,\n      hyper = pcprior) + # include pc.prior\n    f(species,\n      model = \"iid\"),\n  data = data_repeat\n)\n\n# BRMS\nmodel_brms_repeat1 &lt;- brm(\n  phen ~ spec_mean_cf + (1|gr(phylo, cov = A)) + (1|species),\n  data = data_repeat,\n  family = gaussian(),\n  data2 = list(A = A),\n  prior = c(\n    prior(normal(0,10), \"b\"),\n    prior(normal(0,50), \"Intercept\"),\n    prior(student_t(3,0,20), \"sd\"),\n    prior(student_t(3,0,20), \"sigma\")\n  ),\n  sample_prior = TRUE, chains = 2, cores = 2,\n  iter = 4000, warmup = 1000\n)\n\n\n# INLA\nposterior_sample = inla.hyperpar.sample(6000, model_inla_repeat1) # 6000 samples to match BRMS default\nlambda = (1 / posterior_sample[,2]) / (rowSums(1 / posterior_sample))\nsummary(lambda)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.4563  0.7220  0.7646  0.7573  0.7997  0.9094 \n\n# BRMS output\nhyp &lt;- paste(\"sd_phylo__Intercept^2 /\", \"(sd_phylo__Intercept^2 + sd_species__Intercept^2 + sigma^2) = 0\")\nhyp &lt;- hypothesis(model_brms_repeat1, hyp, class = NULL)\nsummary(hyp$samples[,1])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.4472  0.7087  0.7475  0.7429  0.7832  0.8925 \n\nplot_df = data.frame(inla = lambda,\n                     brms = c(hyp$samples[,1]))\n\nsummary(plot_df)\n\n      inla             brms       \n Min.   :0.4563   Min.   :0.4472  \n 1st Qu.:0.7220   1st Qu.:0.7087  \n Median :0.7646   Median :0.7475  \n Mean   :0.7573   Mean   :0.7429  \n 3rd Qu.:0.7997   3rd Qu.:0.7832  \n Max.   :0.9094   Max.   :0.8925  \n\nggplot() + \n  geom_density(data = plot_df, aes(inla), fill = \"red\", alpha = 0.3) + \n  geom_density(data = plot_df, aes(brms), fill = \"blue\", alpha = 0.3) +\n  xlab(\"Phylogenetic signal\")"
  },
  {
    "objectID": "posts/2022-07-15-phylogenetic-signal-in-inla/index.html#a-phylogenetic-model-with-repeated-measurements",
    "href": "posts/2022-07-15-phylogenetic-signal-in-inla/index.html#a-phylogenetic-model-with-repeated-measurements",
    "title": "Phylogenetic Signal in INLA",
    "section": "",
    "text": "B√ºrkner also presents an example for modelling phylogenetic signal when there are multiple samples per taxa in the phylogeny. Again I quote the original article, and show the INLA formulation alongside the BRMS formulation. The organising of the precision matrix for this model is much the same as above, so I will brush over this part here.\n\nOften, we have multiple observations per species and this allows to fit more complicated phylogenetic models.\n\n\ndata_repeat &lt;- read.table(\n  \"https://paul-buerkner.github.io/data/data_repeat.txt\",\n  header = TRUE\n)\n\ndata_repeat$spec_mean_cf &lt;-\n  with(data_repeat, sapply(split(cofactor, phylo), mean)[phylo])\nhead(data_repeat)\n\n       phen  cofactor species phylo spec_mean_cf\n1 107.41919 11.223724    sp_1  sp_1    10.309588\n2 109.16403  9.805934    sp_1  sp_1    10.309588\n3  91.88672 10.308423    sp_1  sp_1    10.309588\n4 121.54341  8.355349    sp_1  sp_1    10.309588\n5 105.31638 11.854510    sp_1  sp_1    10.309588\n6  64.99859  4.314015    sp_2  sp_2     3.673914\n\n# recreate phylo id variable\ndata_repeat$phylo_id_rep &lt;- match(data_repeat$phylo, rownames(phylo_prec_mat))\ndata_repeat$species_id = data_repeat$phylo_id_rep\n\n\nThe variable spec_mean_cf just contains the mean of the cofactor for each species. The code for the repeated measurement phylogenetic model looks as follows:\n\n\nThe variables phylo and species are identical as they are both identifiers of the species. However, we model the phylogenetic covariance only for phylo and thus the species variable accounts for any specific effect that would be independent of the phylogenetic relationship between species (e.g., environmental or niche effects). Again we can obtain model summaries as well as estimates of the phylogenetic signal.\n\n\n# INLA\nmodel_inla_repeat1 = inla(\n  phen ~ spec_mean_cf + \n    f(phylo_id_rep,\n      model = \"generic0\",\n      Cmatrix = phylo_prec_mat,\n      hyper = pcprior) + # include pc.prior\n    f(species,\n      model = \"iid\"),\n  data = data_repeat\n)\n\n# BRMS\nmodel_brms_repeat1 &lt;- brm(\n  phen ~ spec_mean_cf + (1|gr(phylo, cov = A)) + (1|species),\n  data = data_repeat,\n  family = gaussian(),\n  data2 = list(A = A),\n  prior = c(\n    prior(normal(0,10), \"b\"),\n    prior(normal(0,50), \"Intercept\"),\n    prior(student_t(3,0,20), \"sd\"),\n    prior(student_t(3,0,20), \"sigma\")\n  ),\n  sample_prior = TRUE, chains = 2, cores = 2,\n  iter = 4000, warmup = 1000\n)\n\n\n# INLA\nposterior_sample = inla.hyperpar.sample(6000, model_inla_repeat1) # 6000 samples to match BRMS default\nlambda = (1 / posterior_sample[,2]) / (rowSums(1 / posterior_sample))\nsummary(lambda)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.4563  0.7220  0.7646  0.7573  0.7997  0.9094 \n\n# BRMS output\nhyp &lt;- paste(\"sd_phylo__Intercept^2 /\", \"(sd_phylo__Intercept^2 + sd_species__Intercept^2 + sigma^2) = 0\")\nhyp &lt;- hypothesis(model_brms_repeat1, hyp, class = NULL)\nsummary(hyp$samples[,1])\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.4472  0.7087  0.7475  0.7429  0.7832  0.8925 \n\nplot_df = data.frame(inla = lambda,\n                     brms = c(hyp$samples[,1]))\n\nsummary(plot_df)\n\n      inla             brms       \n Min.   :0.4563   Min.   :0.4472  \n 1st Qu.:0.7220   1st Qu.:0.7087  \n Median :0.7646   Median :0.7475  \n Mean   :0.7573   Mean   :0.7429  \n 3rd Qu.:0.7997   3rd Qu.:0.7832  \n Max.   :0.9094   Max.   :0.8925  \n\nggplot() + \n  geom_density(data = plot_df, aes(inla), fill = \"red\", alpha = 0.3) + \n  geom_density(data = plot_df, aes(brms), fill = \"blue\", alpha = 0.3) +\n  xlab(\"Phylogenetic signal\")"
  },
  {
    "objectID": "posts/2022-10-11-funding-in-australia-and-new-zealand/index.html#rutherford-foundation-postdoctorl-fellowship",
    "href": "posts/2022-10-11-funding-in-australia-and-new-zealand/index.html#rutherford-foundation-postdoctorl-fellowship",
    "title": "Funding in Australia and New Zealand",
    "section": "Rutherford Foundation Postdoctorl Fellowship",
    "text": "Rutherford Foundation Postdoctorl Fellowship\n\nUp to four years after your is PhD conferred\nI can apply up to 2024"
  },
  {
    "objectID": "posts/2025-04-02-benefords-law/index.html",
    "href": "posts/2025-04-02-benefords-law/index.html",
    "title": "Beneford‚Äôs Law",
    "section": "",
    "text": "Testing"
  },
  {
    "objectID": "presentations/2025-presentationtest/index.html",
    "href": "presentations/2025-presentationtest/index.html",
    "title": "Introduction to Infectious Disease Modeling",
    "section": "",
    "text": "Preparation\n\nInstall R and (optional) Rstudio.\nInstall the DSAIDE R package and take a brief look. This website provides instructions.\nOptional: Read this paper which provides background reading for the ‚ÄúID Control of multiple outbreaks‚Äù app in DSAIDE.\n\n\n\nOutline\nThe following topics are covered in this workshop:\n\nIntroduction to infectious disease modeling\nSome example models\nHow to use simulation models\nTypes of models\nSources of uncertainty\nHow to build (good) models\nHow to assess modeling studies\nActive learning of infectious disease epidemiology\n\n\n\nPresentation Slides\nAll pdf slides as zip file"
  },
  {
    "objectID": "projects/PacificCreoles/index.html",
    "href": "projects/PacificCreoles/index.html",
    "title": "Documenting Three Pacific Creoles",
    "section": "",
    "text": "The wider Pacific is home to hundreds of languages, that reflect the multiple expansions of humans into the region. However, the increasing connectivity across linguistic borders has seen the rise in lingua franca within states across the region. Three languages have seen particularly rapid expansion: Tok Pisin in Papua New Guinea, Solomon Island Pijin in the Solomon Islands, and Bislama, in Vanuatu.\nEach of the languages have spread quickly across many linguistic boundaries, creating new and large second-language speaker populations, and are increasingly becoming first-languages within their home states. Tok Pisin has between 3‚Äì5 million speakers, including 500,000 first language speakers, over 300,000 Solomon Island Pijin speakers (~25,000 first-language speakers), while around 318,000 Ni-Vanuatu speak Bislama.\nThis project will build large corpora for each of the languages, creating a platform for a significant expansion of linguistic research in this region, including research in the sociophonetic variation across the region, and the creation of language tools such as forced alignment and automatic translation.\nThis project is led by the team of Danielle Barth, Bethwyn Evans, Nicholas Evans, and Nick Theiberger, supported by the post-doctoral team of Lila San-Roque, Kirsty Gillespie, and myself, and research assistants Kira Davey, Saurabh Nath, and Keria Mullan.\nThis research is supported by the Commonwealth of Australia.\nPublications\nSam Passmore, Lila San Roque, Kirsty Gillespie, Saurabh Nath, Kira Davey, Keira Mullan, Tim Cawley, Jennifer Biggs, Rosey Billington, Bethwyn Evans, Nick Thieberger, and Danielle Barth. 2025. English-based acoustic models perform well in the forced alignment of two English-based Pacific Creoles. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 31172‚Äì31183, Vienna, Austria. Association for Computational Linguistics.\nPublicly Available Datasets\nTok Pisin: https://catalog.paradisec.org.au/collections/3PAC1\nSolomon Island Pijin: https://catalog.paradisec.org.au/collections/3PAC2\nBislama: https://catalog.paradisec.org.au/repository/3PAC3"
  },
  {
    "objectID": "projects/Kinbank/index.html",
    "href": "projects/Kinbank/index.html",
    "title": "Diversity of kinship terminology and Kinbank",
    "section": "",
    "text": "Kinship terminology are a small set of words use to designate different categories of kin, such a mother, father, or cousin in English or [another language example]. This small set of words have attracted immense interest in anthropology, linguistics, and cognitive science, because of they way they intersect the social organisation of family, and the biological constraints of reproduction.\nMy interest in kinship terminology sits across many domains. First and foremost I am interested in the diversity of kinship terminology, and what that might tell us about the diversity of kinship systems. Although kinship can be theoretically organised in many ways, we tend to see some structures that appear repeatedly across the globe. For example, if we consider the relatives elder brother, younger brother, elder sister, and younger sister, as well as whether the person speaking is a man or women (an important part of kinterms in many languages), we have 16 possible categories of kin. These 16 categories can be organised 4,140 possible ways. Of these, we observe many fewer organisations, with some proposing there are only around 12 ‚Äòprimary‚Äô types of sibling systems. Why and what causes such restriction in the variation of kinship terms is one of my core research questions.\n\n\n\n\n\nAs part of the study of kinship terminology diversity, I helped develop the database KinBank. KinBank is a CLDF structured database containing kinship terminology for over 1,200 languages. The data is stored on github, and there is an interactive website to explore the data at kinbank.net. Kinbank is part of a larger ERC project, lead by Professor Fiona Jordan, called VariKin.\nAnother interest I have in kinship terminology is whether patterns of borrowed kinship terms are indicative of genetic histories. While kinterms are often considered a slow changing lexical domain, there is considerable evidence that kinterms a routinely borrowed between languages. Borrowing of words, in general, is usually caused by some form of contact between speakers of each langauges. The borrowing (or sharing) of kinterms, would seem to be more likely the result of sustained contact through marriage practices. In particular, the types of borrowings that occur might be indicative of different types of contact. Sharing words for lineal relatives, like mother or father, might indicate a strong and long relationship between two groups, whereas shared words for affines, like wife or husband, might be indicative of a mutual but seperate relationship between groups. By combining population genetic and phylogenetic models of the genetic histories, with the connections between languages based on kinterm borrowings, we can identify\nI have been awarded a small grant, the Asia-Pacific Innovations Program, to collect a focused set of kinship terminology explore this avenue further in the context of South New Guinea.\nPublications\nPassmore, S., Barth, W., Greenhill, S. J., Quinn, K., Sheard, C., Argyriou, P., Birchall, J., Bowern, C., Calladine, J., Deb, A., Diederen, A., Mets√§ranta, N. P., Araujo, L. H., Schembri, R., Hickey-Hall, J., Honkola, T., Mitchell, A., Poole, L., R√°cz, P. M., ‚Ä¶ Jordan, F. M. (2023). Kinbank: A global database of kinship terminology. PLOS ONE, 18(5), e0283218. https://doi.org/10.1371/journal.pone.0283218\nPassmore, S., Barth, W., Quinn, K., Greenhill, S. J., Evans, N., & Jordan, F. M. (2021). Kin Against Kin: Internal Co-selection and the Coherence of Kinship Typologies. Biological Theory. https://doi.org/10.1007/s13752-021-00379-6\nPassmore, S., & Jordan, F. M. (2020). No universals in the cultural evolution of kinship terminology. Evolutionary Human Sciences, 2, e42. doi:10.1017/ehs.2020.41\nR√°cz, P., Passmore, S., & Jordan, F. M. (2019). Social Practice and Shared History, Not Social Scale, Structure Cross-Cultural Complexity in Kinship Systems. Topics in Cognitive Science, 12(2), 744‚Äì765. https://doi.org/10.1111/tops.12430\nR√°cz, P., Passmore, S., Sheard, C., & Jordan, F. M. (2019). Usage frequency and lexical class determine the evolution of kinship terms in Indo-European. Royal Society Open Science, 6(10), 191385. https://doi.org/10.1098/rsos.191385\nPassmore, S. (2019). Kinship Distinctions According to Generation. In Encyclopedia of Evolutionary Psychological Science (pp.¬†1‚Äì2). Springer, Cham. https://doi.org/10.1007/978-3-319-16999-6_1505-1\nPassmore, S. (2021a). Ego-Centered Kin Terminology. In Encyclopedia of Evolutionary Psychological Science (pp.¬†2303‚Äì2304). Springer, Cham. https://doi.org/10.1007/978-3-319-19650-3_1503\nPassmore, S. (2021b). Kinship Distinctions According to Sex. In Encyclopedia of Evolutionary Psychological Science (pp.¬†4417‚Äì4418). Springer, Cham. https://doi.org/10.1007/978-3-319-19650-3_1504"
  },
  {
    "objectID": "projects/GlobalJukeBox/index.html",
    "href": "projects/GlobalJukeBox/index.html",
    "title": "Musical Diversity and Cultural History",
    "section": "",
    "text": "The Global Jukebox is a free, interactive online database of coded songs from more than a thousand cultures around the world. As well as listening to the songs, you can also analyse them using science to comparing similarities and differences. The concept of The Global Jukebox was conceived by musicologist Alan Lomax long before the current digital technology was available. Led by Dr Anna Lomax-Wood, and A/ Prof.¬†Patrick Savage, this project to digitised the historical collection, making it publicaly available.\nThere are now over 6,000 songs in the database, but the bigger it is the better it can represent the full diversity of each culture.\nMy role in the project was to organise, release, and analyse the Cantometrics dataset. Cantometrics is a dataset of 5,776 coded songs from 1,026 societies, and is more than an order of magnitude larger than the largest previously available global datasets. It contains 37 features that cover dimensions of musical style, including: social organization of the performers, rhythm, melody, and ornamentation. You can view the coded data of Cantometrics on github.\nPublications:\nPassmore, S., Wood, A. L. C., Barbieri, C., Shilton, D., Daikoku, H., Atkinson, Q. D., & Savage, P. E. (2024). Global musical diversity is largely independent of linguistic and genetic histories. Nature Communications, 15(1), 3964. https://doi.org/10.1038/s41467-024-48113-7\nPassmore, S., & Savage, P. E. (2023). The Exceptions and the Rules in Global Musical Diversity. Journal of Cognition, 6(1), Article 1. https://doi.org/10.5334/joc.312\nShilton, D., Passmore, S., & Savage, P. E. (2023). Group singing is globally dominant and associated with social context. Royal Society Open Science, 10(9), 230562. https://doi.org/10.1098/rsos.230562\nWood, A. L. C., Kirby, K. R., Ember, C. R., Silbert, S., Passmore, S., Daikoku, H., McBride, J., Paulay, F., Flory, M. J., Szinger, J., D‚ÄôArcangelo, G., Bradley, K. K., Guarino, M., Atayeva, M., Rifkin, J., Baron, V., Hajli, M. E., Szinger, M., & Savage, P. E. (2022). The Global Jukebox: A public database of performing arts and culture. PLOS ONE, 17(11), e0275469. https://doi.org/10.1371/journal.pone.0275469\nSavage, P. E., Passmore, S., Chiba, G., Currie, T. E., Suzuki, H., & Atkinson, Q. D. (2022). Sequence alignment of folk song melodies reveals cross-cultural regularities of musical evolution. Current Biology. https://doi.org/10.1016/j.cub.2022.01.039"
  }
]